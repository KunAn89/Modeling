{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunan-au/Modeling_Risk/blob/main/Market_Clustering_and_Factor_Driven_Portfolio_Strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU0FSSaYGb0x"
      },
      "source": [
        "# Factor Timing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtnkwHlEG1wR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t414_umhGehk"
      },
      "source": [
        "Data From:\n",
        "\n",
        "https://www.rba.gov.au/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7sEBgYVqb4u"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHhVbfculx93"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgNoGJajcogv"
      },
      "source": [
        "Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqCLZQm_KYJK"
      },
      "outputs": [],
      "source": [
        "!pip install requests pandas matplotlib scipy statsmodels\n",
        "!pip install yfinance\n",
        "!pip show gspread\n",
        "!pip install --upgrade gspread\n",
        "!pip install TA-Lib\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXTl9rNVmcEd"
      },
      "outputs": [],
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Gb1G7pKBeU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import gspread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import talib\n",
        "import yfinance as yf\n",
        "import getpass\n",
        "import re\n",
        "import optuna\n",
        "import logging\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import pearsonr, skew, kurtosis\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOd4MNtHtviy"
      },
      "source": [
        "Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "outputs": [],
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "outputs": [],
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    # Print columns to debug\n",
        "    print(\"Columns in DataFrame:\", df.columns)\n",
        "\n",
        "    # Check if 'Date' column exists\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'].str.split().str[0], format='%m/%d/%Y').dt.date\n",
        "    else:\n",
        "        print(f\"'Date' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"'{col}' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "outputs": [],
      "source": [
        "sheet_names = [\"MSFT\", \"META\", \"NFLX\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZWpmKeiJhhXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate daily returns\n",
        "for df in dataframes:\n",
        "    df['Daily Return'] = df['Close'].pct_change()\n",
        "\n",
        "# Combine daily returns into one DataFrame\n",
        "returns = pd.concat([df['Daily Return'] for df in dataframes], axis=1)\n",
        "returns.columns = ['MSFT', 'META', 'NFLX']\n",
        "\n",
        "# Calculate annualized return and volatility of the portfolio (equal-weight strategy)\n",
        "weights = np.array([1/3, 1/3, 1/3])\n",
        "portfolio_return = np.sum(returns.mean() * weights) * 252\n",
        "portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
        "\n",
        "# Optimization function: minimize negative Sharpe Ratio\n",
        "def neg_sharpe_ratio(weights):\n",
        "    p_return = np.sum(returns.mean() * weights) * 252\n",
        "    p_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
        "    return -p_return / p_volatility\n",
        "\n",
        "# Constraints and bounds\n",
        "constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "bounds = tuple((0, 1) for asset in range(len(weights)))\n",
        "\n",
        "# Optimization process\n",
        "opt_result = minimize(neg_sharpe_ratio, weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "# Output optimization results\n",
        "optimized_weights = opt_result.x\n",
        "optimized_return = np.sum(returns.mean() * optimized_weights) * 252\n",
        "optimized_volatility = np.sqrt(np.dot(optimized_weights.T, np.dot(returns.cov() * 252, optimized_weights)))\n",
        "optimized_sharpe = -opt_result.fun\n",
        "\n",
        "print(\"Optimized Weights:\", optimized_weights)\n",
        "print(\"Optimized Annual Return:\", optimized_return)\n",
        "print(\"Optimized Annual Volatility:\", optimized_volatility)\n",
        "print(\"Optimized Sharpe Ratio:\", optimized_sharpe)"
      ],
      "metadata": {
        "id": "RCCWlLZXhgpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LknMEXnxjzDZ"
      },
      "outputs": [],
      "source": [
        "def get_sheet_data_updated(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # The first row is the header\n",
        "    header = data[0]\n",
        "    # The first column 'Year' has no header, add it manually\n",
        "    header[0] = 'Year'\n",
        "\n",
        "    df = pd.DataFrame(data[1:], columns=header)\n",
        "\n",
        "    # Convert 'Year' to datetime format\n",
        "    if 'Annual' in spreadsheet_name:\n",
        "        # Annual data, set 'Year' as the end of each year to forward fill\n",
        "        df['Year'] = pd.to_datetime(df['Year']+'-12-31')\n",
        "    else:\n",
        "        # Monthly data\n",
        "        df['Year'] = pd.to_datetime(df['Year'], format='%Y%m')\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCWgA63PcLIA"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "sheet_name_factors = ['F-F_Research_Data_5_Annual_Factors_2x3', 'F-F_Research_Data_5_Month_Factors_2x3']\n",
        "factorframes = [get_sheet_data_updated(name) for name in sheet_name_factors]\n",
        "\n",
        "# Ensure 'Year' column is datetime for both dataframes\n",
        "factorframes[0]['Year'] = pd.to_datetime(factorframes[0]['Year'], format='%Y')\n",
        "factorframes[1]['Year'] = pd.to_datetime(factorframes[1]['Year'], format='%Y%m')\n",
        "\n",
        "# Set the index to 'Year' and sort it\n",
        "annual_data = factorframes[0].set_index('Year').sort_index()\n",
        "monthly_data = factorframes[1].set_index('Year').sort_index()\n",
        "\n",
        "# Resample annual data to monthly, filling forward\n",
        "# Note: The annual data is on the last day of the year, so we need to shift it to the first day\n",
        "annual_data = annual_data.resample('MS').ffill()\n",
        "\n",
        "# Combine the data\n",
        "combined_data = pd.concat([monthly_data, annual_data]).sort_index().fillna(method='ffill')\n",
        "\n",
        "# Reset the index if needed\n",
        "combined_data = combined_data.reset_index()\n",
        "\n",
        "# Print combined data to check\n",
        "print(combined_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp39FS_EVO0_"
      },
      "source": [
        "Revelant Factors Read\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rcqy41ZCsUz"
      },
      "outputs": [],
      "source": [
        "def add_identifier(df, identifier):\n",
        "    df['identifier'] = identifier\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR5bFie3CjE7"
      },
      "source": [
        "A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSOPiM-0VSQy"
      },
      "outputs": [],
      "source": [
        "def visualize_rba_balance_sheet(spreadsheet_name, worksheet_index=0):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Extract the date column starting from the 12th row\n",
        "    date_column = [row[0] for row in data[11:]]  # Dates start at index 11 (12th row)\n",
        "    data_columns = list(zip(*data[11:]))  # Transpose rows to columns\n",
        "\n",
        "    # Clean the date column to ensure all dates are in correct format\n",
        "    # This assumes dates are in the format 'Day-Month-Year'\n",
        "    date_column_cleaned = []\n",
        "    for date_str in date_column:\n",
        "        try:\n",
        "            # If the date format is different, adjust the format parameter accordingly\n",
        "            date_column_cleaned.append(pd.to_datetime(date_str, format='%d-%b-%Y', errors='coerce'))\n",
        "        except ValueError:\n",
        "            date_column_cleaned.append(pd.NaT)  # Use NaT for any values that couldn't be converted\n",
        "\n",
        "    # Initialize DataFrame with date column\n",
        "    df = pd.DataFrame({\"date\": date_column_cleaned})\n",
        "\n",
        "    # Define variable names based on the provided column names\n",
        "    variable_names = [\n",
        "        'Notes on issue', 'Exchange settlement balances', 'Deposits of overseas institutions',\n",
        "        'Australian Government Deposits', 'State Governments Deposits', 'Other Deposits',\n",
        "        'Other liabilities', 'Other reserves and current year earnings',\n",
        "        'Capital and Reserve Bank Reserve Fund', 'Total liabilities and equity',\n",
        "        'Gold and foreign exchange', 'Australian dollar investments',\n",
        "        'Other assets (including clearing items)', 'Total assets'\n",
        "    ]\n",
        "\n",
        "    # Populate the DataFrame with the variables\n",
        "    for i, name in enumerate(variable_names, start=1):\n",
        "        df[name] = pd.to_numeric(data_columns[i], errors='coerce')\n",
        "\n",
        "    # Visualize each variable over time\n",
        "    for name in variable_names:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(x='date', y=name, data=df)\n",
        "        plt.title(f'{name.replace(\"_\", \" \").title()} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{name} ($ million)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5aysvnhVSWJ"
      },
      "outputs": [],
      "source": [
        "spreadsheet_name = 'A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET'\n",
        "visualize_rba_balance_sheet(spreadsheet_name, 0)\n",
        "balance_sheet_df = add_identifier(visualize_rba_balance_sheet('A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET'), 'Balance Sheet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdwOWNvTCkK4"
      },
      "source": [
        "A3 Reserve Bank of Australia - Open Market Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EshnGF5VOk3"
      },
      "outputs": [],
      "source": [
        "def visualize_rba_open_market_operations(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    date_column = [row[0] for row in data[12:]]  # Dates start at index 12 (13th row)\n",
        "    data_columns = list(zip(*data[12:]))  # Transpose rows to columns\n",
        "\n",
        "    column_indices = {\n",
        "        'cash_position': 1,\n",
        "        'outright_transactions_australian_govt': 2,\n",
        "        'outright_transactions_state_territory_govts': 3,\n",
        "        'foreign_exchange_swaps': 4,\n",
        "        'repurchase_agreements_general_collateral': 5,\n",
        "        'repurchase_agreements_private_securities': 6,\n",
        "        'exchange_settlement_account_balances': 7,\n",
        "        'overnight_repurchase_agreements': 8,\n",
        "        'market_value_securities_held_under_reverse_repo_cgs': 9,\n",
        "        'market_value_securities_held_under_reverse_repo_semis': 10,\n",
        "        'market_value_securities_held_under_reverse_repo_other_government_related': 11,\n",
        "        'market_value_securities_held_under_reverse_repo_adi_issued': 12,\n",
        "        'market_value_securities_held_under_reverse_repo_asset_backed_securities': 13,\n",
        "        'market_value_securities_held_under_reverse_repo_other': 14\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(date_column)})\n",
        "    for name, index in column_indices.items():\n",
        "        df[name] = pd.to_numeric(data_columns[index], errors='coerce')\n",
        "\n",
        "    for name in column_indices.keys():\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(x='date', y=name, data=df)\n",
        "        plt.title(f'{name.replace(\"_\", \" \").title()} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{name.replace(\"_\", \" \").title()} / $m')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqRBgQ6YHNNN"
      },
      "outputs": [],
      "source": [
        "spreadsheet_name = 'A3 Reserve Bank of Australia - Open Market Operations'\n",
        "visualize_rba_open_market_operations(spreadsheet_name)\n",
        "open_market_operations_df = add_identifier(visualize_rba_open_market_operations('A3 Reserve Bank of Australia - Open Market Operations'), 'Open Market Operations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wir2FZD_ClzK"
      },
      "source": [
        "A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N53aUsM6Cgyn"
      },
      "outputs": [],
      "source": [
        "def visualize_holdings_data(spreadsheet_name, worksheet_index=0):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Extract the date column and the specified data columns starting from row 12\n",
        "    date_column = [row[0] for row in data[11:]]  # Dates start at index 11 (12th row)\n",
        "    data_columns = list(zip(*data[11:]))  # Transpose rows to columns\n",
        "\n",
        "    # Column indices based on the spreadsheet structure\n",
        "    column_indices = {\n",
        "        \"Australian Government Securities and Semis Holdings Total\": 1,\n",
        "        \"AGS Total\": 2,\n",
        "        \"Semis Total\": 3,\n",
        "        \"Australian Government Bonds\": 5,\n",
        "        \"Australian Government T-Notes\": 6,\n",
        "        \"NSWTC Bonds\": 7,\n",
        "        \"TCV Bonds\": 8,\n",
        "        \"QTC Bonds\": 9,\n",
        "        \"WATC Bonds\": 10,\n",
        "        \"SAFA Bonds\": 11,\n",
        "        \"TASC Bonds\": 12,\n",
        "        \"NTTY Bonds\": 13,\n",
        "        \"ACTT Bonds\": 14\n",
        "    }\n",
        "\n",
        "    # Convert date column to datetime and initialize DataFrame with it\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(date_column)})\n",
        "\n",
        "    # Add each data column to the DataFrame and convert to numeric\n",
        "    for name, index in column_indices.items():\n",
        "        df[name] = pd.to_numeric(data_columns[index], errors='coerce')\n",
        "\n",
        "    # Plot each variable over time\n",
        "    for name in column_indices.keys():\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(x='date', y=name, data=df)\n",
        "        plt.title(f'{name} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{name} / $m')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwD6ahK2Cg2j"
      },
      "outputs": [],
      "source": [
        "spreadsheet_name = 'A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS'\n",
        "visualize_holdings_data(spreadsheet_name)\n",
        "holdings_data_df = add_identifier(visualize_holdings_data('A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS'), 'Holdings Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDImBTNhOyzr"
      },
      "source": [
        "Reshape Factors and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_p1Z__CDLWV"
      },
      "outputs": [],
      "source": [
        "final_df = pd.concat([balance_sheet_df, open_market_operations_df, holdings_data_df], axis=0, ignore_index=True)\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqbN9s8jR98a"
      },
      "outputs": [],
      "source": [
        "def calculate_correlation(df, feature1, feature2):\n",
        "    common_nonzero = df[(df[feature1] != 0) & (df[feature2] != 0)]\n",
        "    if len(common_nonzero) < 2:\n",
        "        return None  # Not enough data points\n",
        "    corr, _ = pearsonr(common_nonzero[feature1], common_nonzero[feature2])\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR21REYAH2f_"
      },
      "outputs": [],
      "source": [
        "def feature_analysis_for_engineering(df):\n",
        "    \"\"\"\n",
        "    Performs enhanced exploratory data analysis with larger histogram sizes,\n",
        "    advanced analytics including PCA and feature importance using Random Forest.\n",
        "    \"\"\"\n",
        "    # Replace NaN and inf with 0 for data cleaning\n",
        "    df_cleaned = df.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "    # Descriptive Statistics\n",
        "    print(\"\\n[Exploratory Data Analysis] Descriptive Statistics:\")\n",
        "    print(df_cleaned.describe())\n",
        "\n",
        "    # Histograms and Box Plots Visualization\n",
        "    numerical_features = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    n_features = len(numerical_features)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_features + n_cols - 1) // n_cols * 2  # Allocate rows for both histograms and box plots\n",
        "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*6, n_rows*3))  # Increase figure size for clarity\n",
        "    fig.suptitle('Numerical Features Analysis: Histograms and Box Plots Excluding Zeros')\n",
        "\n",
        "    for i, feature in enumerate(numerical_features):\n",
        "        non_zero_values = df_cleaned[feature][df_cleaned[feature] != 0]\n",
        "        ax_hist = axs[i // n_cols * 2, i % n_cols]\n",
        "        ax_box = axs[i // n_cols * 2 + 1, i % n_cols]\n",
        "        if not non_zero_values.empty:\n",
        "            sns.histplot(non_zero_values, bins='auto', ax=ax_hist)\n",
        "            sns.boxplot(x=non_zero_values, ax=ax_box)\n",
        "        else:\n",
        "            ax_hist.text(0.5, 0.5, 'All values are 0', ha='center', va='center')\n",
        "            ax_hist.axis('off')\n",
        "            ax_box.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.95)\n",
        "    plt.show()\n",
        "\n",
        "    # Skewness and Kurtosis\n",
        "    print(\"\\n[Skewness and Kurtosis Analysis]\")\n",
        "    for feature in numerical_features:\n",
        "        print(f\"{feature} - Skewness: {skew(df_cleaned[feature]):.2f}, Kurtosis: {kurtosis(df_cleaned[feature]):.2f}\")\n",
        "\n",
        "    # Improved Correlation Analysis\n",
        "    print(\"\\n[Correlation Analysis - Improved]\")\n",
        "    numerical_features = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    correlations = []\n",
        "    for i in range(len(numerical_features)):\n",
        "        for j in range(i+1, len(numerical_features)):\n",
        "            corr = calculate_correlation(df_cleaned, numerical_features[i], numerical_features[j])\n",
        "            if corr is not None:  # Exclude pairs with correlation based entirely on zeros\n",
        "                print(f\"Correlation between {numerical_features[i]} and {numerical_features[j]}: {corr:.2f}\")\n",
        "            else:\n",
        "                print(f\"Excluded {numerical_features[i]} and {numerical_features[j]} due to insufficient data.\")\n",
        "\n",
        "    # Variability and Change Analysis\n",
        "    print(\"\\n[Variability and Change Analysis]\")\n",
        "    print(f\"{'Feature':<30} {'Mean':<15} {'Standard Deviation':<15}\")\n",
        "    for feature in numerical_features:\n",
        "        mean_value = df_cleaned[feature].mean()\n",
        "        std_dev = df_cleaned[feature].std()\n",
        "        print(f\"{feature:<30} {mean_value:<15.2f} {std_dev:<15.2f}\")\n",
        "\n",
        "    # Segmentation Analysis if applicable\n",
        "    if 'segment' in df_cleaned.columns:\n",
        "        segments = df_cleaned['segment'].unique()\n",
        "        print(\"\\n[Segmentation Analysis]\")\n",
        "        for feature in numerical_features:\n",
        "            print(f\"\\nFeature: {feature}\")\n",
        "            for segment in segments:\n",
        "                segment_mean = df_cleaned[df_cleaned['segment'] == segment][feature].mean()\n",
        "                segment_std = df_cleaned[df_cleaned['segment'] == segment][feature].std()\n",
        "                print(f\"Segment {segment} - Mean: {segment_mean:.2f}, Std Dev: {segment_std:.2f}\")\n",
        "\n",
        "    # Correlation Matrix Visualization\n",
        "    if len(numerical_features) > 1:\n",
        "        print(\"\\n[Correlation Matrix Visualization]\")\n",
        "        correlation_matrix = df_cleaned[numerical_features].corr()\n",
        "        plt.figure(figsize=(100, 80))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "        plt.title('Correlation Matrix of Numerical Features')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Advanced Analytics\n",
        "    # Principal Component Analysis (PCA)\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(df_cleaned[numerical_features])\n",
        "    print(\"\\n[PCA] Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "    # Feature Importance using Random Forest\n",
        "    # Assuming a target variable 'y' exists. You might need to adjust this part.\n",
        "    if 'target' in df_cleaned.columns:\n",
        "        X = df_cleaned[numerical_features]\n",
        "        y = df_cleaned['target']\n",
        "        rf = RandomForestRegressor(n_estimators=100)\n",
        "        rf.fit(X, y)\n",
        "        importance = rf.feature_importances_\n",
        "        print(\"\\n[Random Forest Feature Importance]\")\n",
        "        for i, feature in enumerate(numerical_features):\n",
        "            print(f\"{feature}: {importance[i]:.4f}\")\n",
        "\n",
        "    # Missing Values Analysis (Proportion of Zeros)\n",
        "    print(\"\\n[Missing Values Analysis] Proportion of Zeros:\")\n",
        "    for feature in numerical_features:\n",
        "        zeros = (df_cleaned[feature] == 0).mean()\n",
        "        print(f\"{feature}: {zeros:.2%} zeros\")\n",
        "\n",
        "    # Significant Yearly Changes Analysis\n",
        "    time_column = df.columns[0]\n",
        "    significant_change_threshold = 0.1\n",
        "\n",
        "    print(\"\\n[Significant Changes Analysis Based on the First Column]\")\n",
        "    for feature in numerical_features:\n",
        "        print(f\"\\nAnalyzing significant changes in {feature}:\")\n",
        "        yearly_means = df.groupby(time_column)[feature].mean()\n",
        "        yearly_changes = yearly_means.pct_change().fillna(0)\n",
        "        significant_changes = yearly_changes[abs(yearly_changes) >= significant_change_threshold]\n",
        "        if not significant_changes.empty:\n",
        "            print(f\"Significant changes detected in {feature}:\")\n",
        "            print(significant_changes)\n",
        "        else:\n",
        "            print(f\"No significant changes detected in {feature} based on the threshold.\")\n",
        "\n",
        "\n",
        "    # Correlation Matrix Visualization with special handling\n",
        "    plt.figure(figsize=(100, 80))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', mask=correlation_matrix == 1)\n",
        "    plt.title('Correlation Matrix of Numerical Features with Special Handling')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkDth4VbKi79"
      },
      "outputs": [],
      "source": [
        "feature_analysis_for_engineering(final_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81nrx_fXpxO4"
      },
      "source": [
        "Genneral Function /  Failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FOGlWPKzD8P"
      },
      "outputs": [],
      "source": [
        "RBA_sheet_names = [\"A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET\", \"A2 RESERVE BANK OF AUSTRALIA – MONETARY POLICY CHANGES\", \"A3 Reserve Bank of Australia - Open Market Operations\"\n",
        "                  \"A3 Reserve Bank of Australia - Open Market Operations\",\"A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\"A3.2 SECURITIES LENDING REPURCHASE TRANSACTIONS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\n",
        "                   \"A4 RESERVE BANK OF AUSTRALIA – FOREIGN EXCHANGE TRANSACTIONS AND HOLDINGS OF OFFICIAL RESERVE ASSETS\",\"A5 RESERVE BANK OF AUSTRALIA - DAILY FOREIGN EXCHANGE MARKET INTERVENTION TRANSACTIONS\",\n",
        "                   \"A6 RESERVE BANK OF AUSTRALIA – BANKNOTES ON ISSUE BY DENOMINATION\",\"A7 RESERVE BANK OF AUSTRALIA – DETECTED AUSTRALIAN COUNTERFEITS BY DENOMINATION\",\"B1 ASSETS OF FINANCIAL INSTITUTIONS\",\n",
        "                   \"B2 BANKS – OFF-BALANCE SHEET BUSINESS\",\"B11.1 INTERNATIONAL ASSETS OF THE AUSTRALIAN-LOCATED OPERATIONS OF BANKS AND RFCs\",\"C1 Credit and Charge Cards – Seasonally Adjusted Series\",\n",
        "                   \"C1.1 Credit and Charge Cards – Original Series – Aggregate Data\",\"C1.3 Credit and Charge Cards – Market Shares of Card Schemes\",\"C2 Debit Cards – Seasonally Adjusted Series\",\n",
        "                   \"C2.1 Debit Cards – Original Series\",\"C2.2 Prepaid Cards – Original Series\",\"D1 GROWTH IN SELECTED FINANCIAL AGGREGATES\",\"F5 INDICATOR LENDING RATES\",\n",
        "                   \"F7 BUSINESS LENDING RATES\",\"F8 PERSONAL LENDING RATES\",\"F11.1 EXCHANGE RATES\",\"F13 INTERNATIONAL OFFICIAL INTEREST RATES\",\"G1 CONSUMER PRICE INFLATION\",\"H1 GROSS DOMESTIC PRODUCT AND INCOME\",\n",
        "                   \"I1 INTERNATIONAL TRADE AND BALANCE OF PAYMENTS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRIgtMJrMAJW"
      },
      "outputs": [],
      "source": [
        "def get_rba_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    numeric_columns = get_numeric_columns(data[1:])  # Skip header row\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pXPrktYMAL2"
      },
      "outputs": [],
      "source": [
        "def get_numeric_columns(data):\n",
        "    numeric_cols = []\n",
        "    for col in range(len(data[0])):  # Check each column\n",
        "        try:\n",
        "            if all(is_number(str(val)) for val in data[1:] if val not in [None, \"\", \" \"]):\n",
        "                numeric_cols.append(data[0][col])\n",
        "        except Exception as e:\n",
        "            print(f\"Error in column {col}: {e}\")\n",
        "    return numeric_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdGeRAJ9MAOd"
      },
      "outputs": [],
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3njeySMARC"
      },
      "outputs": [],
      "source": [
        "def find_start_row(df):\n",
        "    date_pattern = re.compile(r'\\d{2}-\\w{3}-\\d{4}')\n",
        "    for i, row in df.iterrows():\n",
        "        if date_pattern.search(str(row[0])):\n",
        "            return i\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfatz5C1MATb"
      },
      "outputs": [],
      "source": [
        "# Function to visualize financial factors\n",
        "def visualize_financial_factors(data):\n",
        "    # Convert the 'Publication date' column to datetime\n",
        "    data['Publication date'] = pd.to_datetime(data['Publication date'])\n",
        "\n",
        "    # Group by 'Publication date' and calculate the mean 'Coupon Rate' for that date\n",
        "    grouped_data = data.groupby('Publication date').agg({'Coupon Rate': 'mean'})\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    grouped_data['Coupon Rate'].plot(ax=ax)\n",
        "\n",
        "    # Annotations (you might want to customize the position or content of annotations)\n",
        "    for idx, row in data.iterrows():\n",
        "        ax.annotate(row['Identifier'], (row['Publication date'], row['Coupon Rate']),\n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "    # Improve plot\n",
        "    ax.set_title('Average Coupon Rate Over Time')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Coupon Rate (%)')\n",
        "    ax.grid(True)\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ticRtTgGMAW6"
      },
      "outputs": [],
      "source": [
        "# Function to visualize data from multiple sheets\n",
        "def visualize_financial_factors_from_multiple_sheets(df_list):\n",
        "    if 'Publication date' not in data.columns or 'Coupon Rate' not in data.columns:\n",
        "      print(\"Required columns are not present in the dataframe.\")\n",
        "      return\n",
        "\n",
        "    plots = []\n",
        "\n",
        "    for df in df_list:\n",
        "        # Find the start row containing the date\n",
        "        start_row = find_start_row(df)\n",
        "        if start_row is None:\n",
        "            raise ValueError(\"No date found in the first column.\")\n",
        "\n",
        "        # Read the data from the start row\n",
        "        data = df.iloc[start_row:].copy()\n",
        "        data.columns = df.iloc[start_row - 1]\n",
        "        data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Convert date format\n",
        "        data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], errors='coerce')\n",
        "\n",
        "        # Visualize each financial variable\n",
        "        for column in data.columns[2:]:  # Assuming the first column is the date, second is the identifier\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            ax.plot(data[data.columns[0]], data[column], label=column)\n",
        "\n",
        "            # Set chart\n",
        "            ax.set_title(f'Time Series of {column}')\n",
        "            ax.set_xlabel('Date')\n",
        "            ax.set_ylabel(column)\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.show()\n",
        "            plots.append(plt)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybKM1KU-O0nc"
      },
      "outputs": [],
      "source": [
        "def visualize_categorical_data(df, categorical_column, numerical_columns):\n",
        "    plots = []\n",
        "\n",
        "    for column in numerical_columns:\n",
        "        # Create bar plot\n",
        "        bar_plot = df.groupby(categorical_column)[column].mean().plot(kind='bar')\n",
        "        plt.show()\n",
        "        plots.append(bar_plot.figure)\n",
        "\n",
        "        # Create box plot\n",
        "        box_plot = df.boxplot(column=column, by=categorical_column, grid=False)\n",
        "        plt.title(f'{column} by {categorical_column}')\n",
        "        plt.suptitle('')  # Remove the default title\n",
        "        plt.show()\n",
        "        plots.append(box_plot.figure)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxIS7bvyPjUy"
      },
      "outputs": [],
      "source": [
        "# Function to visualize significant relationships\n",
        "def visualize_significant_relationships(df, date_column, numerical_columns, threshold=0.05):\n",
        "    plots = []\n",
        "\n",
        "    # Convert date column to datetime type\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "    # Iterate over each numerical variable\n",
        "    for num_col in numerical_columns:\n",
        "        # Iterate over each column in DataFrame to find relationships with numerical variables\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' or col == date_column:\n",
        "                continue  # Skip non-numerical variables and the date column\n",
        "\n",
        "            # Calculate the correlation between numerical variables\n",
        "            correlation, p_value = stats.pearsonr(df[col], df[num_col])\n",
        "\n",
        "            # If correlation is significant, visualize it\n",
        "            if p_value < threshold:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.regplot(x=col, y=num_col, data=df)\n",
        "                plt.title(f'Relationship between {col} and {num_col} (p={p_value:.4f})')\n",
        "                plt.show()\n",
        "                plots.append(plt)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVIFDY2xPlfO"
      },
      "outputs": [],
      "source": [
        "def check_sheets_in_excel(file_path):\n",
        "    xls = pd.ExcelFile(file_path)\n",
        "    sheet_names = xls.sheet_names  # Get all sheet names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JtQx24HPnQf"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    RBA_sheet_names = RBA_sheet_names = [\"A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET\", \"A2 RESERVE BANK OF AUSTRALIA – MONETARY POLICY CHANGES\", \"A3 Reserve Bank of Australia - Open Market Operations\"\n",
        "                  \"A3 Reserve Bank of Australia - Open Market Operations\",\"A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\"A3.2 SECURITIES LENDING REPURCHASE TRANSACTIONS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\n",
        "                   \"A4 RESERVE BANK OF AUSTRALIA – FOREIGN EXCHANGE TRANSACTIONS AND HOLDINGS OF OFFICIAL RESERVE ASSETS\",\"A5 RESERVE BANK OF AUSTRALIA - DAILY FOREIGN EXCHANGE MARKET INTERVENTION TRANSACTIONS\",\n",
        "                   \"A6 RESERVE BANK OF AUSTRALIA – BANKNOTES ON ISSUE BY DENOMINATION\",\"A7 RESERVE BANK OF AUSTRALIA – DETECTED AUSTRALIAN COUNTERFEITS BY DENOMINATION\",\"B1 ASSETS OF FINANCIAL INSTITUTIONS\",\n",
        "                   \"B2 BANKS – OFF-BALANCE SHEET BUSINESS\",\"B11.1 INTERNATIONAL ASSETS OF THE AUSTRALIAN-LOCATED OPERATIONS OF BANKS AND RFCs\",\"C1 Credit and Charge Cards – Seasonally Adjusted Series\",\n",
        "                   \"C1.1 Credit and Charge Cards – Original Series – Aggregate Data\",\"C1.3 Credit and Charge Cards – Market Shares of Card Schemes\",\"C2 Debit Cards – Seasonally Adjusted Series\",\n",
        "                   \"C2.1 Debit Cards – Original Series\",\"C2.2 Prepaid Cards – Original Series\",\"D1 GROWTH IN SELECTED FINANCIAL AGGREGATES\",\"F5 INDICATOR LENDING RATES\",\n",
        "                   \"F7 BUSINESS LENDING RATES\",\"F8 PERSONAL LENDING RATES\",\"F11.1 EXCHANGE RATES\",\"F13 INTERNATIONAL OFFICIAL INTEREST RATES\",\"G1 CONSUMER PRICE INFLATION\",\"H1 GROSS DOMESTIC PRODUCT AND INCOME\",\n",
        "                   \"I1 INTERNATIONAL TRADE AND BALANCE OF PAYMENTS\"]\n",
        "\n",
        "    all_dataframes = []\n",
        "\n",
        "    for spreadsheet_name in RBA_sheet_names:\n",
        "        try:\n",
        "            df = get_rba_sheet_data(spreadsheet_name, worksheet_index=0)\n",
        "            start_row = find_start_row(df)\n",
        "\n",
        "            if start_row is not None:\n",
        "                new_header = df.iloc[start_row]\n",
        "                df = df[start_row + 1:]\n",
        "                df.columns = new_header\n",
        "\n",
        "                if 'Series ID' in df.columns:\n",
        "                    df['Series ID'] = pd.to_datetime(df['Series ID'], errors='coerce')\n",
        "\n",
        "                if 'SomeNumericColumn' in df.columns:\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.plot(df['Series ID'], df['SomeNumericColumn'], label='SomeNumericColumn')\n",
        "                    plt.xlabel('Date')\n",
        "                    plt.ylabel('Value')\n",
        "                    plt.title(f'Data Visualization for {spreadsheet_name}')\n",
        "                    plt.legend()\n",
        "                    plt.show()\n",
        "\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing document: {spreadsheet_name}\")\n",
        "            print(f\"Error details: {e}\")\n",
        "            import traceback\n",
        "            print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj8kAfjW2ln-"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vf6vkCY1jI2"
      },
      "source": [
        "**Individual Stock Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY19Oxb-kjw3"
      },
      "source": [
        "Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYrD47uIZuTw"
      },
      "outputs": [],
      "source": [
        "# Price and Volume Plot for all DataFrames\n",
        "for df, name in zip(dataframes, sheet_names):\n",
        "    # Price and Volume Plot\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(df.index, df['Volume'], color='orange')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'Daily Closing Prices for {name}')\n",
        "    print(f'Daily Volume for {name}')\n",
        "\n",
        "    # Moving Average Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['MA30'] = df['Close'].rolling(window=30).mean()\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.plot(df['MA5'], label='5-Day MA')\n",
        "    plt.plot(df['MA30'], label='30-Day MA')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f'Moving Averages for {name}')\n",
        "\n",
        "    # Price Fluctuation Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    plt.plot(df['Price_Range'])\n",
        "    plt.show()\n",
        "    print(f'Price Fluctuation for {name}')\n",
        "\n",
        "    # Momentum Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
        "    plt.plot(df['Momentum'])\n",
        "    plt.show()\n",
        "    print(f'Momentum for {name}')\n",
        "\n",
        "    # Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation = df.corr()\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.show()\n",
        "    print(f'Correlation Heatmap for {name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1WW9k4Sc6Of"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns for each asset\n",
        "returns_dict = {}\n",
        "\n",
        "for asset_name, df in zip(sheet_names, dataframes):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    returns_dict[asset_name] = df[['Date', 'Returns']].copy()\n",
        "\n",
        "# Print the returns for each asset\n",
        "for asset_name, returns_df in returns_dict.items():\n",
        "    print(f\"Asset: {asset_name}\")\n",
        "    print(returns_df.head())\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br_AlYcDK0Nd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIkSEr4rK0UV"
      },
      "outputs": [],
      "source": [
        "def get_alpha_vantage_data(symbol, api_key, function=\"TIME_SERIES_DAILY\"):\n",
        "    base_url = \"https://www.alphavantage.co/query\"\n",
        "    params = {\n",
        "        \"function\": function,\n",
        "        \"symbol\": symbol,\n",
        "        \"apikey\": api_key,\n",
        "        \"datatype\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wVeGx--K0XB"
      },
      "outputs": [],
      "source": [
        "def process_alpha_vantage_data(data):\n",
        "    df = pd.DataFrame(data['Time Series (Daily)']).T\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df.apply(pd.to_numeric)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67vj_NsK8gR"
      },
      "outputs": [],
      "source": [
        "api_key = getpass.getpass(\"Please enter Alpha Vantage API key:\")\n",
        "symbol = \"MSFT\", \"META\", \"NFLX\"\n",
        "\n",
        "raw_data = get_alpha_vantage_data(symbol, api_key)\n",
        "print(raw_data)\n",
        "df = process_alpha_vantage_data(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMIaHuYvPPpk"
      },
      "outputs": [],
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSK_oBPIJ78m"
      },
      "outputs": [],
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "print(sp500_df.head())\n",
        "\n",
        "market_returns = sp500_df['Close'].pct_change()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paZFyrmJklWU"
      },
      "source": [
        "ADV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N098NWA7xoGG"
      },
      "outputs": [],
      "source": [
        "def calculate_asset_returns(df):\n",
        "    asset_returns = df['Close'].pct_change().dropna()\n",
        "    return asset_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdwNnSV4x453"
      },
      "outputs": [],
      "source": [
        "def calculate_alpha_and_beta(asset_returns, market_returns):\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(asset_returns, market_returns)\n",
        "    alpha = intercept\n",
        "    beta = slope\n",
        "    return alpha, beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8VTHQwz7Hf"
      },
      "outputs": [],
      "source": [
        "def get_risk_factors(start_date, end_date):\n",
        "    try:\n",
        "        # Download Fama-French factors\n",
        "        ff_factors = yf.download(\"F-F_Research_Data_Factors_daily\", start=start_date, end=end_date)\n",
        "\n",
        "        # Rename columns based on the number of columns\n",
        "        if len(ff_factors.columns) == 4:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate']\n",
        "        elif len(ff_factors.columns) == 6:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate', 'Column5', 'Column6']\n",
        "\n",
        "        ff_factors = ff_factors / 100  # Convert to percentages\n",
        "\n",
        "        return ff_factors\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB0oLHVO1liZ"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_volatility(df, window=20):\n",
        "    # Calculate and plot volatility\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    volatility = returns.rolling(window=window).std()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(volatility, label='Volatility')\n",
        "    plt.title('Volatility Factor')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-se7onuZuWW"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_sharpe_ratio(df, risk_free_rate=0.02):\n",
        "    # Calculate and visualize Sharpe Ratio\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    volatility = returns.std()\n",
        "    sharpe_ratio = (mean_return - risk_free_rate) / volatility\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(returns.index, returns, label='Asset Returns')\n",
        "    plt.title('Asset Returns')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Sharpe Ratio: {sharpe_ratio}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxVXh5ZeZuY8"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_r_squared(asset_returns, market_returns):\n",
        "    # Calculate R-squared\n",
        "    r_squared = np.corrcoef(asset_returns, market_returns)[0, 1] ** 2\n",
        "    print(f'R-Squared Value: {r_squared}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og9boVWNZubs"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    # Calculate and visualize technical indicators\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    elif indicator == 'RSI':\n",
        "        # RSI calculation\n",
        "        delta = df['Close'].diff(1)\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=window).mean()\n",
        "        avg_loss = loss.rolling(window=window).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        df['RSI'] = rsi\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['RSI'], label='RSI')\n",
        "        plt.title('Relative Strength Index (RSI)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        # Bollinger Bands calculation\n",
        "        df['MA'] = df['Close'].rolling(window=window).mean()\n",
        "        df['Upper_Band'] = df['MA'] + 2 * df['Close'].rolling(window=window).std()\n",
        "        df['Lower_Band'] = df['MA'] - 2 * df['Close'].rolling(window=window).std()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['Close'], label='Close Price', alpha=0.5)\n",
        "        plt.plot(df['Upper_Band'], label='Upper Bollinger Band')\n",
        "        plt.plot(df['Lower_Band'], label='Lower Bollinger Band')\n",
        "        plt.title('Bollinger Bands')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEFBhsbgVaC1"
      },
      "outputs": [],
      "source": [
        "def factor_model_analysis(asset_returns, market_returns, risk_factors):\n",
        "    # Prepare the risk factors DataFrame with a constant for the OLS regression\n",
        "    X = sm.add_constant(risk_factors)\n",
        "\n",
        "    # Run the OLS regression\n",
        "    model = sm.OLS(asset_returns, X).fit()\n",
        "\n",
        "    # Print the summary of the regression\n",
        "    print(model.summary())\n",
        "\n",
        "    # Visualize the factor loadings\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(X.columns[1:], model.params[1:])  # Skip the constant\n",
        "    plt.xlabel('Risk Factors')\n",
        "    plt.ylabel('Factor Loadings')\n",
        "    plt.title('Factor Loadings for Asset Returns')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nohcZAqWEMNh"
      },
      "outputs": [],
      "source": [
        "for df in dataframes:\n",
        "    # Calculate asset returns for the current DataFrame\n",
        "    asset_returns = calculate_asset_returns(df)\n",
        "\n",
        "    # Visualize volatility and Sharpe ratio for the asset\n",
        "    calculate_and_visualize_volatility(df)\n",
        "    calculate_and_visualize_sharpe_ratio(df)\n",
        "\n",
        "    # Calculate alpha and beta relative to market returns\n",
        "    alpha, beta = calculate_alpha_and_beta(asset_returns, market_returns)\n",
        "    print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
        "\n",
        "    # Visualize R-squared value\n",
        "    calculate_and_visualize_r_squared(asset_returns, market_returns)\n",
        "\n",
        "    # Apply and visualize a Moving Average technical indicator\n",
        "    calculate_and_visualize_technical_indicator(df, 'MA')\n",
        "\n",
        "    # Check if the DataFrame contains risk factors\n",
        "    if 'risk_factors' not in df.columns:\n",
        "        print(\"Risk factors are not in the DataFrame\")\n",
        "        continue  # Skip the rest of the loop if risk factors are missing\n",
        "\n",
        "    # Align asset returns with the risk factors based on their index (dates)\n",
        "    risk_factors_aligned = risk_factors.loc[asset_returns.index]\n",
        "\n",
        "    # Drop missing values to ensure clean data for analysis\n",
        "    asset_returns.dropna(inplace=True)\n",
        "    risk_factors_aligned.dropna(inplace=True)\n",
        "\n",
        "    # Find common dates between asset returns and risk factors for alignment\n",
        "    common_dates = asset_returns.index.intersection(risk_factors_aligned.index)\n",
        "    asset_returns_aligned = asset_returns.loc[common_dates]\n",
        "    risk_factors_aligned = risk_factors_aligned.loc[common_dates]\n",
        "\n",
        "    # Perform factor model analysis using the aligned data\n",
        "    factor_model_analysis(asset_returns_aligned, market_returns.loc[common_dates], risk_factors_aligned)\n",
        "\n",
        "    # Print the head of the DataFrame to check data\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xDwzM_87Sgg"
      },
      "source": [
        "**Index Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4BPnKRcesdv"
      },
      "outputs": [],
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thm31nbyLi0H"
      },
      "outputs": [],
      "source": [
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLu4npUveshu"
      },
      "outputs": [],
      "source": [
        "# Function to get S&P 500 data\n",
        "def get_sp500_data(start_date, end_date):\n",
        "    return yf.download('^GSPC', start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3f7iTAULdmr"
      },
      "outputs": [],
      "source": [
        "# Function to calculate max drawdown\n",
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh1-Djo4LdpD"
      },
      "outputs": [],
      "source": [
        "# Adding technical indicators\n",
        "def add_technical_indicators(df):\n",
        "    df['SMA_50'] = talib.SMA(df['Close'], timeperiod=50)\n",
        "    df['RSI_14'] = talib.RSI(df['Close'], timeperiod=14)\n",
        "    df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtNcpNfaHD7q"
      },
      "outputs": [],
      "source": [
        "cluster_names = {\n",
        "    0: 'Stable Growth Market',\n",
        "    1: 'High Volatility Market',\n",
        "    2: 'Correction/Bear Market'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBAe4ApesTr"
      },
      "source": [
        "S&P 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_re7W7jPBoJc"
      },
      "outputs": [],
      "source": [
        "# Data Retrieval\n",
        "sp500_df = get_sp500_data('2000-01-01', '2022-12-30')\n",
        "\n",
        "# Data Preparation and Feature Engineering\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "sp500_df.dropna(inplace=True)\n",
        "add_technical_indicators(sp500_df)\n",
        "\n",
        "# Feature Scaling\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_features = robust_scaler.fit_transform(sp500_df[['Max Drawdown', 'Returns']])\n",
        "\n",
        "# PCA for Dimensionality Reduction\n",
        "pca = PCA(n_components=2)\n",
        "sp500_pca = pca.fit_transform(scaled_features)\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_pca)\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Model Preparation and Evaluation\n",
        "features = sp500_df[['SMA_50', 'RSI_14', 'Volume', 'Max Drawdown', 'Returns']]\n",
        "labels = sp500_df['KMeans_Cluster']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(X_resampled, y_resampled)\n",
        "scores = cross_val_score(rf_classifier, X_resampled, y_resampled, cv=5)\n",
        "print(f\"Cross-validated scores: {scores.mean()}\")\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Visualization of PCA-transformed features with corrected cluster names for S&P 500 data\n",
        "plt.figure(figsize=(30, 18))\n",
        "cluster_labels_named = pd.Series(kmeans.labels_).map(cluster_names)\n",
        "sns.scatterplot(x=sp500_pca[:, 0], y=sp500_pca[:, 1], hue=cluster_labels_named, palette='viridis')\n",
        "plt.title('PCA of S&P 500 Data with Cluster Names')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "plt.figure(figsize=(30, 18))\n",
        "sns.scatterplot(x=sp500_df.index, y=sp500_df['Max Drawdown'], hue=sp500_df['KMeans_Cluster'].map(cluster_names), palette='viridis')\n",
        "plt.title('K-Means Clustering Results on S&P 500 Data with Cluster Names')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# Advanced Analysis: Sub-cluster Analysis and Clustering Stability\n",
        "for cluster_label in np.unique(kmeans.labels_):\n",
        "    cluster_data = sp500_pca[kmeans.labels_ == cluster_label]\n",
        "    sub_kmeans = KMeans(n_clusters=2, random_state=0).fit(cluster_data)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=cluster_data[:, 0], y=cluster_data[:, 1], hue=sub_kmeans.labels_, palette='viridis')\n",
        "    plt.title(f'Sub-clusters within Cluster {cluster_label}')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.legend(title='Sub-cluster')\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "initial_labels = kmeans.labels_\n",
        "rand_scores = []\n",
        "for _ in range(10):\n",
        "    kmeans_new = KMeans(n_clusters=3, random_state=None).fit(sp500_pca)\n",
        "    new_labels = kmeans_new.labels_\n",
        "    rand_score = adjusted_rand_score(initial_labels, new_labels)\n",
        "    rand_scores.append(rand_score)\n",
        "print(f\"Average Adjusted Rand Index for cluster stability: {np.mean(rand_scores)}\")\n",
        "\n",
        "# Evaluating the performance of the classification model using confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Calculating and printing the F1 Score, Precision, and Recall of the classification model\n",
        "# These metrics give a more comprehensive understanding of model performance, especially in imbalanced datasets\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Calculate the silhouette score to assess the clustering quality, where a higher score indicates better-defined clusters\n",
        "silhouette_avg = silhouette_score(sp500_pca, kmeans.labels_)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Calculate the Calinski-Harabasz Index, a higher score suggests better cluster separation and cohesion\n",
        "calinski_harabasz = calinski_harabasz_score(sp500_pca, kmeans.labels_)\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "# Calculate the Davies-Bouldin Index, a lower score indicates better clustering\n",
        "davies_bouldin = davies_bouldin_score(sp500_pca, kmeans.labels_)\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9MLAaQvUwJK"
      },
      "source": [
        "S&P / ASX 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGsSpewKUvXw"
      },
      "outputs": [],
      "source": [
        "asx_df = yf.download('^AXJO', start=\"2000-01-01\", end=\"2023-12-31\")\n",
        "\n",
        "# Data Preparation and Feature Engineering\n",
        "asx_df['Returns'] = asx_df['Close'].pct_change()\n",
        "asx_df['Max Drawdown'] = calculate_max_drawdown(asx_df['Returns'])\n",
        "asx_df.dropna(inplace=True)\n",
        "add_technical_indicators(asx_df)\n",
        "\n",
        "# Feature Scaling\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_features = robust_scaler.fit_transform(asx_df[['Max Drawdown', 'Returns']])\n",
        "\n",
        "# PCA for Dimensionality Reduction\n",
        "pca = PCA(n_components=2)\n",
        "asx_pca = pca.fit_transform(scaled_features)\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(asx_pca)\n",
        "asx_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Model Preparation and Evaluation\n",
        "features = asx_df[['SMA_50', 'RSI_14', 'Volume', 'Max Drawdown', 'Returns']]\n",
        "labels = asx_df['KMeans_Cluster']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(X_resampled, y_resampled)\n",
        "scores = cross_val_score(rf_classifier, X_resampled, y_resampled, cv=5)\n",
        "print(f\"Cross-validated scores: {scores.mean()}\")\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Visualization of PCA-transformed and Original Features\n",
        "plt.figure(figsize=(30, 18))\n",
        "cluster_labels_named_asx = pd.Series(kmeans.labels_).map(cluster_names)\n",
        "sns.scatterplot(x=asx_pca[:, 0], y=asx_pca[:, 1], hue=cluster_labels_named_asx, palette='viridis')\n",
        "plt.title('PCA of ASX Data with Cluster Names')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "plt.figure(figsize=(30, 18))\n",
        "sns.scatterplot(x=asx_df.index, y=asx_df['Max Drawdown'], hue=asx_df['KMeans_Cluster'].map(cluster_names), palette='viridis')\n",
        "plt.title('K-Means Clustering Results on ASX Data with Cluster Names')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# Feature Importance Visualization\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(feature_importances)[::-1]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Feature Importances in Random Forest Classifier')\n",
        "plt.bar(range(features.shape[1]), feature_importances[indices], align='center')\n",
        "plt.xticks(range(features.shape[1]), features.columns[indices], rotation=45)\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# Advanced Analysis: Sub-cluster Analysis and Clustering Stability\n",
        "for cluster_label in np.unique(kmeans.labels_):\n",
        "    cluster_data = asx_pca[kmeans.labels_ == cluster_label]\n",
        "    sub_kmeans = KMeans(n_clusters=2, random_state=0).fit(cluster_data)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=cluster_data[:, 0], y=cluster_data[:, 1], hue=sub_kmeans.labels_, palette='viridis')\n",
        "    plt.title(f'Sub-clusters within Cluster {cluster_label}')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.legend(title='Sub-cluster')\n",
        "    plt.show()\n",
        "\n",
        "print()\n",
        "# Evaluating the performance of the classification model using confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# Calculating and printing F1 Score, Precision, and Recall\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Clustering Quality Metrics\n",
        "silhouette_avg = silhouette_score(asx_pca, kmeans.labels_)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "calinski_harabasz = calinski_harabasz_score(asx_pca, kmeans.labels_)\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
        "\n",
        "davies_bouldin = davies_bouldin_score(asx_pca, kmeans.labels_)\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVfeYBm56TzH"
      },
      "outputs": [],
      "source": [
        "print(sp500_df.groupby('KMeans_Cluster').describe())\n",
        "print(asx_df.groupby('KMeans_Cluster').describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq9mtb4cw87y"
      },
      "source": [
        "Find Correlation betweeen Factors and Cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouLeAHbJ1pzY"
      },
      "outputs": [],
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df['date']).dt.normalize()\n",
        "final_df.set_index('Date', inplace=True)\n",
        "\n",
        "all_dates = pd.date_range(start=min(final_df.index.min(), sp500_df.index.min(), asx_df.index.min()),\n",
        "                          end=max(final_df.index.max(), sp500_df.index.max(), asx_df.index.max()),\n",
        "                          freq='D').normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyrz6-HFuiuP"
      },
      "outputs": [],
      "source": [
        "# Convert the index for each DataFrame to date format without time\n",
        "asx_df.index = pd.to_datetime(asx_df.index).date\n",
        "sp500_df.index = pd.to_datetime(sp500_df.index).date\n",
        "final_df.index = pd.to_datetime(final_df.index).date\n",
        "\n",
        "print(asx_df.index)\n",
        "print(sp500_df.index)\n",
        "print(final_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWMHt434txSr"
      },
      "outputs": [],
      "source": [
        "# Loop for ASX data\n",
        "for date in asx_df.index:\n",
        "    if date in final_df.index:\n",
        "        variable_data = final_df.loc[date]\n",
        "        print(f\"Data on {date}:\")\n",
        "        print(variable_data)\n",
        "        asx_close_price = asx_df.loc[date, 'Close']\n",
        "        print(f\"ASX Close Price on {date}: {asx_close_price}\")\n",
        "    else:\n",
        "        print(f\"No data available for {date} in the variables dataset.\")\n",
        "\n",
        "# Loop for S&P 500 data\n",
        "for date in sp500_df.index:\n",
        "    if date in final_df.index:\n",
        "        variable_data = final_df.loc[date]\n",
        "        print(f\"Data on {date}:\")\n",
        "        print(variable_data)\n",
        "        sp500_close_price = sp500_df.loc[date, 'Close']\n",
        "        print(f\"S&P 500 Close Price on {date}: {sp500_close_price}\")\n",
        "    else:\n",
        "        print(f\"No data available for {date} in the variables dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHPCVw229mJT"
      },
      "outputs": [],
      "source": [
        "sp500_df['Cluster Name'] = sp500_df['KMeans_Cluster'].map(cluster_names)\n",
        "\n",
        "cluster_summary = pd.DataFrame()\n",
        "\n",
        "for cluster in sp500_df['KMeans_Cluster'].unique():\n",
        "\n",
        "    cluster_dates = sp500_df[sp500_df['KMeans_Cluster'] == cluster].index.intersection(final_df.index)\n",
        "\n",
        "    if not cluster_dates.empty:\n",
        "        cluster_data = final_df.loc[cluster_dates]\n",
        "        cluster_summary[f'Cluster {cluster} Mean'] = cluster_data.mean()\n",
        "\n",
        "\n",
        "cluster_summary = cluster_summary.T\n",
        "\n",
        "print(cluster_summary)\n",
        "print(cluster_names)\n",
        "\n",
        "if not cluster_summary.empty:\n",
        "    cluster_summary.plot(kind='bar', figsize=(42, 24))\n",
        "    plt.title('Variable Mean Values by Cluster')\n",
        "    plt.ylabel('Mean Value')\n",
        "    plt.xlabel('Cluster')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(title='Variable')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsbZH25T-uDE"
      },
      "outputs": [],
      "source": [
        "asx_df['Cluster Name'] = asx_df['KMeans_Cluster'].map(cluster_names)\n",
        "\n",
        "asx_cluster_summary = pd.DataFrame()\n",
        "\n",
        "for cluster in asx_df['KMeans_Cluster'].unique():\n",
        "\n",
        "    cluster_dates = asx_df[asx_df['KMeans_Cluster'] == cluster].index.intersection(final_df.index)\n",
        "\n",
        "    if not cluster_dates.empty:\n",
        "\n",
        "        cluster_data = final_df.loc[cluster_dates]\n",
        "\n",
        "        asx_cluster_summary[f'Cluster {cluster} Mean'] = cluster_data.mean()\n",
        "\n",
        "\n",
        "asx_cluster_summary = asx_cluster_summary.T\n",
        "\n",
        "print(asx_cluster_summary)\n",
        "print(cluster_names)\n",
        "\n",
        "if not asx_cluster_summary.empty:\n",
        "    asx_cluster_summary.plot(kind='bar', figsize=(42, 24))\n",
        "    plt.title('Variable Mean Values by ASX Cluster')\n",
        "    plt.ylabel('Mean Value')\n",
        "    plt.xlabel('Cluster')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(title='Variable')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVHNLGU7Y4tJ"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame for combined data\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop for ASX data\n",
        "for date in asx_df.index:\n",
        "    if date in final_df.index:\n",
        "        variable_data = final_df.loc[date]\n",
        "        asx_close_price = asx_df.loc[date, 'Close']\n",
        "        # Handle both Series and DataFrame cases\n",
        "        if isinstance(variable_data, pd.Series):\n",
        "            # For a Series, convert it to DataFrame format\n",
        "            data_row = variable_data.to_frame().T\n",
        "        else:\n",
        "            # If it's already a DataFrame, use it directly\n",
        "            data_row = variable_data\n",
        "        data_row['ASX_Close'] = asx_close_price\n",
        "        # Append to combined_df\n",
        "        combined_df = pd.concat([combined_df, data_row])\n",
        "\n",
        "# Repeat the process for S&P 500 data\n",
        "for date in sp500_df.index:\n",
        "    if date in final_df.index:\n",
        "        variable_data = final_df.loc[date]\n",
        "        sp500_close_price = sp500_df.loc[date, 'Close']\n",
        "        if isinstance(variable_data, pd.Series):\n",
        "            data_row = variable_data.to_frame().T\n",
        "        else:\n",
        "            data_row = variable_data\n",
        "        data_row['SP500_Close'] = sp500_close_price\n",
        "        combined_df = pd.concat([combined_df, data_row])\n",
        "\n",
        "# Reset index if needed and handle any additional cleanup\n",
        "combined_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Show the combined DataFrame\n",
        "print(combined_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZHxzvrmaJTU"
      },
      "outputs": [],
      "source": [
        "# Replace NaN values with 0\n",
        "combined_df.fillna(0, inplace=True)\n",
        "\n",
        "# Example of further analysis: Compute basic statistics\n",
        "basic_stats = combined_df.describe()\n",
        "\n",
        "# Compute correlations between variables and the ASX & S&P 500 close prices\n",
        "correlation_matrix = combined_df.corr()\n",
        "\n",
        "# Display basic statistics\n",
        "print(basic_stats)\n",
        "\n",
        "# Display correlation matrix\n",
        "print(correlation_matrix)\n",
        "\n",
        "plt.figure(figsize=(100, 80))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kpxShnp2FWU"
      },
      "source": [
        "Clusters Internal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylVpyS8rd-dm"
      },
      "outputs": [],
      "source": [
        "def check_and_remove_duplicates(df):\n",
        "    if df.index.duplicated().any():\n",
        "        print(f\"{df.name} contains duplicate indices.\")\n",
        "        df = df[~df.index.duplicated(keep='first')]\n",
        "    else:\n",
        "        print(f\"{df.name} indices are unique.\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUWxROgld-ob"
      },
      "outputs": [],
      "source": [
        "def safe_merge_dfs(df_list, rename_columns):\n",
        "    combined_df = pd.concat(df_list, axis=1).rename(columns=rename_columns)\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LymtpSEdd-yv"
      },
      "outputs": [],
      "source": [
        "def check_for_column(df, column_name):\n",
        "    if column_name in df.columns:\n",
        "        print(f\"'{column_name}' column exists in the DataFrame.\")\n",
        "    else:\n",
        "        print(f\"'{column_name}' column does not exist in the DataFrame.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvHRgISKd-6E"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(df):\n",
        "    df['30d_ma'] = df['Close'].rolling(window=30).mean()\n",
        "    df['Volume_30d_ma'] = df['Volume'].rolling(window=30).mean()\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['30d_vol'] = df['Returns'].rolling(window=30).std() * (252 ** 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAPkzxKreFyI"
      },
      "outputs": [],
      "source": [
        "def plot_correlation_matrix(df):\n",
        "    correlation_matrix = df.corr()\n",
        "    plt.figure(figsize=(100, 80))\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix of Financial Data')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6rYLeveIxC"
      },
      "outputs": [],
      "source": [
        "final_df.name = 'final_df'\n",
        "asx_df.name = 'asx_df'\n",
        "sp500_df.name = 'sp500_df'\n",
        "\n",
        "final_df = check_and_remove_duplicates(final_df)\n",
        "asx_df = check_and_remove_duplicates(asx_df)\n",
        "sp500_df = check_and_remove_duplicates(sp500_df)\n",
        "\n",
        "# Merge data safely\n",
        "combined_df = safe_merge_dfs(\n",
        "    [final_df, asx_df[['Close']].rename(columns={'Close': 'ASX_Close'}), sp500_df[['Close']].rename(columns={'Close': 'SP500_Close'})],\n",
        "    {'Close': 'ASX_Close', 'Close': 'SP500_Close'}\n",
        ")\n",
        "\n",
        "# Check for 'Indicator'\n",
        "check_for_column(df, 'Indicator')\n",
        "\n",
        "# Calculate metrics\n",
        "calculate_metrics(asx_df)\n",
        "calculate_metrics(sp500_df)\n",
        "\n",
        "# Update combined_df for correlation analysis\n",
        "combined_df = pd.concat([final_df, asx_df[['Close']].rename(columns={'Close': 'ASX_Close'}), sp500_df[['Close']].rename(columns={'Close': 'SP500_Close'})], axis=1)\n",
        "\n",
        "# Plot correlation matrix\n",
        "plot_correlation_matrix(combined_df)\n",
        "\n",
        "# Statistical analysis of clusters\n",
        "for cluster in sp500_df['KMeans_Cluster'].unique():\n",
        "    valid_indices = sp500_df[sp500_df['KMeans_Cluster'] == cluster].index.intersection(final_df.index)\n",
        "    cluster_data = final_df.loc[valid_indices]\n",
        "    if not cluster_data.empty:\n",
        "        print(f\"Cluster {cluster} Summary Statistics:\")\n",
        "        display(cluster_data.describe())\n",
        "\n",
        "# Visualization of key metrics across clusters\n",
        "variables = final_df.columns\n",
        "if 'KMeans_Cluster' in final_df.columns:\n",
        "    variables = final_df.columns.drop('KMeans_Cluster')\n",
        "else:\n",
        "    variables = final_df.columns\n",
        "\n",
        "for variable in variables:\n",
        "    if np.issubdtype(final_df[variable].dtype, np.number):\n",
        "        plt.figure(figsize=(50, 35))\n",
        "        for cluster in sp500_df['KMeans_Cluster'].unique():\n",
        "            valid_indices = sp500_df[sp500_df['KMeans_Cluster'] == cluster].index.intersection(final_df.index)\n",
        "            cluster_data = final_df.loc[valid_indices]\n",
        "            if not cluster_data.empty and variable in cluster_data.columns:\n",
        "                cluster_data[variable].plot(kind='hist', alpha=0.5, label=f\"Cluster {cluster}\", bins=20)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.title(f'Distribution of {variable} Across Clusters')\n",
        "        plt.xlabel(variable)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "# Time series analysis of S&P 500 Close Price\n",
        "sp500_df['Close'].plot(figsize=(14, 7), label='Actual Close Price')\n",
        "sp500_df['Close'].rolling(window=30).mean().plot(label='30-Day Moving Average')\n",
        "plt.legend()\n",
        "plt.title('S&P 500 Close Price with 30-Day Moving Average')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOVmgNRN2sg"
      },
      "source": [
        "Model Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ldIN7sBXkdM"
      },
      "outputs": [],
      "source": [
        "if 'KMeans_Cluster' not in combined_df.columns and 'KMeans_Cluster' in sp500_df.columns:\n",
        "    combined_df = pd.concat([combined_df, sp500_df[['KMeans_Cluster']]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVV0aAI9N2B6"
      },
      "outputs": [],
      "source": [
        "combined_df.fillna(combined_df.median(), inplace=True)\n",
        "\n",
        "numeric_features = combined_df.select_dtypes(include=[np.number])\n",
        "\n",
        "X = numeric_features.drop(['KMeans_Cluster'], axis=1)\n",
        "y = combined_df['KMeans_Cluster']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "predictions = svm_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epi0-wRbN3T1"
      },
      "outputs": [],
      "source": [
        "logging.getLogger('sklearn').setLevel(logging.INFO)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1],\n",
        "    'gamma': [1],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best score: {grid_search.best_score_}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "predictions = best_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "scores = cross_val_score(xgb, X_test_scaled, y_test, cv=5)\n",
        "print(\"XGBoost average accuracy:\", scores.mean())"
      ],
      "metadata": {
        "id": "ht_lAwyk8tq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_xgb(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 100, 600)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.5)\n",
        "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
        "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
        "\n",
        "    xgb = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                        learning_rate=learning_rate, subsample=subsample,\n",
        "                        colsample_bytree=colsample_bytree, use_label_encoder=False,\n",
        "                        eval_metric='mlogloss', random_state=42)\n",
        "    xgb.fit(X_train_scaled, y_train)\n",
        "    score = cross_val_score(xgb, X_train_scaled, y_train, n_jobs=-1, cv=3).mean()\n",
        "    return score"
      ],
      "metadata": {
        "id": "k18dOqlnZ5oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(optimize_xgb, n_trials=50)\n",
        "\n",
        "best_xgb_params = study_xgb.best_trial.params\n",
        "xgb_best = XGBClassifier(**best_xgb_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_best.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "Odesknj19lhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "    ('svm', SVC(C=1.0, gamma='auto', probability=True)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))\n",
        "]\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "stacked_predictions = stacking_clf.predict(X_test_scaled)\n",
        "print(\"Stacking Model Accuracy:\", accuracy_score(y_test, stacked_predictions))\n",
        "print(\"Stacking Model Classification Report:\\n\", classification_report(y_test, stacked_predictions))"
      ],
      "metadata": {
        "id": "FmcNet7K8ttb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_lr(trial):\n",
        "    C = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
        "\n",
        "    stacking_clf = StackingClassifier(\n",
        "        estimators=estimators,\n",
        "        final_estimator=LogisticRegression(C=C, random_state=42)\n",
        "    )\n",
        "    stacking_clf.fit(X_train_scaled, y_train)\n",
        "    score = cross_val_score(stacking_clf, X_train_scaled, y_train, n_jobs=-1, cv=3).mean()\n",
        "    return score"
      ],
      "metadata": {
        "id": "ZYUwRCFnZ5rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study_lr = optuna.create_study(direction='maximize')\n",
        "study_lr.optimize(optimize_lr, n_trials=50)\n",
        "\n",
        "best_lr_params = study_lr.best_trial.params\n",
        "final_estimator_best = LogisticRegression(**best_lr_params, random_state=42)\n",
        "\n",
        "stacking_clf_best = StackingClassifier(estimators=estimators, final_estimator=final_estimator_best)\n",
        "stacking_clf_best.fit(X_train_scaled, y_train)\n",
        "stacked_predictions_best = stacking_clf_best.predict(X_test_scaled)\n",
        "\n",
        "print(\"Optimized Stacking Model Accuracy:\", accuracy_score(y_test, stacked_predictions_best))\n",
        "print(\"Optimized Stacking Model Classification Report:\\n\", classification_report(y_test, stacked_predictions_best))"
      ],
      "metadata": {
        "id": "EGo70Oyy9tln"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ6rA4uIRZAbVmF+jHmghk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}