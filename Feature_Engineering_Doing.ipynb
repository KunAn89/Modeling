{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2EMcdSzBZ0HEd2pBZ3aVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunan-au/Modeling_Risk/blob/main/Feature_Engineering_Doing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeking Factors"
      ],
      "metadata": {
        "id": "c85nnOIPlrUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimize"
      ],
      "metadata": {
        "id": "bKGsYMVnl0YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model using\n"
      ],
      "metadata": {
        "id": "msC2jkxBl9kX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7sEBgYVqb4u"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "HHhVbfculx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package"
      ],
      "metadata": {
        "id": "JgNoGJajcogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas matplotlib scipy statsmodels\n",
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "pqCLZQm_KYJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "!pip install TA-Lib"
      ],
      "metadata": {
        "id": "yXTl9rNVmcEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import gspread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import talib\n",
        "import yfinance as yf\n",
        "import getpass\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from google.colab import auth\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "H8Gb1G7pKBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Importing"
      ],
      "metadata": {
        "id": "HOd4MNtHtviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    # Print columns to debug\n",
        "    print(\"Columns in DataFrame:\", df.columns)\n",
        "\n",
        "    # Check if 'Date' column exists\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'].str.split().str[0], format='%m/%d/%Y').dt.date\n",
        "    else:\n",
        "        print(f\"'Date' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"'{col}' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names = [\"MSFT\", \"META\", \"NFLX\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data_updated(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # The first row is the header\n",
        "    header = data[0]\n",
        "    # The first column 'Year' has no header, add it manually\n",
        "    header[0] = 'Year'\n",
        "\n",
        "    df = pd.DataFrame(data[1:], columns=header)\n",
        "\n",
        "    # Convert 'Year' to datetime format\n",
        "    if 'Annual' in spreadsheet_name:\n",
        "        # Annual data, set 'Year' as the end of each year to forward fill\n",
        "        df['Year'] = pd.to_datetime(df['Year']+'-12-31')\n",
        "    else:\n",
        "        # Monthly data\n",
        "        df['Year'] = pd.to_datetime(df['Year'], format='%Y%m')\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "LknMEXnxjzDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "sheet_name_factors = ['F-F_Research_Data_5_Annual_Factors_2x3', 'F-F_Research_Data_5_Month_Factors_2x3']\n",
        "factorframes = [get_sheet_data_updated(name) for name in sheet_name_factors]\n",
        "\n",
        "# Ensure 'Year' column is datetime for both dataframes\n",
        "factorframes[0]['Year'] = pd.to_datetime(factorframes[0]['Year'], format='%Y')\n",
        "factorframes[1]['Year'] = pd.to_datetime(factorframes[1]['Year'], format='%Y%m')\n",
        "\n",
        "# Set the index to 'Year' and sort it\n",
        "annual_data = factorframes[0].set_index('Year').sort_index()\n",
        "monthly_data = factorframes[1].set_index('Year').sort_index()\n",
        "\n",
        "# Resample annual data to monthly, filling forward\n",
        "# Note: The annual data is on the last day of the year, so we need to shift it to the first day\n",
        "annual_data = annual_data.resample('MS').ffill()\n",
        "\n",
        "# Combine the data\n",
        "combined_data = pd.concat([monthly_data, annual_data]).sort_index().fillna(method='ffill')\n",
        "\n",
        "# Reset the index if needed\n",
        "combined_data = combined_data.reset_index()\n",
        "\n",
        "# Print combined data to check\n",
        "print(combined_data.head())"
      ],
      "metadata": {
        "id": "yCWgA63PcLIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic"
      ],
      "metadata": {
        "id": "gY19Oxb-kjw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Price and Volume Plot for all DataFrames\n",
        "for df, name in zip(dataframes, sheet_names):\n",
        "    # Price and Volume Plot\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(df.index, df['Volume'], color='orange')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'Daily Closing Prices for {name}')\n",
        "    print(f'Daily Volume for {name}')\n",
        "\n",
        "    # Moving Average Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['MA30'] = df['Close'].rolling(window=30).mean()\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.plot(df['MA5'], label='5-Day MA')\n",
        "    plt.plot(df['MA30'], label='30-Day MA')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f'Moving Averages for {name}')\n",
        "\n",
        "    # Price Fluctuation Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    plt.plot(df['Price_Range'])\n",
        "    plt.show()\n",
        "    print(f'Price Fluctuation for {name}')\n",
        "\n",
        "    # Momentum Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
        "    plt.plot(df['Momentum'])\n",
        "    plt.show()\n",
        "    print(f'Momentum for {name}')\n",
        "\n",
        "    # Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation = df.corr()\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.show()\n",
        "    print(f'Correlation Heatmap for {name}')"
      ],
      "metadata": {
        "id": "mYrD47uIZuTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate daily returns for each asset\n",
        "returns_dict = {}\n",
        "\n",
        "for asset_name, df in zip(sheet_names, dataframes):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    returns_dict[asset_name] = df[['Date', 'Returns']].copy()\n",
        "\n",
        "# Print the returns for each asset\n",
        "for asset_name, returns_df in returns_dict.items():\n",
        "    print(f\"Asset: {asset_name}\")\n",
        "    print(returns_df.head())\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "k1WW9k4Sc6Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Br_AlYcDK0Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_alpha_vantage_data(symbol, api_key, function=\"TIME_SERIES_DAILY\"):\n",
        "    base_url = \"https://www.alphavantage.co/query\"\n",
        "    params = {\n",
        "        \"function\": function,\n",
        "        \"symbol\": symbol,\n",
        "        \"apikey\": api_key,\n",
        "        \"datatype\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data"
      ],
      "metadata": {
        "id": "hIkSEr4rK0UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_alpha_vantage_data(data):\n",
        "    df = pd.DataFrame(data['Time Series (Daily)']).T\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df.apply(pd.to_numeric)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    return df"
      ],
      "metadata": {
        "id": "9wVeGx--K0XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass.getpass(\"Please enter Alpha Vantage API key:\")\n",
        "symbol = \"MSFT\", \"META\", \"NFLX\"\n",
        "\n",
        "raw_data = get_alpha_vantage_data(symbol, api_key)\n",
        "print(raw_data)\n",
        "df = process_alpha_vantage_data(raw_data)"
      ],
      "metadata": {
        "id": "M67vj_NsK8gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ],
      "metadata": {
        "id": "vMIaHuYvPPpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "print(sp500_df.head())\n",
        "\n",
        "market_returns = sp500_df['Close'].pct_change()"
      ],
      "metadata": {
        "id": "gSK_oBPIJ78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADV"
      ],
      "metadata": {
        "id": "paZFyrmJklWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_risk_factors(start_date, end_date):\n",
        "    try:\n",
        "        ff_factors = yf.download(\"F-F_Research_Data_Factors_daily\", start=start_date, end=end_date)\n",
        "\n",
        "        if len(ff_factors.columns) == 4:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate']\n",
        "        elif len(ff_factors.columns) == 6:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate', 'Column5', 'Column6']\n",
        "\n",
        "        ff_factors = ff_factors / 100\n",
        "\n",
        "        return ff_factors\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "0d8VTHQwz7Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empty Sheet"
      ],
      "metadata": {
        "id": "uBuWMyi1cuAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "risk_factors = get_risk_factors(start_date, end_date)\n",
        "\n",
        "if risk_factors is not None:\n",
        "    print(risk_factors.head())\n",
        "else:\n",
        "    print(\"Failed to download risk factors.\")"
      ],
      "metadata": {
        "id": "vBZsevDkz_7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_asset_returns(df):\n",
        "    asset_returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    return asset_returns"
      ],
      "metadata": {
        "id": "uB0oLHVO1liZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_volatility(df, window=20):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    volatility = returns.rolling(window=window).std()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(volatility, label='Volatility')\n",
        "    plt.title('Volatility Factor')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A-se7onuZuWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_alpha_and_beta(asset_returns, market_returns):\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(asset_returns, market_returns)\n",
        "    alpha = intercept\n",
        "    beta = slope\n",
        "    return alpha, beta"
      ],
      "metadata": {
        "id": "YxVXh5ZeZuY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_sharpe_ratio(df, risk_free_rate=0.02):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    volatility = returns.std()\n",
        "    sharpe_ratio = (mean_return - risk_free_rate) / volatility\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(returns, label='Asset Returns')\n",
        "    plt.title('Asset Returns')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Sharpe Ratio: {sharpe_ratio}')"
      ],
      "metadata": {
        "id": "Og9boVWNZubs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_r_squared(asset_returns, market_returns):\n",
        "    r_squared = np.corrcoef(asset_returns, market_returns)[0, 1] ** 2\n",
        "    print(f'R-Squared Value: {r_squared}')"
      ],
      "metadata": {
        "id": "d60nHo-zZud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "    elif indicator == 'RSI':\n",
        "        delta = df['Close'].diff(1)\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=window).mean()\n",
        "        avg_loss = loss.rolling(window=window).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        df['RSI'] = rsi\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        df['MA'] = df['Close'].rolling(window=window).mean()\n",
        "        df['Upper_Band'] = df['MA'] + 2 * df['Close'].rolling(window=window).std()\n",
        "        df['Lower_Band'] = df['MA'] - 2 * df['Close'].rolling(window=window).std()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if indicator == 'MA':\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "    elif indicator == 'RSI':\n",
        "        plt.plot(df['RSI'], label='RSI')\n",
        "        plt.title('Relative Strength Index (RSI)')\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        plt.plot(df['Close'], label='Close Price', alpha=0.5)\n",
        "        plt.plot(df['Upper_Band'], label='Upper Bollinger Band')\n",
        "        plt.plot(df['Lower_Band'], label='Lower Bollinger Band')\n",
        "        plt.title('Bollinger Bands')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M68oYaK_kKaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def factor_model_analysis(asset_returns, market_returns, risk_factors):\n",
        "    # Prepare the risk factors DataFrame with a constant for the OLS regression\n",
        "    X = sm.add_constant(risk_factors)\n",
        "\n",
        "    # Run the OLS regression\n",
        "    model = sm.OLS(asset_returns, X).fit()\n",
        "\n",
        "    # Print the summary of the regression\n",
        "    print(model.summary())\n",
        "\n",
        "    # Visualize the factor loadings\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(X.columns[1:], model.params[1:])  # Skip the constant\n",
        "    plt.xlabel('Risk Factors')\n",
        "    plt.ylabel('Factor Loadings')\n",
        "    plt.title('Factor Loadings for Asset Returns')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LHDfT067kQ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "        plt.legend()"
      ],
      "metadata": {
        "id": "lv_vsZ6GCaNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in dataframes:\n",
        "    asset_returns = calculate_asset_returns(df)\n",
        "    calculate_and_visualize_volatility(df)\n",
        "    calculate_and_visualize_sharpe_ratio(df)\n",
        "\n",
        "    alpha, beta = calculate_alpha_and_beta(asset_returns, market_returns)\n",
        "    print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
        "\n",
        "    calculate_and_visualize_r_squared(asset_returns, market_returns)\n",
        "    calculate_and_visualize_technical_indicator(df, 'MA')\n",
        "\n",
        "    # Align asset_returns with risk_factors\n",
        "    if 'risk_factors' not in df.columns:\n",
        "        print(\"Risk factors are not in the DataFrame\")\n",
        "        continue\n",
        "\n",
        "    risk_factors_aligned = risk_factors.loc[asset_returns.index]\n",
        "\n",
        "    # Drop missing values\n",
        "    asset_returns.dropna(inplace=True)\n",
        "    risk_factors_aligned.dropna(inplace=True)\n",
        "\n",
        "    # Ensure all inputs are aligned by index\n",
        "    common_dates = asset_returns.index.intersection(risk_factors_aligned.index)\n",
        "    asset_returns_aligned = asset_returns.loc[common_dates]\n",
        "    risk_factors_aligned = risk_factors_aligned.loc[common_dates]\n",
        "\n",
        "    # Run the factor model analysis\n",
        "    factor_model_analysis(asset_returns_aligned, market_returns.loc[common_dates], risk_factors_aligned)\n",
        "\n",
        "    # Print the head of the dataframe to check\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "nohcZAqWEMNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TWO Stage"
      ],
      "metadata": {
        "id": "CHBAe4ApesTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ],
      "metadata": {
        "id": "q4BPnKRcesdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ],
      "metadata": {
        "id": "Thm31nbyLi0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['KMeans_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by K-Means Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "linked = linkage(sp500_df[['Max Drawdown']], 'single')\n",
        "\n",
        "plt.figure(figsize=(50, 35))\n",
        "dendrogram(linked, orientation='top', labels=sp500_df.index.astype(str), distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "db = DBSCAN(eps=0.001, min_samples=3).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['DBSCAN_Cluster'] = db.labels_\n",
        "\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['DBSCAN_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by DBSCAN Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "IoS1aetKKKYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get S&P 500 data\n",
        "def get_sp500_data(start_date, end_date):\n",
        "    return yf.download('^GSPC', start=start_date, end=end_date)"
      ],
      "metadata": {
        "id": "HLu4npUveshu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate max drawdown\n",
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ],
      "metadata": {
        "id": "a3f7iTAULdmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding technical indicators\n",
        "def add_technical_indicators(df):\n",
        "    df['SMA_50'] = talib.SMA(df['Close'], timeperiod=50)\n",
        "    df['RSI_14'] = talib.RSI(df['Close'], timeperiod=14)\n",
        "    df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "qh1-Djo4LdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "sp500_df = get_sp500_data('2000-01-01', '2022-12-30')\n",
        "\n",
        "# Calculate returns and max drawdown\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "# Add technical indicators\n",
        "add_technical_indicators(sp500_df)\n",
        "\n",
        "# Feature scaling with RobustScaler to mitigate the effect of outliers\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_features = robust_scaler.fit_transform(sp500_df[['Max Drawdown', 'Returns']])\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "sp500_pca = pca.fit_transform(scaled_features)\n",
        "\n",
        "# K-Means clustering on PCA-transformed data\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_pca)\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Prepare features and labels for the classification model\n",
        "features = sp500_df[['SMA_50', 'RSI_14', 'Volume', 'Max Drawdown', 'Returns']]\n",
        "labels = sp500_df['KMeans_Cluster']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# Oversampling with SMOTE to balance the classes\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Cross-validated score\n",
        "scores = cross_val_score(rf_classifier, X_resampled, y_resampled, cv=5)\n",
        "print(f\"Cross-validated scores: {scores.mean()}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Classification report and accuracy\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Visualization of PCA-transformed features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_pca[:, 0], y=sp500_pca[:, 1], hue=kmeans.labels_, palette='viridis')\n",
        "plt.title('PCA of S&P 500 Data')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Visualization of clustering results on original features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_df.index, y=sp500_df['Max Drawdown'], hue=sp500_df['KMeans_Cluster'], palette='viridis')\n",
        "plt.title('K-Means Clustering Results on S&P 500 Data')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pwUXgbNWKbV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}