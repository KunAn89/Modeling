{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunan-au/Modeling_Risk/blob/main/Feature_Engineering_Doing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factor Timing"
      ],
      "metadata": {
        "id": "nU0FSSaYGb0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GtnkwHlEG1wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data From:\n",
        "\n",
        "https://www.rba.gov.au/"
      ],
      "metadata": {
        "id": "t414_umhGehk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7sEBgYVqb4u"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHhVbfculx93"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgNoGJajcogv"
      },
      "source": [
        "Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNMh69_UTEfL"
      },
      "outputs": [],
      "source": [
        "!pip show gspread\n",
        "!pip install --upgrade gspread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqCLZQm_KYJK"
      },
      "outputs": [],
      "source": [
        "!pip install requests pandas matplotlib scipy statsmodels\n",
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXTl9rNVmcEd"
      },
      "outputs": [],
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "!pip install TA-Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Gb1G7pKBeU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import gspread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import talib\n",
        "import yfinance as yf\n",
        "import getpass\n",
        "import re\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from oauth2client.service_account import ServiceAccountCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOd4MNtHtviy"
      },
      "source": [
        "Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "outputs": [],
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "outputs": [],
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    # Print columns to debug\n",
        "    print(\"Columns in DataFrame:\", df.columns)\n",
        "\n",
        "    # Check if 'Date' column exists\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'].str.split().str[0], format='%m/%d/%Y').dt.date\n",
        "    else:\n",
        "        print(f\"'Date' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"'{col}' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "outputs": [],
      "source": [
        "sheet_names = [\"MSFT\", \"META\", \"NFLX\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LknMEXnxjzDZ"
      },
      "outputs": [],
      "source": [
        "def get_sheet_data_updated(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # The first row is the header\n",
        "    header = data[0]\n",
        "    # The first column 'Year' has no header, add it manually\n",
        "    header[0] = 'Year'\n",
        "\n",
        "    df = pd.DataFrame(data[1:], columns=header)\n",
        "\n",
        "    # Convert 'Year' to datetime format\n",
        "    if 'Annual' in spreadsheet_name:\n",
        "        # Annual data, set 'Year' as the end of each year to forward fill\n",
        "        df['Year'] = pd.to_datetime(df['Year']+'-12-31')\n",
        "    else:\n",
        "        # Monthly data\n",
        "        df['Year'] = pd.to_datetime(df['Year'], format='%Y%m')\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCWgA63PcLIA"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "sheet_name_factors = ['F-F_Research_Data_5_Annual_Factors_2x3', 'F-F_Research_Data_5_Month_Factors_2x3']\n",
        "factorframes = [get_sheet_data_updated(name) for name in sheet_name_factors]\n",
        "\n",
        "# Ensure 'Year' column is datetime for both dataframes\n",
        "factorframes[0]['Year'] = pd.to_datetime(factorframes[0]['Year'], format='%Y')\n",
        "factorframes[1]['Year'] = pd.to_datetime(factorframes[1]['Year'], format='%Y%m')\n",
        "\n",
        "# Set the index to 'Year' and sort it\n",
        "annual_data = factorframes[0].set_index('Year').sort_index()\n",
        "monthly_data = factorframes[1].set_index('Year').sort_index()\n",
        "\n",
        "# Resample annual data to monthly, filling forward\n",
        "# Note: The annual data is on the last day of the year, so we need to shift it to the first day\n",
        "annual_data = annual_data.resample('MS').ffill()\n",
        "\n",
        "# Combine the data\n",
        "combined_data = pd.concat([monthly_data, annual_data]).sort_index().fillna(method='ffill')\n",
        "\n",
        "# Reset the index if needed\n",
        "combined_data = combined_data.reset_index()\n",
        "\n",
        "# Print combined data to check\n",
        "print(combined_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revelant Factors Read\n"
      ],
      "metadata": {
        "id": "vp39FS_EVO0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iR5bFie3CjE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_rba_balance_sheet_from_google_sheets(spreadsheet_name, worksheet_index=0):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Get variable names from the second row, starting from the second column\n",
        "    variables = data[1][1:]  # Exclude the first cell\n",
        "    # Clean up the variable names\n",
        "    variables = [var.strip().replace(' ', '_').lower() for var in variables]\n",
        "\n",
        "    # Data starts from the 12th row, and includes the date in the first column\n",
        "    data_rows = data[11:]\n",
        "\n",
        "    # Create the DataFrame with the correct headers, excluding the first column which is the date\n",
        "    df = pd.DataFrame(data_rows, columns=['date'] + variables)\n",
        "\n",
        "    # Convert the 'date' column to datetime format\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    # Convert all other columns to numeric, coercing errors to NaN\n",
        "    for col in variables:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Create a separate plot for each variable\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(data=df, x='date', y=var)\n",
        "        plt.title(f'Trend of {var.replace(\"_\", \" \").title()} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{var.replace(\"_\", \" \").title()} ($ million)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()  # Adjust the layout to make room for the rotated x-axis labels\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "wSOPiM-0VSQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the actual name of your Google Sheet\n",
        "spreadsheet_name = 'A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET'\n",
        "visualize_rba_balance_sheet_from_google_sheets(spreadsheet_name)"
      ],
      "metadata": {
        "id": "K5aysvnhVSWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DdwOWNvTCkK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_rba_open_market_operations(spreadsheet_name, worksheet_index=0):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Define the column headers based on the provided names, starting from second column\n",
        "    headers = [\n",
        "        'cash_position',\n",
        "        'outright_transactions_australian_govt',\n",
        "        'outright_transactions_state_territory_govts',\n",
        "        'foreign_exchange_swaps',\n",
        "        'repurchase_agreements_general_collateral',\n",
        "        'repurchase_agreements_private_securities',\n",
        "        'exchange_settlement_account_balances',\n",
        "        'overnight_repurchase_agreements',\n",
        "        'market_value_securities_held_under_reverse_repo_cgs',\n",
        "        'market_value_securities_held_under_reverse_repo_semis',\n",
        "        'market_value_securities_held_under_reverse_repo_other_government_related',\n",
        "        'market_value_securities_held_under_reverse_repo_adi_issued',\n",
        "        'market_value_securities_held_under_reverse_repo_asset_backed_securities',\n",
        "        'market_value_securities_held_under_reverse_repo_other'\n",
        "    ]\n",
        "\n",
        "    # The dates are in the first column, data starts from row 13\n",
        "    df = pd.DataFrame(data[12:], columns=['date'] + headers)\n",
        "\n",
        "    # Convert the 'date' column to datetime format\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    # Convert all other columns to numeric, coercing errors to NaN\n",
        "    for col in headers:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Create a separate plot for each variable\n",
        "    for col in headers:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(data=df, x='date', y=col)\n",
        "        plt.title(f'Trend of {col.replace(\"_\", \" \").title()} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(col.replace(\"_\", \" \").title())\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "3EshnGF5VOk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the actual name of your Google Sheet\n",
        "spreadsheet_name = 'A3 Reserve Bank of Australia - Open Market Operations'\n",
        "visualize_rba_open_market_operations(spreadsheet_name)"
      ],
      "metadata": {
        "id": "TqRBgQ6YHNNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wir2FZD_ClzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_holdings_data(spreadsheet_name, worksheet_index=0):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Extract the date column and the specified data columns starting from row 12\n",
        "    date_column = [row[0] for row in data[11:]]  # Dates start at index 11 (12th row)\n",
        "    data_columns = list(zip(*data[11:]))  # Transpose rows to columns\n",
        "\n",
        "    # Column indices based on the spreadsheet structure\n",
        "    column_indices = {\n",
        "        \"Australian Government Securities and Semis Holdings Total\": 1,\n",
        "        \"AGS Total\": 2,\n",
        "        \"Semis Total\": 3,\n",
        "        \"Australian Government Bonds\": 5,\n",
        "        \"Australian Government T-Notes\": 6,\n",
        "        \"NSWTC Bonds\": 7,\n",
        "        \"TCV Bonds\": 8,\n",
        "        \"QTC Bonds\": 9,\n",
        "        \"WATC Bonds\": 10,\n",
        "        \"SAFA Bonds\": 11,\n",
        "        \"TASC Bonds\": 12,\n",
        "        \"NTTY Bonds\": 13,\n",
        "        \"ACTT Bonds\": 14\n",
        "    }\n",
        "\n",
        "    # Convert date column to datetime and initialize DataFrame with it\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(date_column)})\n",
        "\n",
        "    # Add each data column to the DataFrame and convert to numeric\n",
        "    for name, index in column_indices.items():\n",
        "        df[name] = pd.to_numeric(data_columns[index], errors='coerce')\n",
        "\n",
        "    # Plot each variable over time\n",
        "    for name in column_indices.keys():\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(x='date', y=name, data=df)\n",
        "        plt.title(f'{name} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{name} / $m')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "N53aUsM6Cgyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spreadsheet_name = 'A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS'\n",
        "visualize_holdings_data(spreadsheet_name)"
      ],
      "metadata": {
        "id": "EwD6ahK2Cg2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshape Data"
      ],
      "metadata": {
        "id": "TDImBTNhOyzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_visualize_data(spreadsheet_name, worksheet_index, column_indices, start_row):\n",
        "    # Open the spreadsheet and select the worksheet\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # Extract the date column and the specified data columns starting from the appropriate row\n",
        "    date_column = [row[0] for row in data[start_row-1:]]  # Adjust for zero-based index\n",
        "    data_columns = list(zip(*data[start_row-1:]))  # Transpose rows to columns\n",
        "\n",
        "    # Create a DataFrame with the date column converted to datetime\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(date_column, errors='coerce')})\n",
        "\n",
        "    # Populate the DataFrame with the variables and convert to numeric\n",
        "    for name, index in column_indices.items():\n",
        "        df[name] = pd.to_numeric(data_columns[index], errors='coerce')\n",
        "\n",
        "    # Perform statistical analysis and visualize each variable over time\n",
        "    for name in column_indices.keys():\n",
        "        # Perform statistical analysis\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(\"Mean:\", np.mean(df[name]))\n",
        "        print(\"Standard Deviation:\", np.std(df[name]))\n",
        "        print(\"Minimum:\", np.min(df[name]))\n",
        "        print(\"Maximum:\", np.max(df[name]))\n",
        "\n",
        "        # Visualize each variable\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(data=df, x='date', y=name)\n",
        "        plt.title(f'Trend of {name} over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(f'{name} ($ million)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "E2RIiI7JQtF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_sheet_indices = {\n",
        "    'cash_position': 1,\n",
        "    'exchange_settlement_account_balances': 6,\n",
        "    'overnight_repurchase_agreements': 7,\n",
        "    # Add more variables as needed\n",
        "}\n",
        "\n",
        "# Define column indices for the RBA open market operations\n",
        "open_market_operations_indices = {\n",
        "    'outright_transactions_australian_govt': 1,\n",
        "    'outright_transactions_state_territory_govts': 2,\n",
        "    'repurchase_agreements_general_collateral': 4,\n",
        "    # Add more variables as needed\n",
        "}\n",
        "\n",
        "# Define column indices for the holdings data\n",
        "holdings_data_indices = {\n",
        "    'Australian Government Securities and Semis Holdings Total': 1,\n",
        "    'AGS Total': 2,\n",
        "    'Semis Total': 3,\n",
        "    # Add more variables as needed\n",
        "}\n",
        "# Call the function for the RBA balance sheet\n",
        "df_balance_sheet = read_and_visualize_data(\n",
        "    'A3 Reserve Bank of Australia - Open Market Operations', 0, balance_sheet_indices, 12\n",
        ")\n",
        "\n",
        "# Call the function for the RBA open market operations\n",
        "df_open_market_operations = read_and_visualize_data(\n",
        "    'A3 Reserve Bank of Australia - Open Market Operations', 0, open_market_operations_indices, 13\n",
        ")\n",
        "\n",
        "# Call the function for the holdings data\n",
        "df_holdings_data = read_and_visualize_data(\n",
        "    'A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS', 0, holdings_data_indices, 12\n",
        ")"
      ],
      "metadata": {
        "id": "nbBUuM7mQybt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81nrx_fXpxO4"
      },
      "source": [
        "Genneral Function Failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FOGlWPKzD8P"
      },
      "outputs": [],
      "source": [
        "RBA_sheet_names = [\"A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET\", \"A2 RESERVE BANK OF AUSTRALIA – MONETARY POLICY CHANGES\", \"A3 Reserve Bank of Australia - Open Market Operations\"\n",
        "                  \"A3 Reserve Bank of Australia - Open Market Operations\",\"A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\"A3.2 SECURITIES LENDING REPURCHASE TRANSACTIONS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\n",
        "                   \"A4 RESERVE BANK OF AUSTRALIA – FOREIGN EXCHANGE TRANSACTIONS AND HOLDINGS OF OFFICIAL RESERVE ASSETS\",\"A5 RESERVE BANK OF AUSTRALIA - DAILY FOREIGN EXCHANGE MARKET INTERVENTION TRANSACTIONS\",\n",
        "                   \"A6 RESERVE BANK OF AUSTRALIA – BANKNOTES ON ISSUE BY DENOMINATION\",\"A7 RESERVE BANK OF AUSTRALIA – DETECTED AUSTRALIAN COUNTERFEITS BY DENOMINATION\",\"B1 ASSETS OF FINANCIAL INSTITUTIONS\",\n",
        "                   \"B2 BANKS – OFF-BALANCE SHEET BUSINESS\",\"B11.1 INTERNATIONAL ASSETS OF THE AUSTRALIAN-LOCATED OPERATIONS OF BANKS AND RFCs\",\"C1 Credit and Charge Cards – Seasonally Adjusted Series\",\n",
        "                   \"C1.1 Credit and Charge Cards – Original Series – Aggregate Data\",\"C1.3 Credit and Charge Cards – Market Shares of Card Schemes\",\"C2 Debit Cards – Seasonally Adjusted Series\",\n",
        "                   \"C2.1 Debit Cards – Original Series\",\"C2.2 Prepaid Cards – Original Series\",\"D1 GROWTH IN SELECTED FINANCIAL AGGREGATES\",\"F5 INDICATOR LENDING RATES\",\n",
        "                   \"F7 BUSINESS LENDING RATES\",\"F8 PERSONAL LENDING RATES\",\"F11.1 EXCHANGE RATES\",\"F13 INTERNATIONAL OFFICIAL INTEREST RATES\",\"G1 CONSUMER PRICE INFLATION\",\"H1 GROSS DOMESTIC PRODUCT AND INCOME\",\n",
        "                   \"I1 INTERNATIONAL TRADE AND BALANCE OF PAYMENTS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRIgtMJrMAJW"
      },
      "outputs": [],
      "source": [
        "def get_rba_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    numeric_columns = get_numeric_columns(data[1:])  # Skip header row\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pXPrktYMAL2"
      },
      "outputs": [],
      "source": [
        "def get_numeric_columns(data):\n",
        "    numeric_cols = []\n",
        "    for col in range(len(data[0])):  # Check each column\n",
        "        try:\n",
        "            if all(is_number(str(val)) for val in data[1:] if val not in [None, \"\", \" \"]):\n",
        "                numeric_cols.append(data[0][col])\n",
        "        except Exception as e:\n",
        "            print(f\"Error in column {col}: {e}\")\n",
        "    return numeric_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdGeRAJ9MAOd"
      },
      "outputs": [],
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3njeySMARC"
      },
      "outputs": [],
      "source": [
        "def find_start_row(df):\n",
        "    date_pattern = re.compile(r'\\d{2}-\\w{3}-\\d{4}')\n",
        "    for i, row in df.iterrows():\n",
        "        if date_pattern.search(str(row[0])):\n",
        "            return i\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfatz5C1MATb"
      },
      "outputs": [],
      "source": [
        "# Function to visualize financial factors\n",
        "def visualize_financial_factors(data):\n",
        "    # Convert the 'Publication date' column to datetime\n",
        "    data['Publication date'] = pd.to_datetime(data['Publication date'])\n",
        "\n",
        "    # Group by 'Publication date' and calculate the mean 'Coupon Rate' for that date\n",
        "    grouped_data = data.groupby('Publication date').agg({'Coupon Rate': 'mean'})\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    grouped_data['Coupon Rate'].plot(ax=ax)\n",
        "\n",
        "    # Annotations (you might want to customize the position or content of annotations)\n",
        "    for idx, row in data.iterrows():\n",
        "        ax.annotate(row['Identifier'], (row['Publication date'], row['Coupon Rate']),\n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "    # Improve plot\n",
        "    ax.set_title('Average Coupon Rate Over Time')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Coupon Rate (%)')\n",
        "    ax.grid(True)\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ticRtTgGMAW6"
      },
      "outputs": [],
      "source": [
        "# Function to visualize data from multiple sheets\n",
        "def visualize_financial_factors_from_multiple_sheets(df_list):\n",
        "    if 'Publication date' not in data.columns or 'Coupon Rate' not in data.columns:\n",
        "      print(\"Required columns are not present in the dataframe.\")\n",
        "      return\n",
        "\n",
        "    plots = []\n",
        "\n",
        "    for df in df_list:\n",
        "        # Find the start row containing the date\n",
        "        start_row = find_start_row(df)\n",
        "        if start_row is None:\n",
        "            raise ValueError(\"No date found in the first column.\")\n",
        "\n",
        "        # Read the data from the start row\n",
        "        data = df.iloc[start_row:].copy()\n",
        "        data.columns = df.iloc[start_row - 1]\n",
        "        data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Convert date format\n",
        "        data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], errors='coerce')\n",
        "\n",
        "        # Visualize each financial variable\n",
        "        for column in data.columns[2:]:  # Assuming the first column is the date, second is the identifier\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            ax.plot(data[data.columns[0]], data[column], label=column)\n",
        "\n",
        "            # Set chart\n",
        "            ax.set_title(f'Time Series of {column}')\n",
        "            ax.set_xlabel('Date')\n",
        "            ax.set_ylabel(column)\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.show()\n",
        "            plots.append(plt)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybKM1KU-O0nc"
      },
      "outputs": [],
      "source": [
        "def visualize_categorical_data(df, categorical_column, numerical_columns):\n",
        "    plots = []\n",
        "\n",
        "    for column in numerical_columns:\n",
        "        # Create bar plot\n",
        "        bar_plot = df.groupby(categorical_column)[column].mean().plot(kind='bar')\n",
        "        plt.show()\n",
        "        plots.append(bar_plot.figure)\n",
        "\n",
        "        # Create box plot\n",
        "        box_plot = df.boxplot(column=column, by=categorical_column, grid=False)\n",
        "        plt.title(f'{column} by {categorical_column}')\n",
        "        plt.suptitle('')  # Remove the default title\n",
        "        plt.show()\n",
        "        plots.append(box_plot.figure)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxIS7bvyPjUy"
      },
      "outputs": [],
      "source": [
        "# Function to visualize significant relationships\n",
        "def visualize_significant_relationships(df, date_column, numerical_columns, threshold=0.05):\n",
        "    plots = []\n",
        "\n",
        "    # Convert date column to datetime type\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "    # Iterate over each numerical variable\n",
        "    for num_col in numerical_columns:\n",
        "        # Iterate over each column in DataFrame to find relationships with numerical variables\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' or col == date_column:\n",
        "                continue  # Skip non-numerical variables and the date column\n",
        "\n",
        "            # Calculate the correlation between numerical variables\n",
        "            correlation, p_value = stats.pearsonr(df[col], df[num_col])\n",
        "\n",
        "            # If correlation is significant, visualize it\n",
        "            if p_value < threshold:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.regplot(x=col, y=num_col, data=df)\n",
        "                plt.title(f'Relationship between {col} and {num_col} (p={p_value:.4f})')\n",
        "                plt.show()\n",
        "                plots.append(plt)\n",
        "\n",
        "    return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVIFDY2xPlfO"
      },
      "outputs": [],
      "source": [
        "def check_sheets_in_excel(file_path):\n",
        "    xls = pd.ExcelFile(file_path)\n",
        "    sheet_names = xls.sheet_names  # Get all sheet names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JtQx24HPnQf"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    RBA_sheet_names = RBA_sheet_names = [\"A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET\", \"A2 RESERVE BANK OF AUSTRALIA – MONETARY POLICY CHANGES\", \"A3 Reserve Bank of Australia - Open Market Operations\"\n",
        "                  \"A3 Reserve Bank of Australia - Open Market Operations\",\"A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\"A3.2 SECURITIES LENDING REPURCHASE TRANSACTIONS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\n",
        "                   \"A4 RESERVE BANK OF AUSTRALIA – FOREIGN EXCHANGE TRANSACTIONS AND HOLDINGS OF OFFICIAL RESERVE ASSETS\",\"A5 RESERVE BANK OF AUSTRALIA - DAILY FOREIGN EXCHANGE MARKET INTERVENTION TRANSACTIONS\",\n",
        "                   \"A6 RESERVE BANK OF AUSTRALIA – BANKNOTES ON ISSUE BY DENOMINATION\",\"A7 RESERVE BANK OF AUSTRALIA – DETECTED AUSTRALIAN COUNTERFEITS BY DENOMINATION\",\"B1 ASSETS OF FINANCIAL INSTITUTIONS\",\n",
        "                   \"B2 BANKS – OFF-BALANCE SHEET BUSINESS\",\"B11.1 INTERNATIONAL ASSETS OF THE AUSTRALIAN-LOCATED OPERATIONS OF BANKS AND RFCs\",\"C1 Credit and Charge Cards – Seasonally Adjusted Series\",\n",
        "                   \"C1.1 Credit and Charge Cards – Original Series – Aggregate Data\",\"C1.3 Credit and Charge Cards – Market Shares of Card Schemes\",\"C2 Debit Cards – Seasonally Adjusted Series\",\n",
        "                   \"C2.1 Debit Cards – Original Series\",\"C2.2 Prepaid Cards – Original Series\",\"D1 GROWTH IN SELECTED FINANCIAL AGGREGATES\",\"F5 INDICATOR LENDING RATES\",\n",
        "                   \"F7 BUSINESS LENDING RATES\",\"F8 PERSONAL LENDING RATES\",\"F11.1 EXCHANGE RATES\",\"F13 INTERNATIONAL OFFICIAL INTEREST RATES\",\"G1 CONSUMER PRICE INFLATION\",\"H1 GROSS DOMESTIC PRODUCT AND INCOME\",\n",
        "                   \"I1 INTERNATIONAL TRADE AND BALANCE OF PAYMENTS\"]\n",
        "\n",
        "    all_dataframes = []\n",
        "\n",
        "    for spreadsheet_name in RBA_sheet_names:\n",
        "        try:\n",
        "            df = get_rba_sheet_data(spreadsheet_name, worksheet_index=0)\n",
        "            start_row = find_start_row(df)\n",
        "\n",
        "            if start_row is not None:\n",
        "                new_header = df.iloc[start_row]\n",
        "                df = df[start_row + 1:]\n",
        "                df.columns = new_header\n",
        "\n",
        "                if 'Series ID' in df.columns:\n",
        "                    df['Series ID'] = pd.to_datetime(df['Series ID'], errors='coerce')\n",
        "\n",
        "                if 'SomeNumericColumn' in df.columns:\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.plot(df['Series ID'], df['SomeNumericColumn'], label='SomeNumericColumn')\n",
        "                    plt.xlabel('Date')\n",
        "                    plt.ylabel('Value')\n",
        "                    plt.title(f'Data Visualization for {spreadsheet_name}')\n",
        "                    plt.legend()\n",
        "                    plt.show()\n",
        "\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing document: {spreadsheet_name}\")\n",
        "            print(f\"Error details: {e}\")\n",
        "            import traceback\n",
        "            print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj8kAfjW2ln-"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY19Oxb-kjw3"
      },
      "source": [
        "Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYrD47uIZuTw"
      },
      "outputs": [],
      "source": [
        "# Price and Volume Plot for all DataFrames\n",
        "for df, name in zip(dataframes, sheet_names):\n",
        "    # Price and Volume Plot\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(df.index, df['Volume'], color='orange')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'Daily Closing Prices for {name}')\n",
        "    print(f'Daily Volume for {name}')\n",
        "\n",
        "    # Moving Average Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['MA30'] = df['Close'].rolling(window=30).mean()\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.plot(df['MA5'], label='5-Day MA')\n",
        "    plt.plot(df['MA30'], label='30-Day MA')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f'Moving Averages for {name}')\n",
        "\n",
        "    # Price Fluctuation Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    plt.plot(df['Price_Range'])\n",
        "    plt.show()\n",
        "    print(f'Price Fluctuation for {name}')\n",
        "\n",
        "    # Momentum Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
        "    plt.plot(df['Momentum'])\n",
        "    plt.show()\n",
        "    print(f'Momentum for {name}')\n",
        "\n",
        "    # Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation = df.corr()\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.show()\n",
        "    print(f'Correlation Heatmap for {name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1WW9k4Sc6Of"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns for each asset\n",
        "returns_dict = {}\n",
        "\n",
        "for asset_name, df in zip(sheet_names, dataframes):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    returns_dict[asset_name] = df[['Date', 'Returns']].copy()\n",
        "\n",
        "# Print the returns for each asset\n",
        "for asset_name, returns_df in returns_dict.items():\n",
        "    print(f\"Asset: {asset_name}\")\n",
        "    print(returns_df.head())\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br_AlYcDK0Nd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIkSEr4rK0UV"
      },
      "outputs": [],
      "source": [
        "def get_alpha_vantage_data(symbol, api_key, function=\"TIME_SERIES_DAILY\"):\n",
        "    base_url = \"https://www.alphavantage.co/query\"\n",
        "    params = {\n",
        "        \"function\": function,\n",
        "        \"symbol\": symbol,\n",
        "        \"apikey\": api_key,\n",
        "        \"datatype\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wVeGx--K0XB"
      },
      "outputs": [],
      "source": [
        "def process_alpha_vantage_data(data):\n",
        "    df = pd.DataFrame(data['Time Series (Daily)']).T\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df.apply(pd.to_numeric)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67vj_NsK8gR"
      },
      "outputs": [],
      "source": [
        "api_key = getpass.getpass(\"Please enter Alpha Vantage API key:\")\n",
        "symbol = \"MSFT\", \"META\", \"NFLX\"\n",
        "\n",
        "raw_data = get_alpha_vantage_data(symbol, api_key)\n",
        "print(raw_data)\n",
        "df = process_alpha_vantage_data(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMIaHuYvPPpk"
      },
      "outputs": [],
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSK_oBPIJ78m"
      },
      "outputs": [],
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "print(sp500_df.head())\n",
        "\n",
        "market_returns = sp500_df['Close'].pct_change()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paZFyrmJklWU"
      },
      "source": [
        "ADV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8VTHQwz7Hf"
      },
      "outputs": [],
      "source": [
        "def get_risk_factors(start_date, end_date):\n",
        "    try:\n",
        "        ff_factors = yf.download(\"F-F_Research_Data_Factors_daily\", start=start_date, end=end_date)\n",
        "\n",
        "        if len(ff_factors.columns) == 4:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate']\n",
        "        elif len(ff_factors.columns) == 6:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate', 'Column5', 'Column6']\n",
        "\n",
        "        ff_factors = ff_factors / 100\n",
        "\n",
        "        return ff_factors\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBuWMyi1cuAh"
      },
      "source": [
        "Empty Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBZsevDkz_7T"
      },
      "outputs": [],
      "source": [
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "risk_factors = get_risk_factors(start_date, end_date)\n",
        "\n",
        "if risk_factors is not None:\n",
        "    print(risk_factors.head())\n",
        "else:\n",
        "    print(\"Failed to download risk factors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB0oLHVO1liZ"
      },
      "outputs": [],
      "source": [
        "def calculate_asset_returns(df):\n",
        "    asset_returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    return asset_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-se7onuZuWW"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_volatility(df, window=20):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    volatility = returns.rolling(window=window).std()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(volatility, label='Volatility')\n",
        "    plt.title('Volatility Factor')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxVXh5ZeZuY8"
      },
      "outputs": [],
      "source": [
        "def calculate_alpha_and_beta(asset_returns, market_returns):\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(asset_returns, market_returns)\n",
        "    alpha = intercept\n",
        "    beta = slope\n",
        "    return alpha, beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og9boVWNZubs"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_sharpe_ratio(df, risk_free_rate=0.02):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    volatility = returns.std()\n",
        "    sharpe_ratio = (mean_return - risk_free_rate) / volatility\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(returns, label='Asset Returns')\n",
        "    plt.title('Asset Returns')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"\")\n",
        "\n",
        "    print(f'Sharpe Ratio: {sharpe_ratio}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d60nHo-zZud0"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_r_squared(asset_returns, market_returns):\n",
        "    r_squared = np.corrcoef(asset_returns, market_returns)[0, 1] ** 2\n",
        "    print(f'R-Squared Value: {r_squared}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M68oYaK_kKaK"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "    elif indicator == 'RSI':\n",
        "        delta = df['Close'].diff(1)\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=window).mean()\n",
        "        avg_loss = loss.rolling(window=window).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        df['RSI'] = rsi\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        df['MA'] = df['Close'].rolling(window=window).mean()\n",
        "        df['Upper_Band'] = df['MA'] + 2 * df['Close'].rolling(window=window).std()\n",
        "        df['Lower_Band'] = df['MA'] - 2 * df['Close'].rolling(window=window).std()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if indicator == 'MA':\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "        print(\"\")\n",
        "    elif indicator == 'RSI':\n",
        "        plt.plot(df['RSI'], label='RSI')\n",
        "        plt.title('Relative Strength Index (RSI)')\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        plt.plot(df['Close'], label='Close Price', alpha=0.5)\n",
        "        plt.plot(df['Upper_Band'], label='Upper Bollinger Band')\n",
        "        plt.plot(df['Lower_Band'], label='Lower Bollinger Band')\n",
        "        plt.title('Bollinger Bands')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHDfT067kQ9_"
      },
      "outputs": [],
      "source": [
        "def factor_model_analysis(asset_returns, market_returns, risk_factors):\n",
        "    # Prepare the risk factors DataFrame with a constant for the OLS regression\n",
        "    X = sm.add_constant(risk_factors)\n",
        "\n",
        "    # Run the OLS regression\n",
        "    model = sm.OLS(asset_returns, X).fit()\n",
        "\n",
        "    # Print the summary of the regression\n",
        "    print(model.summary())\n",
        "\n",
        "    # Visualize the factor loadings\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(X.columns[1:], model.params[1:])  # Skip the constant\n",
        "    plt.xlabel('Risk Factors')\n",
        "    plt.ylabel('Factor Loadings')\n",
        "    plt.title('Factor Loadings for Asset Returns')\n",
        "    plt.show()\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv_vsZ6GCaNs"
      },
      "outputs": [],
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "        plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nohcZAqWEMNh"
      },
      "outputs": [],
      "source": [
        "for df in dataframes:\n",
        "    asset_returns = calculate_asset_returns(df)\n",
        "    calculate_and_visualize_volatility(df)\n",
        "    calculate_and_visualize_sharpe_ratio(df)\n",
        "\n",
        "    alpha, beta = calculate_alpha_and_beta(asset_returns, market_returns)\n",
        "    print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
        "\n",
        "    calculate_and_visualize_r_squared(asset_returns, market_returns)\n",
        "    calculate_and_visualize_technical_indicator(df, 'MA')\n",
        "\n",
        "    # Align asset_returns with risk_factors\n",
        "    if 'risk_factors' not in df.columns:\n",
        "        print(\"Risk factors are not in the DataFrame\")\n",
        "        continue\n",
        "\n",
        "    risk_factors_aligned = risk_factors.loc[asset_returns.index]\n",
        "\n",
        "    # Drop missing values\n",
        "    asset_returns.dropna(inplace=True)\n",
        "    risk_factors_aligned.dropna(inplace=True)\n",
        "\n",
        "    # Ensure all inputs are aligned by index\n",
        "    common_dates = asset_returns.index.intersection(risk_factors_aligned.index)\n",
        "    asset_returns_aligned = asset_returns.loc[common_dates]\n",
        "    risk_factors_aligned = risk_factors_aligned.loc[common_dates]\n",
        "\n",
        "    # Run the factor model analysis\n",
        "    factor_model_analysis(asset_returns_aligned, market_returns.loc[common_dates], risk_factors_aligned)\n",
        "\n",
        "    # Print the head of the dataframe to check\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBAe4ApesTr"
      },
      "source": [
        "TWO Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4BPnKRcesdv"
      },
      "outputs": [],
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thm31nbyLi0H"
      },
      "outputs": [],
      "source": [
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoS1aetKKKYm"
      },
      "outputs": [],
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['KMeans_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by K-Means Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "linked = linkage(sp500_df[['Max Drawdown']], 'single')\n",
        "\n",
        "plt.figure(figsize=(50, 35))\n",
        "dendrogram(linked, orientation='top', labels=sp500_df.index.astype(str), distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "db = DBSCAN(eps=0.001, min_samples=3).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['DBSCAN_Cluster'] = db.labels_\n",
        "\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['DBSCAN_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by DBSCAN Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLu4npUveshu"
      },
      "outputs": [],
      "source": [
        "# Function to get S&P 500 data\n",
        "def get_sp500_data(start_date, end_date):\n",
        "    return yf.download('^GSPC', start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3f7iTAULdmr"
      },
      "outputs": [],
      "source": [
        "# Function to calculate max drawdown\n",
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh1-Djo4LdpD"
      },
      "outputs": [],
      "source": [
        "# Adding technical indicators\n",
        "def add_technical_indicators(df):\n",
        "    df['SMA_50'] = talib.SMA(df['Close'], timeperiod=50)\n",
        "    df['RSI_14'] = talib.RSI(df['Close'], timeperiod=14)\n",
        "    df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwUXgbNWKbV_"
      },
      "outputs": [],
      "source": [
        "# Get the data\n",
        "sp500_df = get_sp500_data('2000-01-01', '2022-12-30')\n",
        "\n",
        "# Calculate returns and max drawdown\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "# Add technical indicators\n",
        "add_technical_indicators(sp500_df)\n",
        "\n",
        "# Feature scaling with RobustScaler to mitigate the effect of outliers\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_features = robust_scaler.fit_transform(sp500_df[['Max Drawdown', 'Returns']])\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "sp500_pca = pca.fit_transform(scaled_features)\n",
        "\n",
        "# K-Means clustering on PCA-transformed data\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_pca)\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Prepare features and labels for the classification model\n",
        "features = sp500_df[['SMA_50', 'RSI_14', 'Volume', 'Max Drawdown', 'Returns']]\n",
        "labels = sp500_df['KMeans_Cluster']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# Oversampling with SMOTE to balance the classes\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Cross-validated score\n",
        "scores = cross_val_score(rf_classifier, X_resampled, y_resampled, cv=5)\n",
        "print(f\"Cross-validated scores: {scores.mean()}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Classification report and accuracy\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Visualization of PCA-transformed features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_pca[:, 0], y=sp500_pca[:, 1], hue=kmeans.labels_, palette='viridis')\n",
        "plt.title('PCA of S&P 500 Data')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# Visualization of clustering results on original features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_df.index, y=sp500_df['Max Drawdown'], hue=sp500_df['KMeans_Cluster'], palette='viridis')\n",
        "plt.title('K-Means Clustering Results on S&P 500 Data')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQLnTYxJ98KGuhc/IltiUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}