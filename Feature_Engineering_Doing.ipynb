{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsLXLScLN+ZOKoQmljYKaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunan-au/Modeling_Risk/blob/main/Feature_Engineering_Doing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeking Factors"
      ],
      "metadata": {
        "id": "c85nnOIPlrUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimize"
      ],
      "metadata": {
        "id": "bKGsYMVnl0YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model using\n"
      ],
      "metadata": {
        "id": "msC2jkxBl9kX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7sEBgYVqb4u"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "HHhVbfculx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package"
      ],
      "metadata": {
        "id": "JgNoGJajcogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas matplotlib scipy statsmodels\n",
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "pqCLZQm_KYJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "!pip install TA-Lib"
      ],
      "metadata": {
        "id": "yXTl9rNVmcEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import gspread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import talib\n",
        "import yfinance as yf\n",
        "import getpass\n",
        "import re\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from matplotlib.ticker import MaxNLocator"
      ],
      "metadata": {
        "id": "H8Gb1G7pKBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Importing"
      ],
      "metadata": {
        "id": "HOd4MNtHtviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    # Print columns to debug\n",
        "    print(\"Columns in DataFrame:\", df.columns)\n",
        "\n",
        "    # Check if 'Date' column exists\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'].str.split().str[0], format='%m/%d/%Y').dt.date\n",
        "    else:\n",
        "        print(f\"'Date' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"'{col}' column not found in {spreadsheet_name}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names = [\"MSFT\", \"META\", \"NFLX\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data_updated(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    # The first row is the header\n",
        "    header = data[0]\n",
        "    # The first column 'Year' has no header, add it manually\n",
        "    header[0] = 'Year'\n",
        "\n",
        "    df = pd.DataFrame(data[1:], columns=header)\n",
        "\n",
        "    # Convert 'Year' to datetime format\n",
        "    if 'Annual' in spreadsheet_name:\n",
        "        # Annual data, set 'Year' as the end of each year to forward fill\n",
        "        df['Year'] = pd.to_datetime(df['Year']+'-12-31')\n",
        "    else:\n",
        "        # Monthly data\n",
        "        df['Year'] = pd.to_datetime(df['Year'], format='%Y%m')\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "LknMEXnxjzDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "sheet_name_factors = ['F-F_Research_Data_5_Annual_Factors_2x3', 'F-F_Research_Data_5_Month_Factors_2x3']\n",
        "factorframes = [get_sheet_data_updated(name) for name in sheet_name_factors]\n",
        "\n",
        "# Ensure 'Year' column is datetime for both dataframes\n",
        "factorframes[0]['Year'] = pd.to_datetime(factorframes[0]['Year'], format='%Y')\n",
        "factorframes[1]['Year'] = pd.to_datetime(factorframes[1]['Year'], format='%Y%m')\n",
        "\n",
        "# Set the index to 'Year' and sort it\n",
        "annual_data = factorframes[0].set_index('Year').sort_index()\n",
        "monthly_data = factorframes[1].set_index('Year').sort_index()\n",
        "\n",
        "# Resample annual data to monthly, filling forward\n",
        "# Note: The annual data is on the last day of the year, so we need to shift it to the first day\n",
        "annual_data = annual_data.resample('MS').ffill()\n",
        "\n",
        "# Combine the data\n",
        "combined_data = pd.concat([monthly_data, annual_data]).sort_index().fillna(method='ffill')\n",
        "\n",
        "# Reset the index if needed\n",
        "combined_data = combined_data.reset_index()\n",
        "\n",
        "# Print combined data to check\n",
        "print(combined_data.head())"
      ],
      "metadata": {
        "id": "yCWgA63PcLIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RBA_sheet_names = [\"A1 RESERVE BANK OF AUSTRALIA - BALANCE SHEET\", \"A2 RESERVE BANK OF AUSTRALIA – MONETARY POLICY CHANGES\", \"A3 Reserve Bank of Australia - Open Market Operations\"\n",
        "                  \"A3 Reserve Bank of Australia - Open Market Operations\",\"A3.1 HOLDINGS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\"A3.2 SECURITIES LENDING REPURCHASE TRANSACTIONS OF AUSTRALIAN GOVERNMENT SECURITIES AND SEMIS\",\n",
        "                   \"A4 RESERVE BANK OF AUSTRALIA – FOREIGN EXCHANGE TRANSACTIONS AND HOLDINGS OF OFFICIAL RESERVE ASSETS\",\"A5 RESERVE BANK OF AUSTRALIA - DAILY FOREIGN EXCHANGE MARKET INTERVENTION TRANSACTIONS\",\n",
        "                   \"A6 RESERVE BANK OF AUSTRALIA – BANKNOTES ON ISSUE BY DENOMINATION\",\"A7 RESERVE BANK OF AUSTRALIA – DETECTED AUSTRALIAN COUNTERFEITS BY DENOMINATION\",\"B1 ASSETS OF FINANCIAL INSTITUTIONS\",\n",
        "                   \"B2 BANKS – OFF-BALANCE SHEET BUSINESS\",\"B11.1 INTERNATIONAL ASSETS OF THE AUSTRALIAN-LOCATED OPERATIONS OF BANKS AND RFCs\",\"C1 Credit and Charge Cards – Seasonally Adjusted Series\",\n",
        "                   \"C1.1 Credit and Charge Cards – Original Series – Aggregate Data\",\"C1.3 Credit and Charge Cards – Market Shares of Card Schemes\",\"C2 Debit Cards – Seasonally Adjusted Series\",\n",
        "                   \"C2.1 Debit Cards – Original Series\",\"C2.2 Prepaid Cards – Original Series\",\"D1 GROWTH IN SELECTED FINANCIAL AGGREGATES\",\"F5 INDICATOR LENDING RATES\",\n",
        "                   \"F7 BUSINESS LENDING RATES\",\"F8 PERSONAL LENDING RATES\",\"F11.1 EXCHANGE RATES\",\"F13 INTERNATIONAL OFFICIAL INTEREST RATES\",\"G1 CONSUMER PRICE INFLATION\",\"H1 GROSS DOMESTIC PRODUCT AND INCOME\",\n",
        "                   \"I1 INTERNATIONAL TRADE AND BALANCE OF PAYMENTS\"]"
      ],
      "metadata": {
        "id": "8FOGlWPKzD8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rba_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "\n",
        "    numeric_columns = get_numeric_columns(data[1:])  # Skip header row\n",
        "    for col in numeric_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "bRIgtMJrMAJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect numeric columns\n",
        "def get_numeric_columns(data):\n",
        "    numeric_cols = []\n",
        "    for col in range(len(data[0])):  # Check each column\n",
        "        if all(is_number(val) for val in data[1:] if val not in [None, \"\", \" \"]):\n",
        "            numeric_cols.append(data[0][col])\n",
        "    return numeric_cols"
      ],
      "metadata": {
        "id": "8pXPrktYMAL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "FdGeRAJ9MAOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_start_row(df):\n",
        "    date_pattern = re.compile(r'\\d{2}-\\w{3}-\\d{4}')\n",
        "    for i, row in df.iterrows():\n",
        "        if date_pattern.search(str(row[0])):\n",
        "            return i\n",
        "    return None"
      ],
      "metadata": {
        "id": "pN3njeySMARC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize financial factors\n",
        "def visualize_financial_factors(data):\n",
        "    # Convert the 'Publication date' column to datetime\n",
        "    data['Publication date'] = pd.to_datetime(data['Publication date'])\n",
        "\n",
        "    # Group by 'Publication date' and calculate the mean 'Coupon Rate' for that date\n",
        "    grouped_data = data.groupby('Publication date').agg({'Coupon Rate': 'mean'})\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    grouped_data['Coupon Rate'].plot(ax=ax)\n",
        "\n",
        "    # Annotations (you might want to customize the position or content of annotations)\n",
        "    for idx, row in data.iterrows():\n",
        "        ax.annotate(row['Identifier'], (row['Publication date'], row['Coupon Rate']),\n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "    # Improve plot\n",
        "    ax.set_title('Average Coupon Rate Over Time')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Coupon Rate (%)')\n",
        "    ax.grid(True)\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    return plt"
      ],
      "metadata": {
        "id": "vfatz5C1MATb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize data from multiple sheets\n",
        "def visualize_financial_factors_from_multiple_sheets(df_list):\n",
        "    plots = []\n",
        "\n",
        "    for df in df_list:\n",
        "        # Find the start row containing the date\n",
        "        start_row = find_start_row(df)\n",
        "        if start_row is None:\n",
        "            raise ValueError(\"No date found in the first column.\")\n",
        "\n",
        "        # Read the data from the start row\n",
        "        data = df.iloc[start_row:].copy()\n",
        "        data.columns = df.iloc[start_row - 1]\n",
        "        data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Convert date format\n",
        "        data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], errors='coerce')\n",
        "\n",
        "        # Visualize each financial variable\n",
        "        for column in data.columns[2:]:  # Assuming the first column is the date, second is the identifier\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            ax.plot(data[data.columns[0]], data[column], label=column)\n",
        "\n",
        "            # Set chart\n",
        "            ax.set_title(f'Time Series of {column}')\n",
        "            ax.set_xlabel('Date')\n",
        "            ax.set_ylabel(column)\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.show()\n",
        "            plots.append(plt)\n",
        "\n",
        "    return plots"
      ],
      "metadata": {
        "id": "ticRtTgGMAW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_categorical_data(df, categorical_column, numerical_columns):\n",
        "    plots = []\n",
        "\n",
        "    for column in numerical_columns:\n",
        "        # Create bar plot\n",
        "        bar_plot = df.groupby(categorical_column)[column].mean().plot(kind='bar')\n",
        "        plt.show()\n",
        "        plots.append(bar_plot.figure)\n",
        "\n",
        "        # Create box plot\n",
        "        box_plot = df.boxplot(column=column, by=categorical_column, grid=False)\n",
        "        plt.title(f'{column} by {categorical_column}')\n",
        "        plt.suptitle('')  # Remove the default title\n",
        "        plt.show()\n",
        "        plots.append(box_plot.figure)\n",
        "\n",
        "    return plots"
      ],
      "metadata": {
        "id": "ybKM1KU-O0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize significant relationships\n",
        "def visualize_significant_relationships(df, date_column, numerical_columns, threshold=0.05):\n",
        "    plots = []\n",
        "\n",
        "    # Convert date column to datetime type\n",
        "    df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "    # Iterate over each numerical variable\n",
        "    for num_col in numerical_columns:\n",
        "        # Iterate over each column in DataFrame to find relationships with numerical variables\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' or col == date_column:\n",
        "                continue  # Skip non-numerical variables and the date column\n",
        "\n",
        "            # Calculate the correlation between numerical variables\n",
        "            correlation, p_value = stats.pearsonr(df[col], df[num_col])\n",
        "\n",
        "            # If correlation is significant, visualize it\n",
        "            if p_value < threshold:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.regplot(x=col, y=num_col, data=df)\n",
        "                plt.title(f'Relationship between {col} and {num_col} (p={p_value:.4f})')\n",
        "                plt.show()\n",
        "                plots.append(plt)\n",
        "\n",
        "    return plots"
      ],
      "metadata": {
        "id": "nxIS7bvyPjUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sheets_in_excel(file_path):\n",
        "    xls = pd.ExcelFile(file_path)\n",
        "    sheet_names = xls.sheet_names  # Get all sheet names"
      ],
      "metadata": {
        "id": "lVIFDY2xPlfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    all_dataframes = []\n",
        "    for spreadsheet_name in RBA_sheet_names:\n",
        "        try:\n",
        "            df = get_rba_sheet_data(spreadsheet_name, worksheet_index=0)\n",
        "            start_row = find_start_row(df)\n",
        "            if start_row is not None:\n",
        "                df = df.iloc[start_row:]\n",
        "                df.reset_index(drop=True, inplace=True)\n",
        "                numeric_columns = get_numeric_columns(df.iloc[0:])\n",
        "                df = df.apply(pd.to_numeric, errors='ignore')\n",
        "                df['Date'] = pd.to_datetime(df['Date'], errors='ignore')\n",
        "\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "            if 'Date' in df.columns and len(numeric_columns) > 0:\n",
        "                visualize_financial_factors_from_multiple_sheets([df])\n",
        "            elif 'Date' in df.columns:\n",
        "                visualize_financial_factors(df)\n",
        "            elif len(numeric_columns) > 1:\n",
        "                visualize_significant_relationships(df, numeric_columns[0], numeric_columns[1:])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing document: {spreadsheet_name}\")\n",
        "            print(f\"Error details: {e}\")"
      ],
      "metadata": {
        "id": "6JtQx24HPnQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2LrZqqAiRUlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic"
      ],
      "metadata": {
        "id": "gY19Oxb-kjw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Price and Volume Plot for all DataFrames\n",
        "for df, name in zip(dataframes, sheet_names):\n",
        "    # Price and Volume Plot\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(df.index, df['Volume'], color='orange')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'Daily Closing Prices for {name}')\n",
        "    print(f'Daily Volume for {name}')\n",
        "\n",
        "    # Moving Average Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "    df['MA30'] = df['Close'].rolling(window=30).mean()\n",
        "    plt.plot(df['Close'], label='Close Price')\n",
        "    plt.plot(df['MA5'], label='5-Day MA')\n",
        "    plt.plot(df['MA30'], label='30-Day MA')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f'Moving Averages for {name}')\n",
        "\n",
        "    # Price Fluctuation Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    plt.plot(df['Price_Range'])\n",
        "    plt.show()\n",
        "    print(f'Price Fluctuation for {name}')\n",
        "\n",
        "    # Momentum Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
        "    plt.plot(df['Momentum'])\n",
        "    plt.show()\n",
        "    print(f'Momentum for {name}')\n",
        "\n",
        "    # Correlation Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation = df.corr()\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.show()\n",
        "    print(f'Correlation Heatmap for {name}')"
      ],
      "metadata": {
        "id": "mYrD47uIZuTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate daily returns for each asset\n",
        "returns_dict = {}\n",
        "\n",
        "for asset_name, df in zip(sheet_names, dataframes):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    returns_dict[asset_name] = df[['Date', 'Returns']].copy()\n",
        "\n",
        "# Print the returns for each asset\n",
        "for asset_name, returns_df in returns_dict.items():\n",
        "    print(f\"Asset: {asset_name}\")\n",
        "    print(returns_df.head())\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "k1WW9k4Sc6Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Br_AlYcDK0Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_alpha_vantage_data(symbol, api_key, function=\"TIME_SERIES_DAILY\"):\n",
        "    base_url = \"https://www.alphavantage.co/query\"\n",
        "    params = {\n",
        "        \"function\": function,\n",
        "        \"symbol\": symbol,\n",
        "        \"apikey\": api_key,\n",
        "        \"datatype\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data"
      ],
      "metadata": {
        "id": "hIkSEr4rK0UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_alpha_vantage_data(data):\n",
        "    df = pd.DataFrame(data['Time Series (Daily)']).T\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df.apply(pd.to_numeric)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    return df"
      ],
      "metadata": {
        "id": "9wVeGx--K0XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass.getpass(\"Please enter Alpha Vantage API key:\")\n",
        "symbol = \"MSFT\", \"META\", \"NFLX\"\n",
        "\n",
        "raw_data = get_alpha_vantage_data(symbol, api_key)\n",
        "print(raw_data)\n",
        "df = process_alpha_vantage_data(raw_data)"
      ],
      "metadata": {
        "id": "M67vj_NsK8gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ],
      "metadata": {
        "id": "vMIaHuYvPPpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "print(sp500_df.head())\n",
        "\n",
        "market_returns = sp500_df['Close'].pct_change()"
      ],
      "metadata": {
        "id": "gSK_oBPIJ78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADV"
      ],
      "metadata": {
        "id": "paZFyrmJklWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_risk_factors(start_date, end_date):\n",
        "    try:\n",
        "        ff_factors = yf.download(\"F-F_Research_Data_Factors_daily\", start=start_date, end=end_date)\n",
        "\n",
        "        if len(ff_factors.columns) == 4:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate']\n",
        "        elif len(ff_factors.columns) == 6:\n",
        "            ff_factors.columns = ['Market Risk Premium', 'SMB', 'HML', 'Risk Free Rate', 'Column5', 'Column6']\n",
        "\n",
        "        ff_factors = ff_factors / 100\n",
        "\n",
        "        return ff_factors\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "0d8VTHQwz7Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empty Sheet"
      ],
      "metadata": {
        "id": "uBuWMyi1cuAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "risk_factors = get_risk_factors(start_date, end_date)\n",
        "\n",
        "if risk_factors is not None:\n",
        "    print(risk_factors.head())\n",
        "else:\n",
        "    print(\"Failed to download risk factors.\")"
      ],
      "metadata": {
        "id": "vBZsevDkz_7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_asset_returns(df):\n",
        "    asset_returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    return asset_returns"
      ],
      "metadata": {
        "id": "uB0oLHVO1liZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_volatility(df, window=20):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    volatility = returns.rolling(window=window).std()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(volatility, label='Volatility')\n",
        "    plt.title('Volatility Factor')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A-se7onuZuWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_alpha_and_beta(asset_returns, market_returns):\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(asset_returns, market_returns)\n",
        "    alpha = intercept\n",
        "    beta = slope\n",
        "    return alpha, beta"
      ],
      "metadata": {
        "id": "YxVXh5ZeZuY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_sharpe_ratio(df, risk_free_rate=0.02):\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    volatility = returns.std()\n",
        "    sharpe_ratio = (mean_return - risk_free_rate) / volatility\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(returns, label='Asset Returns')\n",
        "    plt.title('Asset Returns')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Sharpe Ratio: {sharpe_ratio}')"
      ],
      "metadata": {
        "id": "Og9boVWNZubs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_r_squared(asset_returns, market_returns):\n",
        "    r_squared = np.corrcoef(asset_returns, market_returns)[0, 1] ** 2\n",
        "    print(f'R-Squared Value: {r_squared}')"
      ],
      "metadata": {
        "id": "d60nHo-zZud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "    elif indicator == 'RSI':\n",
        "        delta = df['Close'].diff(1)\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=window).mean()\n",
        "        avg_loss = loss.rolling(window=window).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        df['RSI'] = rsi\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        df['MA'] = df['Close'].rolling(window=window).mean()\n",
        "        df['Upper_Band'] = df['MA'] + 2 * df['Close'].rolling(window=window).std()\n",
        "        df['Lower_Band'] = df['MA'] - 2 * df['Close'].rolling(window=window).std()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if indicator == 'MA':\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "    elif indicator == 'RSI':\n",
        "        plt.plot(df['RSI'], label='RSI')\n",
        "        plt.title('Relative Strength Index (RSI)')\n",
        "    elif indicator == 'Bollinger_Bands':\n",
        "        plt.plot(df['Close'], label='Close Price', alpha=0.5)\n",
        "        plt.plot(df['Upper_Band'], label='Upper Bollinger Band')\n",
        "        plt.plot(df['Lower_Band'], label='Lower Bollinger Band')\n",
        "        plt.title('Bollinger Bands')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M68oYaK_kKaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def factor_model_analysis(asset_returns, market_returns, risk_factors):\n",
        "    # Prepare the risk factors DataFrame with a constant for the OLS regression\n",
        "    X = sm.add_constant(risk_factors)\n",
        "\n",
        "    # Run the OLS regression\n",
        "    model = sm.OLS(asset_returns, X).fit()\n",
        "\n",
        "    # Print the summary of the regression\n",
        "    print(model.summary())\n",
        "\n",
        "    # Visualize the factor loadings\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(X.columns[1:], model.params[1:])  # Skip the constant\n",
        "    plt.xlabel('Risk Factors')\n",
        "    plt.ylabel('Factor Loadings')\n",
        "    plt.title('Factor Loadings for Asset Returns')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LHDfT067kQ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_visualize_technical_indicator(df, indicator='MA', window=20):\n",
        "    if indicator == 'MA':\n",
        "        df['Moving_Average'] = df['Close'].rolling(window=window).mean()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['Moving_Average'], label='Moving Average')\n",
        "        plt.title('Moving Average')\n",
        "        plt.legend()"
      ],
      "metadata": {
        "id": "lv_vsZ6GCaNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in dataframes:\n",
        "    asset_returns = calculate_asset_returns(df)\n",
        "    calculate_and_visualize_volatility(df)\n",
        "    calculate_and_visualize_sharpe_ratio(df)\n",
        "\n",
        "    alpha, beta = calculate_alpha_and_beta(asset_returns, market_returns)\n",
        "    print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
        "\n",
        "    calculate_and_visualize_r_squared(asset_returns, market_returns)\n",
        "    calculate_and_visualize_technical_indicator(df, 'MA')\n",
        "\n",
        "    # Align asset_returns with risk_factors\n",
        "    if 'risk_factors' not in df.columns:\n",
        "        print(\"Risk factors are not in the DataFrame\")\n",
        "        continue\n",
        "\n",
        "    risk_factors_aligned = risk_factors.loc[asset_returns.index]\n",
        "\n",
        "    # Drop missing values\n",
        "    asset_returns.dropna(inplace=True)\n",
        "    risk_factors_aligned.dropna(inplace=True)\n",
        "\n",
        "    # Ensure all inputs are aligned by index\n",
        "    common_dates = asset_returns.index.intersection(risk_factors_aligned.index)\n",
        "    asset_returns_aligned = asset_returns.loc[common_dates]\n",
        "    risk_factors_aligned = risk_factors_aligned.loc[common_dates]\n",
        "\n",
        "    # Run the factor model analysis\n",
        "    factor_model_analysis(asset_returns_aligned, market_returns.loc[common_dates], risk_factors_aligned)\n",
        "\n",
        "    # Print the head of the dataframe to check\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "nohcZAqWEMNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TWO Stage"
      ],
      "metadata": {
        "id": "CHBAe4ApesTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_data(start_date, end_date):\n",
        "    sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return sp500"
      ],
      "metadata": {
        "id": "q4BPnKRcesdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ],
      "metadata": {
        "id": "Thm31nbyLi0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_df = get_sp500_data('2022-01-01', '2022-12-30')\n",
        "\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['KMeans_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by K-Means Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "linked = linkage(sp500_df[['Max Drawdown']], 'single')\n",
        "\n",
        "plt.figure(figsize=(50, 35))\n",
        "dendrogram(linked, orientation='top', labels=sp500_df.index.astype(str), distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "db = DBSCAN(eps=0.001, min_samples=3).fit(sp500_df[['Max Drawdown']])\n",
        "\n",
        "sp500_df['DBSCAN_Cluster'] = db.labels_\n",
        "\n",
        "\n",
        "plt.scatter(sp500_df.index, sp500_df['Max Drawdown'], c=sp500_df['DBSCAN_Cluster'], cmap='viridis')\n",
        "plt.title('Market Regimes Identified by DBSCAN Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.show()\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "IoS1aetKKKYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get S&P 500 data\n",
        "def get_sp500_data(start_date, end_date):\n",
        "    return yf.download('^GSPC', start=start_date, end=end_date)"
      ],
      "metadata": {
        "id": "HLu4npUveshu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate max drawdown\n",
        "def calculate_max_drawdown(return_series):\n",
        "    cumulative_returns = (1 + return_series).cumprod()\n",
        "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - running_max) / running_max\n",
        "    return drawdown"
      ],
      "metadata": {
        "id": "a3f7iTAULdmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding technical indicators\n",
        "def add_technical_indicators(df):\n",
        "    df['SMA_50'] = talib.SMA(df['Close'], timeperiod=50)\n",
        "    df['RSI_14'] = talib.RSI(df['Close'], timeperiod=14)\n",
        "    df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "qh1-Djo4LdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "sp500_df = get_sp500_data('2000-01-01', '2022-12-30')\n",
        "\n",
        "# Calculate returns and max drawdown\n",
        "sp500_df['Returns'] = sp500_df['Close'].pct_change()\n",
        "sp500_df['Max Drawdown'] = calculate_max_drawdown(sp500_df['Returns'])\n",
        "sp500_df.dropna(inplace=True)\n",
        "\n",
        "# Add technical indicators\n",
        "add_technical_indicators(sp500_df)\n",
        "\n",
        "# Feature scaling with RobustScaler to mitigate the effect of outliers\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_features = robust_scaler.fit_transform(sp500_df[['Max Drawdown', 'Returns']])\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "sp500_pca = pca.fit_transform(scaled_features)\n",
        "\n",
        "# K-Means clustering on PCA-transformed data\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(sp500_pca)\n",
        "sp500_df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Prepare features and labels for the classification model\n",
        "features = sp500_df[['SMA_50', 'RSI_14', 'Volume', 'Max Drawdown', 'Returns']]\n",
        "labels = sp500_df['KMeans_Cluster']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# Oversampling with SMOTE to balance the classes\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_classifier = RandomForestClassifier(random_state=0)\n",
        "rf_classifier.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Cross-validated score\n",
        "scores = cross_val_score(rf_classifier, X_resampled, y_resampled, cv=5)\n",
        "print(f\"Cross-validated scores: {scores.mean()}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Classification report and accuracy\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Visualization of PCA-transformed features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_pca[:, 0], y=sp500_pca[:, 1], hue=kmeans.labels_, palette='viridis')\n",
        "plt.title('PCA of S&P 500 Data')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Visualization of clustering results on original features\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=sp500_df.index, y=sp500_df['Max Drawdown'], hue=sp500_df['KMeans_Cluster'], palette='viridis')\n",
        "plt.title('K-Means Clustering Results on S&P 500 Data')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max Drawdown')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pwUXgbNWKbV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}