{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "12qNFYhKl5MuoDqGu9iyzRDUTbv5rqAIY",
      "authorship_tag": "ABX9TyM5wfXMt+AZLtC+xz/si9Nz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunan-au/Modeling_Risk/blob/main/Modeling_Market_Risk_VaR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value at Risk\n"
      ],
      "metadata": {
        "id": "Eo-bQwKPBsaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the Frontier of Financial Risk Analysis in the Australian Healthcare Technology Sector"
      ],
      "metadata": {
        "id": "Jp7KU8cYbH7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic world of financial markets, understanding and managing risk is paramount. My project is at the forefront of this challenge, specifically targeting the **Australian Healthcare Technology Sector (BK7094)**. Embarking on a groundbreaking journey to analyze the **Value at Risk (VaR)** of stocks within this niche yet vital sector.\n",
        "\n",
        "My approach is a fusion of tradition and innovation. I utilize established VaR calculation methodologies like historical analysis, variance-covariance, and Monte Carlo simulations. But I don't stop there. I integrate these with cutting-edge machine learning techniques, including **Random Forest** algorithms, and advanced optimization methods like **simulated annealing** and **genetic algorithms**. This blend not only enriches my analysis but also elevates its precision.\n",
        "\n",
        "Recognizing the challenges posed by data scarcity in specialized markets like healthcare technology, adopt an innovative approach using **Generative Adversarial Networks (GAN)**. This technique allows I to generate synthetic, realistic market data scenarios, thus overcoming the limitations of traditional data sources.\n",
        "\n",
        "Moreover, my project leverages the power of **Natural Language Processing (NLP)** technologies, such as **BERT** and **CNN**, to automate the analysis of vast market research reports. This automation not only enhances my efficiency but also ensures comprehensive market coverage and deeper insights.\n",
        "\n",
        "Another highlight of my project is the **individual stock VaR analysis**. This allows for a meticulous dissection of investment risks associated with specific stocks in the Australian Healthcare Technology sector. It's a crucial step towards providing investors and analysts with detailed, actionable insights.\n",
        "\n",
        "By marrying the robustness of traditional financial risk assessment with the agility of modern technology, project aims to deliver **a series weights** about nuanced understanding of market risks â€“ for stakeholders in the rapidly evolving Australian Healthcare Technology sector.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4st-5UkbJTzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specials**\n",
        "\n",
        "**1. Combining Traditional and Advanced Methods:** Integrates historical, variance-covariance, and Monte Carlo simulation methods with modern techniques for a foundational understanding of VaR.\n",
        "\n",
        "**2. Utilization of Advanced Optimization Algorithms**: Employs sophisticated techniques like simulated annealing and genetic algorithms for effective optimization of model parameters.\n",
        "\n",
        "**3. Application of Machine Learning Technologies:** Uses algorithms like Random Forest to capture complex and non-linear relationships in financial data, offering detailed risk analysis.\n",
        "\n",
        "**4. Adaptability and Flexibility of Models:** Continuously adjusts and optimizes models to adapt to market changes.\n",
        "\n",
        "**5. Comprehensive Understanding of Market Dynamics:** Merges traditional financial risk assessment methods with modern technology for in-depth market risk analysis.\n",
        "\n",
        "**6. Innovative Approach to Address Data Scarcity:** Enhances data using Generative Adversarial Networks (GAN), creating synthetic yet realistic market data scenarios.\n",
        "\n",
        "**7. Synthetic Data Generation to Overcome Data Limitations**: Utilizes GANs to create synthetic data, compensating for the lack of original data.\n",
        "\n",
        "**8. Enhancing Model Training Effectiveness and Generalizability**: Improves model training and adaptability to diverse market scenarios through GAN-generated data.\n",
        "\n",
        "**9. Automated Scenario Analysis:** Applies NLP technologies like BERT and CNN for automatic interpretation of market research reports, forming new scenario analyses.\n",
        "\n",
        "**10. Efficiency and Coverage Improvement:** Utilizes NLP technologies for automated scenario analysis, enhancing efficiency and market information coverage.\n",
        "\n",
        "**11. Depth of Understanding and Insight:** Extracts key information and insights from a vast amount of text using NLP technologies.\n",
        "\n",
        "**12. Diversity and Innovation in Scenario Analysis:** Offers new analytical perspectives and patterns through automated scenario analysis using advanced NLP technologies.\n",
        "\n",
        "**13. Targeted Industry Analysis:** Focuses on the Australian Healthcare Technology Sector, providing in-depth and industry-specific risk analysis.\n",
        "\n",
        "**14. Explanatory Power and Transparency:** Offers models and methods that are easy to understand, enhancing confidence and comprehension among decision-makers.\n",
        "\n",
        "**15. Comprehensive Analytical Capability:** Combines macroeconomic data with market-specific information for a holistic risk assessment.\n",
        "\n",
        "**16. Individual Stock VaR Analysis:** Conducts Value at Risk analysis for individual stocks, enabling detailed dissection and understanding of specific investment risks within the sector.\n",
        "\n",
        "**17. Multi Asset Class Combination:** Simulates multiple asset classes such as funds and options to generate a VaR distribution that is richer and more realistic in composite performance"
      ],
      "metadata": {
        "id": "8Zldy1iv8YTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drawbacks & Todo**\n",
        "\n",
        "**1. Limited Data Volume:** The available data for the Australian stock market, especially with issues like the import failure of [Adherium Ltd (ADR)](https://docs.google.com/spreadsheets/d/1SkXt8Jfm6l1ucnbBetF7RsidqNeq8VZ37l1MJJIKsIE/edit?usp=sharing), [Beamtree Holdings Ltd(BMT)](https://docs.google.com/spreadsheets/d/1pD79cCnjSs9p8FuFwo2BO0ba7iwrgCIz8RZ3ItyeXI0/edit?usp=sharing/), [Tali Digital Ltd (TD1)](https://docs.google.com/spreadsheets/d/11CXtRS-DMZBSRCAcEvAL-WGZDa4g-K_-typO0QIWz6c/edit?usp=sharing)  is relatively small, which may limit the generalizability and accuracy of your models.\n",
        "\n",
        "**2. Accuracy of Simulated Data:** The simulated data for funds and equities may not accurately reflect real market dynamics, impacting the authenticity and reliability of your models.\n",
        "\n",
        "**3. Depth of Scenario Analysis:** The current scenario analysis might not comprehensively cover all possible market variations, especially under extreme market conditions.\n",
        "\n",
        "**4. Model Assumption Limitations:** The financial models used may be based on certain assumptions (like market behavior, asset correlations) that might not hold true under real market conditions.\n",
        "\n",
        "**5. Continuity of Time Series Data:** The continuity of time series might be impacted due to the method of data updating and processing, affecting the time-sensitive analysis of your models.\n",
        "\n",
        "**6. Adaptability of Quantitative Models:** In rapidly changing market conditions, quantitative models may require more frequent updates and adjustments to maintain their accuracy and relevance.\n",
        "\n",
        "**7. Computational Resource Requirements:** Advanced data analysis and model training may require significant computational resources, which could be a limiting factor in resource-constrained situations.\n",
        "\n",
        "**8. Flaws in VaR Calculation:** There are flaws in the calculation of Value at Risk (VaR) for stocks in the Australian Healthcare Technology Sector, especially in adequately addressing tail risks.\n",
        "\n",
        "**9. Insufficient Sector Mapping:** The study, though focused on the Australian Healthcare Technology Sector (BK7094), might have inadequate sector mapping, affecting the precision of the research.\n",
        "\n",
        "**10. Mixed and Unclassified Data Sets:** The training data set might be focused on bullish and bearish stocks, but inference data could be research reports for understanding the economic environment, leading to inconsistencies between training and inference data.\n",
        "\n",
        "**11. Financial Formulas and Asset Class Correlation Estimation:** More financial formulas are needed for valuation, along with establishing correlations between different asset classes.\n",
        "\n",
        "**12. Extreme Value and Inherent Limitations:** The model might have limitations in extreme value and lacks independent quantification of scenarios.\n",
        "\n",
        "**13. Limitations of NLP Models:** The use of NLP technologies like CNN and Bert for scenario generation did not fully consider the limitations and issues of these models, which may affect their ability to understand complex financial texts and generate accurate scenarios.\n",
        "\n",
        "**14. Lack of Model Cascading:**The project did not consider effective model cascading strategies, which are crucial in complex financial data analysis.\n",
        "\n",
        "**15. Computational Power Limitations:** Limited computational power might restrict optimal model parameter tuning, hindering model performance maximization.\n",
        "\n",
        "**16. Data Restrictions and Inconsistencies:** Restrictions and mixing issues in the data could impact the training and inference effectiveness of the models.\n",
        "\n",
        "**17. Flaws in VaR Calculation Method:** Improvements are needed in the calculation method of Value at Risk, especially in enhancing predictions for extreme market situations.\n",
        "\n",
        "**18. Underutilization of GPT for Scenario Enhancement:** Although considering the use of GPT for enhancing scenario analysis, its capabilities might not have been fully utilized, especially in dealing with complex and uncertain market conditions.\n",
        "\n",
        "**19. Insufficient Mapping and Correlation in Sector Analysis:** The research on the Australian Healthcare Technology Sector stocks might lack sufficient mapping and analysis of correlations between asset classes.\n",
        "\n",
        "**20. Issues with Not Importing Real Data:** For fund options, the failure to import real data and relying only on simulations could affect the models' realism and applicability.\n",
        "\n",
        "**21. Inadequate Assessment of Company Health:** Without fundamental analysis, there's a limited understanding of the actual financial health and stability of the companies within the Australian stock market. This can lead to inaccurate valuations and risk assessments.\n",
        "\n",
        "**22. Missed Insights into Market Dynamics:** Fundamental analysis often provides deep insights into market trends, sector dynamics, and economic indicators. Its absence means missing out on these crucial insights, which can inform better investment decisions.\n",
        "\n",
        "**23. Challenges in Long-Term Forecasting:** Fundamental analysis plays a key role in long-term forecasting and in assessing the sustainability of a company's growth. Not incorporating it can weaken the long-term forecasting ability of your models.\n",
        "\n",
        "......\n",
        "\n",
        "\n",
        "Details Update / Parameters / Names abt Genetic Alogrithm"
      ],
      "metadata": {
        "id": "PCaCYwA669hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data From:\n",
        "\n",
        "https://www.morningstar.com.au/\n",
        "\n",
        "https://www.tigerbrokers.com.au/\n",
        "\n",
        "https://www.google.com/finance/?hl=en\n",
        "\n",
        "https://www.finance.gov.au/\n",
        "\n",
        "https://www.bis.org/bcbs/\n",
        "\n",
        "https://www.rba.gov.au/"
      ],
      "metadata": {
        "id": "R5V3bGotw6EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All outputs in below link\n",
        "\n",
        "https://colab.research.google.com/drive/1ViG2YY5FUOomsTtv3CNzXVX_ViSy_pgy?usp=sharing"
      ],
      "metadata": {
        "id": "Dt_cC5t7YDSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Colab Running Information**"
      ],
      "metadata": {
        "id": "UfbdIPYv7wSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Package**"
      ],
      "metadata": {
        "id": "ozKwot_r73co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch\n",
        "!pip install deap\n",
        "!pip install tensorflow\n",
        "!pip install scikit-optimize\n",
        "!pip install copulas\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install sumy\n",
        "!pip install PyPDF2\n",
        "!pip install summa\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch] -U\n",
        "!pip install datasets\n",
        "!pip install keras-tuner\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "g4dsrUkDEvkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDPU6dXtBa_-"
      },
      "outputs": [],
      "source": [
        "# Data Processing and Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import gspread\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Machine Learning and Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Concatenate, BatchNormalization, Reshape, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Natural Language Processing (NLP) and Transformers\n",
        "import spacy\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from datasets import Dataset, load_dataset\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from summa import summarizer\n",
        "\n",
        "# Network Analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Statistics and Optimization\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm, t, uniform\n",
        "from arch import arch_model\n",
        "\n",
        "# Miscellaneous\n",
        "import random\n",
        "import PyPDF2\n",
        "import re\n",
        "import requests\n",
        "from getpass import getpass\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from deap import base, creator, tools, algorithms\n",
        "from skopt import BayesSearchCV\n",
        "from copulas.multivariate import GaussianMultivariate\n",
        "from kerastuner import HyperParameters, Tuner\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Reading**"
      ],
      "metadata": {
        "id": "KeUX1C1mi5VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BK7094 Stock Data"
      ],
      "metadata": {
        "id": "4qwBOoQDXBQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "    df['Date'] = df['Date'].str.split().str[0]\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y').dt.date\n",
        "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "    return df"
      ],
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names = [\"ICR\",\"CTQ\",\"AYA\",\"AHI\",\"TRI\",\n",
        "               \"MDR\",\"M7T\",\"PME\",\"VHT\", \"SHG\",\n",
        "               \"PCK\",\"JTL\",\"IME\",\"DOC\",\"ALC\",\n",
        "               \"4DX\",\"ONE\",\"CGS\",\"GLH\",\"HIQ\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Data"
      ],
      "metadata": {
        "id": "gwIxiU4rfqpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = [\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Bullish.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Bearish.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Netrual.pdf'\n",
        "]\n",
        "for path in pdf_paths:\n",
        "    with open(path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        print(f\"Contents of {path} (first page):\")\n",
        "        first_page = reader.pages[0]\n",
        "        print(first_page.extract_text())\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "E-k-HoULfq4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Economic Situation Data"
      ],
      "metadata": {
        "id": "BYnNJzFzIxAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = [\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/rba-annual-report-2023.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/rba-annual-report-2022.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/2021-report.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/statement-on-monetary-policy-2023-11.pdf',\n",
        "    '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/statement-on-monetary-policy-2023-08.pdf'\n",
        "]\n",
        "\n",
        "\n",
        "reports = []\n",
        "for path in pdf_paths:\n",
        "    with open(path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + '\\n'\n",
        "        reports.append(text)\n",
        "\n",
        "df_reports = pd.DataFrame({'report': reports})\n",
        "print(df_reports.head())"
      ],
      "metadata": {
        "id": "EmJz1oGaIxMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Data Import"
      ],
      "metadata": {
        "id": "qn2SiHgXv0PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass('Enter your API key:')"
      ],
      "metadata": {
        "id": "a1adnV77fG_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"https://newsapi.org/v2/everything?q=economy&apiKey={api_key}\"\n",
        "\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "articles = data['articles']\n",
        "for article in articles:\n",
        "    print(article['title'])\n",
        "    print(article['description'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "MbyNdfCGg_pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "for article in articles:\n",
        "    title = article['title']\n",
        "    sentiment = sentiment_pipeline(title)[0]\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Sentiment: {sentiment['label']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "oQFyis7voI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentiment_label(sentiment_label):\n",
        "    mapping = {\n",
        "        'POSITIVE': 'Bullish',\n",
        "        'NEGATIVE': 'Bearish',\n",
        "        'NEUTRAL': 'Neutral'\n",
        "    }\n",
        "    return mapping.get(sentiment_label, 'Neutral')"
      ],
      "metadata": {
        "id": "ipR-owMyolql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in articles:\n",
        "    title = article['title']\n",
        "    sentiment_result = sentiment_pipeline(title)[0]\n",
        "    custom_label = map_sentiment_label(sentiment_result['label'])\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Custom Sentiment: {custom_label}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "yX_ym3s_olxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Integration"
      ],
      "metadata": {
        "id": "d735pgS7YQk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = {\n",
        "    'Bullish': '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Bullish.pdf',\n",
        "    'Bearish': '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Bearish.pdf',\n",
        "    'Neutral': '/content/drive/My Drive/Colab Notebooks/BK7094 Modeling Market Risk Data/Training Text pdf form/Netrual.pdf'\n",
        "}\n",
        "\n",
        "data = []\n",
        "for label, path in pdf_paths.items():\n",
        "    with open(path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            data.append({'text': text, 'label': label})\n",
        "\n",
        "# Fetch the articles\n",
        "response = requests.get(url)\n",
        "articles = response.json()['articles']\n",
        "\n",
        "# Initialize the sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Process the articles and add them to your data list\n",
        "for article in articles:\n",
        "    title = article['title']\n",
        "    sentiment_result = sentiment_pipeline(title)[0]\n",
        "    custom_label = map_sentiment_label(sentiment_result['label'])\n",
        "    data.append({'text': title, 'label': custom_label})\n",
        "\n",
        "# Create the final DataFrame\n",
        "final_df_nlp = pd.DataFrame(data)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(final_df_nlp.head())"
      ],
      "metadata": {
        "id": "VDtSrD3MX1cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visulization"
      ],
      "metadata": {
        "id": "nEFMbIP-zSsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes_forSeen = {name: get_sheet_data(name) for name in sheet_names}\n",
        "plt.figure(figsize=(12, 6))\n",
        "for name, df in dataframes_forSeen.items():\n",
        "    plt.plot(df['Date'], df['Close'], label=name)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Stock Closing Prices')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jNc2A449zki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and display a word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "text = ' '.join(final_df_nlp['text'])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Article Titles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6JE2Uxn7kin_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df_nlp)\n",
        "\n",
        "unique_labels = final_df_nlp['label'].unique()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "\n",
        "label_counts = final_df_nlp['label'].value_counts()\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ],
      "metadata": {
        "id": "4doU7NjHitHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Set Reinforcement**"
      ],
      "metadata": {
        "id": "CX2TyWWsOVHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstraping"
      ],
      "metadata": {
        "id": "DcpUmZoHvX4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_data(df, n_bootstraps=1):\n",
        "    bootstrap_samples = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        sample = df.sample(n=len(df), replace=True).copy(deep=True)\n",
        "        bootstrap_samples.append(sample)\n",
        "    return bootstrap_samples"
      ],
      "metadata": {
        "id": "zjLbKfk5OVAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootstraped_dataframes = [bootstrap_data(df) for df in dataframes]\n",
        "\n",
        "for i, bootstrapped_dfs in enumerate(bootstraped_dataframes):\n",
        "    print(f\"Original DataFrame {i}:\")\n",
        "    for j, df_sample in enumerate(bootstrapped_dfs):\n",
        "        print(f\"Bootstrap Sample {j + 1} Head:\")\n",
        "        print(df_sample.head())\n",
        "        print()"
      ],
      "metadata": {
        "id": "gFvs0h1Ouh-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short-Term Memory (LSTM)"
      ],
      "metadata": {
        "id": "3WBntsZpvZCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_lstm(df, feature_columns, target_column, n_steps):\n",
        "    df_copy = df.copy()\n",
        "    X, y = [], []\n",
        "    scalers = {}\n",
        "\n",
        "    for col in feature_columns:\n",
        "        scalers[col] = MinMaxScaler(feature_range=(0, 1))\n",
        "        df_copy[col] = scalers[col].fit_transform(df_copy[col].values.reshape(-1, 1))\n",
        "\n",
        "    for i in range(n_steps, len(df_copy)):\n",
        "        X.append(df_copy[feature_columns].iloc[i-n_steps:i].values)\n",
        "        y.append(df_copy[target_column].iloc[i])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], len(feature_columns)))\n",
        "\n",
        "    return X, y, scalers[target_column]"
      ],
      "metadata": {
        "id": "I5r8B8ISxamu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
        "        LSTM(50),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "3sV6QrIhu-_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 10\n",
        "\n",
        "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "target_column = 'Close'"
      ],
      "metadata": {
        "id": "p2aWFOI1OtO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_models = []\n",
        "predictions = []\n",
        "\n",
        "for df in dataframes:\n",
        "    X, y, scaler = prepare_data_for_lstm(df, feature_columns, target_column, n_steps)\n",
        "    model = build_lstm_model(X.shape[1:])\n",
        "    model.fit(X, y, epochs=10, batch_size=32)\n",
        "    lstm_models.append(model)\n",
        "    predictions.append(model.predict(X))"
      ],
      "metadata": {
        "id": "TNBQe-sWu_CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(X)\n",
        "predicted_original_scale = scaler.inverse_transform(predicted)\n",
        "new_df = df.copy(deep=True)\n",
        "\n",
        "new_df['Predicted_Close'] = np.nan\n",
        "\n",
        "start_idx = n_steps\n",
        "end_idx = start_idx + len(predicted_original_scale)\n",
        "\n",
        "new_df.loc[start_idx:end_idx-1, 'Predicted_Close'] = predicted_original_scale.squeeze()\n",
        "\n",
        "print(new_df)\n",
        "\n",
        "print(new_df['Predicted_Close'])"
      ],
      "metadata": {
        "id": "_uVdyUNZRemI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative Adversarial Networks (GAN)\n",
        "\n"
      ],
      "metadata": {
        "id": "WWw9PAiNvZ55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(seq_length, latent_dim, n_features=5):\n",
        "    input_noise = Input(shape=(latent_dim,))\n",
        "    x = Dense(128)(input_noise)\n",
        "    x = LeakyReLU(alpha=0.01)(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = Dense(seq_length * n_features)(x)\n",
        "    x = Reshape((seq_length, n_features))(x)\n",
        "    return Model(input_noise, x)"
      ],
      "metadata": {
        "id": "R7KRmtEivgFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    z = Input(shape=(latent_dim,))\n",
        "    fake_seq = generator(z)\n",
        "    discriminator.trainable = False\n",
        "    validity = discriminator(fake_seq)\n",
        "    return Model(z, validity)"
      ],
      "metadata": {
        "id": "pzLx-C5ivoYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(seq_length, n_features=5):\n",
        "    seq = Input(shape=(seq_length, n_features))\n",
        "    x = LSTM(64, return_sequences=True)(seq)\n",
        "    x = LSTM(64)(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(seq, x)"
      ],
      "metadata": {
        "id": "8PpmFd1pe45j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_create_sequences(df, selected_columns, seq_length):\n",
        "    data = df[selected_columns]\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    def create_sequences(data, seq_length):\n",
        "        xs, ys = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            x = data[i:(i + seq_length)]\n",
        "            y = data[i + seq_length]\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)\n",
        "\n",
        "    X, y = create_sequences(scaled_data, seq_length)\n",
        "    return X, y, scaler"
      ],
      "metadata": {
        "id": "8LP91xbceZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 32\n",
        "seq_length = 60\n",
        "processed_data = [preprocess_and_create_sequences(df, ['Open', 'High', 'Low', 'Close', 'Volume'], seq_length) for df in dataframes]"
      ],
      "metadata": {
        "id": "oe4aZv_Ef2Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator(seq_length, latent_dim, n_features=5)  # Make sure to match n_features\n",
        "discriminator = build_discriminator(seq_length, n_features=5)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
      ],
      "metadata": {
        "id": "OwMgqzXRvt3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, processed_data, epochs, batch_size, latent_dim):\n",
        "    for data_tuple in processed_data:\n",
        "        X, y, _ = data_tuple  # Assuming each tuple is (X, y, scaler)\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Randomly select real sequences\n",
        "            idx = np.random.randint(0, X.shape[0], batch_size)\n",
        "            real_seqs = X[idx]\n",
        "\n",
        "            # Generate fake sequences\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            fake_seqs = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator\n",
        "            d_loss_real = discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")"
      ],
      "metadata": {
        "id": "sN-o478pwqsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gan(generator, discriminator, gan, processed_data, epochs=3, batch_size=32, latent_dim=latent_dim)"
      ],
      "metadata": {
        "id": "hkwGO7WIwu0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(generator, latent_dim, num_samples):\n",
        "\n",
        "    noise = np.random.normal(0, 1, size=(num_samples, latent_dim))\n",
        "\n",
        "    generated_data = generator.predict(noise)\n",
        "\n",
        "    return generated_data"
      ],
      "metadata": {
        "id": "-4deqoo-VXw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "latent_dim = 32\n",
        "generated_data = generate_data(generator, latent_dim, num_samples)\n",
        "\n",
        "flattened_data = generated_data.reshape(-1, generated_data.shape[1] * generated_data.shape[2])\n",
        "\n",
        "columns = ['Price', 'Volume', 'Interest', 'Sentiment', 'Trend']\n",
        "flattened_columns = [f'{col}_t{i}' for i in range(generated_data.shape[1]) for col in columns]\n",
        "\n",
        "generated_df = pd.DataFrame(flattened_data, columns=flattened_columns)\n",
        "\n",
        "print(generated_df)"
      ],
      "metadata": {
        "id": "AXRNhLOCVX1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Individual Stock VaR Analysis**"
      ],
      "metadata": {
        "id": "IbwEUZPd9L-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistic Method"
      ],
      "metadata": {
        "id": "lilVXlmWwuL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_var(df, confidence_level=95):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Calculate VaR at the specified confidence level\n",
        "    var = np.percentile(df['Return'].dropna(), 100 - confidence_level)\n",
        "    return var"
      ],
      "metadata": {
        "id": "k6MuqYg-uy23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_var(df)\n",
        "    print(f\"VaR at 95% confidence level for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "xh-BqNGnwoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historical Maximum Loss"
      ],
      "metadata": {
        "id": "ssOPLGq6xyzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate VaR by finding the greatest loss\n",
        "def calculate_VaR_simple_loss(df):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Find the greatest loss (minimum return)\n",
        "    greatest_loss = df['Return'].min()\n",
        "    return greatest_loss"
      ],
      "metadata": {
        "id": "XyTAytO9x7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_simple_loss(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "b6PsufsrylFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historical & Time Decay Factor EWMA"
      ],
      "metadata": {
        "id": "qymWmLg4fBgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_time_decay(df, decay_factor=1):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Apply exponential weighting\n",
        "    weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "    weighted_returns = df['Return'] * weights\n",
        "    weighted_returns /= weights.sum()\n",
        "\n",
        "    # Find the greatest loss (minimum return) in weighted returns\n",
        "    greatest_loss = weighted_returns.min()\n",
        "    return greatest_loss"
      ],
      "metadata": {
        "id": "bxnQ9H2ldUal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_with_time_decay(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "58DkR0-qdU8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_time_decay_amend(df, decay_factor=1):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Handle NaN values in returns\n",
        "    df.dropna(subset=['Return'], inplace=True)\n",
        "\n",
        "    # When decay_factor is 1, use unweighted returns directly\n",
        "    if decay_factor == 1:\n",
        "        greatest_loss = df['Return'].min()\n",
        "    else:\n",
        "        # Apply exponential weighting\n",
        "        weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "        weighted_returns = df['Return'] * weights\n",
        "        weighted_returns /= weights.sum()\n",
        "\n",
        "        # Find the greatest loss in weighted returns\n",
        "        greatest_loss = weighted_returns.min()\n",
        "\n",
        "    return greatest_loss"
      ],
      "metadata": {
        "id": "YsviZ6Nh9N1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_with_time_decay_amend(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "yIHgYXaH9OD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CVaR  Expected Shortfall (ES) Tail VaR"
      ],
      "metadata": {
        "id": "3RbWiNqfaO_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_CVaR_with_time_decay(df, decay_factor=1, confidence_level=0.95):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Apply exponential weighting\n",
        "    weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "    weighted_returns = df['Return'] * weights\n",
        "    weighted_returns /= weights.sum()\n",
        "\n",
        "    # Find the VaR (Value at Risk)\n",
        "    VaR_threshold = np.percentile(weighted_returns.dropna(), (1 - confidence_level) * 100)\n",
        "\n",
        "    # Calculate CVaR (Conditional Value at Risk)\n",
        "    # Only consider the returns that are less than the VaR threshold\n",
        "    tail_losses = weighted_returns[weighted_returns < VaR_threshold]\n",
        "    CVaR = tail_losses.mean()  # Conditional VaR is the mean of the losses beyond the VaR threshold\n",
        "\n",
        "    return CVaR"
      ],
      "metadata": {
        "id": "JVPeji_daPX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating CVaR for each stock by identifying the conditional mean loss beyond the VaR threshold\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    cvar = calculate_CVaR_with_time_decay(df)\n",
        "    print(f\"CVaR (conditional mean loss) for {name}: {cvar*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Wi81N0NjCtLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo"
      ],
      "metadata": {
        "id": "055X-n-oxwjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual Stock"
      ],
      "metadata": {
        "id": "Tbjx_CqjikZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_stock_returns(initial_value, final_value, days=365):\n",
        "    \"\"\"\n",
        "    Calculate the annual and daily returns of a stock.\n",
        "\n",
        "    :param initial_value: The initial value of the stock.\n",
        "    :param final_value: The final value of the stock after a period.\n",
        "    :param days: The number of days over which the final value is measured. Default is 365 for one year.\n",
        "    :return: A tuple containing the annual return and daily return as percentages.\n",
        "    \"\"\"\n",
        "    # Calculate annual return\n",
        "    annual_return = ((final_value - initial_value) / initial_value) * 100\n",
        "\n",
        "    # Calculate daily return based on the number of days\n",
        "    daily_return = ((final_value / initial_value) ** (1/days) - 1) * 100\n",
        "\n",
        "    return annual_return, daily_return"
      ],
      "metadata": {
        "id": "3VftVtbotgXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate return"
      ],
      "metadata": {
        "id": "wx0ar6U4izqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "\n",
        "        initial_value = df['Close'].iloc[0]\n",
        "        final_value = df['Close'].iloc[-1]\n",
        "\n",
        "        days = (df['Date'].iloc[-1] - df['Date'].iloc[0]).days\n",
        "\n",
        "        annual_return, daily_return = calculate_stock_returns(initial_value, final_value, days)\n",
        "\n",
        "        print(name, f\"Annual Return: {annual_return:.2f}%, Daily Return: {daily_return:.4f}%\")"
      ],
      "metadata": {
        "id": "X_jpAbRm3PMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal Distribution"
      ],
      "metadata": {
        "id": "7HGP_OtqIIe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_MonteCarlo_Normal(df, days=252, iterations=10000, confidence_level=0.95, plot=False, plot_paths=False):\n",
        "    # Calculate daily returns\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    # Fit a GARCH model to estimate volatility\n",
        "    garch = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    model = garch.fit(disp='off')\n",
        "    forecast = model.forecast(horizon=days)\n",
        "    vol = np.sqrt(forecast.variance.iloc[-1])\n",
        "\n",
        "    # Calculate mean return\n",
        "    mean_return = returns.mean()\n",
        "    # Simulate returns using the normal distribution\n",
        "    simulated_returns = np.random.normal(mean_return, vol, (iterations, days))\n",
        "    # Calculate simulated price changes\n",
        "    simulated_price_changes = np.exp(simulated_returns) - 1\n",
        "\n",
        "    # Calculate VaR\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    if plot:\n",
        "      # Flatten the array to make it one-dimensional\n",
        "      flattened_simulated_changes = simulated_price_changes.flatten()\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.hist(flattened_simulated_changes, bins=50, alpha=0.7, color='red')\n",
        "      plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR*100:.2f}%\")\n",
        "      plt.title(f\"Simulated Price Changes Distribution\\nVaR (at {confidence_level*100}%): {VaR*100:.2f}%\")\n",
        "      plt.xlabel('Simulated Price Changes')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "    if plot_paths:\n",
        "        vol = np.sqrt(forecast.variance.dropna().mean(axis=0))\n",
        "        simulated_paths = np.zeros((iterations, days))\n",
        "\n",
        "        for i in range(iterations):\n",
        "            daily_returns = np.random.normal(mean_return, vol, days)\n",
        "            simulated_paths[i, :] = np.cumprod(1 + daily_returns) * df['Close'].iloc[-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i in range(iterations):\n",
        "            plt.plot(simulated_paths[i], alpha=0.2)\n",
        "\n",
        "        plt.title(f\"Monte Carlo Simulation Paths ({iterations} iterations)\")\n",
        "        plt.xlabel('Days')\n",
        "        plt.ylabel('Simulated Price')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "T8YdcDivGw-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T Distrubtion"
      ],
      "metadata": {
        "id": "L_CtSlKkIH5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_MonteCarlo_Advanced(df, days=252, iterations=10000, confidence_level=0.99, scale_factor=10, plot=False, plot_paths=False):\n",
        "    # Rescaling returns\n",
        "    returns = df['Close'].pct_change().dropna() * scale_factor\n",
        "\n",
        "    # GARCH model\n",
        "    garch = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    model = garch.fit(update_freq=10, disp='off')\n",
        "    forecast = model.forecast(horizon=days)\n",
        "    vol = np.sqrt(forecast.variance.iloc[-1].iloc[-1]) / scale_factor\n",
        "\n",
        "    # Fitting t-distribution\n",
        "    deg_freedom, loc, scale = t.fit(returns)\n",
        "    simulated_returns = t.rvs(deg_freedom, loc, scale, size=(iterations, days))\n",
        "\n",
        "    # Adjust for extreme values to prevent overflow\n",
        "    simulated_returns = np.clip(simulated_returns, a_min=np.percentile(simulated_returns, 1), a_max=np.percentile(simulated_returns, 99))\n",
        "\n",
        "    # Simulated price changes\n",
        "    simulated_price_changes = np.exp(simulated_returns * vol) - 1\n",
        "\n",
        "    # Remove or limit infinite values\n",
        "    simulated_price_changes = np.clip(simulated_price_changes, a_min=-np.inf, a_max=np.percentile(simulated_price_changes, 99.9))\n",
        "\n",
        "    # VaR calculation\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    if plot:\n",
        "        flattened_simulated_changes = simulated_price_changes.flatten()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(flattened_simulated_changes, bins=50, alpha=0.7, color='blue')\n",
        "        plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR*100:.2f}%\")\n",
        "        plt.title(f\"Simulated Price Changes Distribution\\nVaR (at {confidence_level*100}%): {VaR*100:.2f}%\")\n",
        "        plt.xlabel('Simulated Price Changes')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if plot_paths:\n",
        "        vol = np.sqrt(forecast.variance.iloc[-1].iloc[-1]) / scale_factor\n",
        "        simulated_paths = np.zeros((iterations, days))\n",
        "\n",
        "        for i in range(iterations):\n",
        "            daily_returns = t.rvs(deg_freedom, loc, scale, size=days) * vol\n",
        "            daily_returns = np.clip(daily_returns, a_min=-np.inf, a_max=np.percentile(daily_returns, 99.9))\n",
        "            simulated_paths[i, :] = np.cumprod(1 + daily_returns) * df['Close'].iloc[-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i in range(iterations):\n",
        "            plt.plot(simulated_paths[i], alpha=0.2)\n",
        "\n",
        "        plt.title(f\"Monte Carlo Simulation Paths ({iterations} iterations)\")\n",
        "        plt.xlabel('Days')\n",
        "        plt.ylabel('Simulated Price')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "yZ3MtxGQGw2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "        # Call the function for the normal distribution\n",
        "        var_normal = calculate_VaR_MonteCarlo_Normal(df, days=252, iterations=100, confidence_level=0.95, plot=True, plot_paths=True)\n",
        "        print(f\"Normal VaR (greatest loss) for {name}: {var_normal*100:.2f}%\")\n",
        "\n",
        "        # Call the function for the t-distribution\n",
        "        var_advanced = calculate_VaR_MonteCarlo_Advanced(df, days=252, iterations=100, confidence_level=0.99, scale_factor=10, plot=True, plot_paths=True)\n",
        "        print(f\"Advanced VaR (greatest loss) for {name}: {var_advanced*100:.2f}%\")"
      ],
      "metadata": {
        "id": "e3MZDySsfskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price-Simulated"
      ],
      "metadata": {
        "id": "i8zERNQbIhsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_monte_carlo_simulation(df, days, iterations, confidence_level, plot=True, plot_subset=100):\n",
        "\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "    returns = returns.clip(lower=returns.quantile(0.01), upper=returns.quantile(0.99))\n",
        "\n",
        "    mean_return = returns.mean()\n",
        "    std_dev = returns.std()\n",
        "\n",
        "    garch = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    model = garch.fit(disp='off')\n",
        "    forecast = model.forecast(horizon=days)\n",
        "    forecasted_vol = np.sqrt(forecast.variance.iloc[-1].mean())\n",
        "\n",
        "    df_param, loc, scale = t.fit(returns)\n",
        "\n",
        "    normal_simulated_returns = norm.rvs(mean_return, forecasted_vol, (iterations, days))\n",
        "    t_simulated_returns = t.rvs(df_param, loc, scale, size=(iterations, days))\n",
        "\n",
        "    initial_price = df['Close'].iloc[-1]\n",
        "    normal_price_paths = initial_price * np.cumprod(1 + normal_simulated_returns, axis=1)\n",
        "    t_price_paths = initial_price * np.cumprod(1 + t_simulated_returns, axis=1)\n",
        "\n",
        "    normal_final_prices = normal_price_paths[:, -1]\n",
        "    t_final_prices = t_price_paths[:, -1]\n",
        "    normal_price_VaR = np.percentile(normal_final_prices, 100 - (confidence_level * 100))\n",
        "    t_price_VaR = np.percentile(t_final_prices, 100 - (confidence_level * 100))\n",
        "\n",
        "    normal_price_VaR_percent = (normal_price_VaR - initial_price) / initial_price * 100\n",
        "    t_price_VaR_percent = (t_price_VaR - initial_price) / initial_price * 100\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        subset_indices = np.random.choice(range(iterations), size=plot_subset, replace=False)\n",
        "        plt.plot(normal_price_paths[subset_indices].T, alpha=0.1, color='blue')\n",
        "        plt.plot(t_price_paths[subset_indices].T, alpha=0.1, color='red')\n",
        "        plt.title(f\"Combined Monte Carlo Simulation ({iterations} simulations)\")\n",
        "        plt.xlabel('Days')\n",
        "        plt.ylabel('Simulated Price Paths')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(normal_final_prices, bins=50, alpha=0.7, color='blue')\n",
        "        plt.hist(t_final_prices, bins=50, alpha=0.7, color='red')\n",
        "        plt.axvline(x=normal_price_VaR, color='navy', linestyle='--', label=f\"Normal VaR: {normal_price_VaR_percent:.2f}%\")\n",
        "        plt.axvline(x=t_price_VaR, color='darkred', linestyle='--', label=f\"t-Distribution VaR: {t_price_VaR_percent:.2f}%\")\n",
        "        plt.title(f\"Distribution of Simulated Final Prices\\nNormal VaR: {normal_price_VaR_percent:.2f}%, t-Distribution VaR: {t_price_VaR_percent:.2f}%\")\n",
        "        plt.xlabel('Simulated Final Prices')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return normal_price_VaR_percent, t_price_VaR_percent"
      ],
      "metadata": {
        "id": "aMs7MFetfsb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "        var_normal, var_advanced = combined_monte_carlo_simulation(\n",
        "            df,\n",
        "            days=252,\n",
        "            iterations=10000,\n",
        "            confidence_level=0.9,\n",
        "            plot=True,\n",
        "            plot_subset=100\n",
        "        )\n",
        "    print(f\"Normal VaR (greatest loss) for {name}: {var_normal:.2f}%\")\n",
        "    print(f\"Advanced VaR (greatest loss) for {name}: {var_advanced:.2f}%\")"
      ],
      "metadata": {
        "id": "YupwbzDxgm4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return"
      ],
      "metadata": {
        "id": "tdCrxEinIlhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_VaR_distributions(dataframes, weights, confidence_level):\n",
        "    # Calculate daily returns for each stock\n",
        "    return_frames = [df['Close'].pct_change().dropna() for df in dataframes]\n",
        "\n",
        "    # Combine returns into a single DataFrame\n",
        "    combined_returns = pd.concat(return_frames, axis=1)\n",
        "\n",
        "    # Clean the data: Remove rows with non-finite values (NaN, inf, -inf)\n",
        "    combined_returns = combined_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    # Calculate portfolio's daily returns\n",
        "    portfolio_returns = combined_returns.dot(weights)\n",
        "\n",
        "    # Fit a normal distribution\n",
        "    mean, std = norm.fit(portfolio_returns)\n",
        "\n",
        "    # Fit a t-distribution\n",
        "    df, loc, scale = t.fit(portfolio_returns)\n",
        "\n",
        "    # Calculate VaR for normal distribution\n",
        "    VaR_normal = norm.ppf(1 - confidence_level, mean, std)\n",
        "\n",
        "    # Calculate VaR for t-distribution\n",
        "    VaR_t = t.ppf(1 - confidence_level, df, loc, scale)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(portfolio_returns, bins=50, alpha=0.7, color='blue', density=True, label='Empirical')\n",
        "\n",
        "    # Plot normal distribution\n",
        "    x = np.linspace(min(portfolio_returns), max(portfolio_returns), 100)\n",
        "    plt.plot(x, norm.pdf(x, mean, std), 'r-', lw=2, label='Normal Distribution')\n",
        "    plt.axvline(x=VaR_normal, color='red', linestyle='dashed', linewidth=2, label=f'Normal VaR: {VaR_normal}')\n",
        "\n",
        "    # Plot t-distribution\n",
        "    plt.plot(x, t.pdf(x, df, loc, scale), 'g-', lw=2, label='t-Distribution')\n",
        "    plt.axvline(x=VaR_t, color='green', linestyle='dashed', linewidth=2, label=f't-VaR: {VaR_t}')\n",
        "\n",
        "    plt.title(\"Comparison of VaR Using Normal and t-Distributions\")\n",
        "    plt.xlabel(\"Returns\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR_normal, VaR_t"
      ],
      "metadata": {
        "id": "Z8P2b2GvM6g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_level = 0.95  # Confidence level\n",
        "num_stocks = len(sheet_names)\n",
        "equal_weights = [1/num_stocks for _ in range(num_stocks)]\n",
        "# Compare VaR\n",
        "VaR_normal, VaR_t = compare_VaR_distributions(dataframes, equal_weights, confidence_level)\n",
        "print(f\"Calculated VaR (Normal): {VaR_normal}, VaR (t-Distribution): {VaR_t}\")"
      ],
      "metadata": {
        "id": "pjnUKpCrM7cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Portfolio Analysis**"
      ],
      "metadata": {
        "id": "pEA9oN6Sy_vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Caculation"
      ],
      "metadata": {
        "id": "U6sR8omB-XzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_real_time_price(api_key, symbol, market='ASX'):\n",
        "    full_symbol = f\"{market}:{symbol}\"\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/quote/{full_symbol}?apikey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    try:\n",
        "        if not data:\n",
        "            print(f\"No data available for symbol: {full_symbol}\")\n",
        "            return None\n",
        "        real_time_price = data[0]['price']\n",
        "        return real_time_price\n",
        "    except (KeyError, IndexError):\n",
        "        print(f\"Error retrieving data for symbol: {full_symbol}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "KFd6lkRE1Rgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_portfolio_weights(amount, stock_prices, stock_quantities):\n",
        "    # Calculate the total investment for each stock\n",
        "    total_investment_per_stock = [price * quantity for price, quantity in zip(stock_prices, stock_quantities)]\n",
        "\n",
        "    # Calculate the total investment in the portfolio\n",
        "    total_investment = sum(total_investment_per_stock)\n",
        "\n",
        "    # Check if the total investment exceeds the provided amount\n",
        "    if total_investment > amount:\n",
        "        raise ValueError(\"The total investment exceeds the provided amount.\")\n",
        "\n",
        "    # Calculate the weights\n",
        "    weights = [investment / total_investment for investment in total_investment_per_stock]\n",
        "\n",
        "    # Normalize weights so they sum up to 1\n",
        "    total_weights = sum(weights)\n",
        "    normalized_weights = [weight / total_weights for weight in weights]\n",
        "\n",
        "    # Visualization\n",
        "    bottom = 0\n",
        "    for i, (weight, name) in enumerate(zip(normalized_weights, sheet_names)):\n",
        "        plt.bar('Portfolio', weight, bottom=bottom, label=name)\n",
        "        bottom += weight\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title('Portfolio Weights Visualization')\n",
        "    plt.ylabel('Weight')\n",
        "    plt.show()\n",
        "\n",
        "    return normalized_weights"
      ],
      "metadata": {
        "id": "5yqnroS7ARY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_investment_by_weights(amount, weights):\n",
        "    # Ensure the sum of weights equals 1\n",
        "    if sum(weights) != 1:\n",
        "        raise ValueError(\"The sum of the weights must equal 1.\")\n",
        "\n",
        "    # Calculate the investment amount for each stock\n",
        "    investment_per_stock = [amount * weight for weight in weights]\n",
        "\n",
        "    return investment_per_stock"
      ],
      "metadata": {
        "id": "qMEDtw26Ag1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass('Enter your API key: ')\n",
        "stock_symbols = sheet_names"
      ],
      "metadata": {
        "id": "FlVZ_9x01UHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_time_stock_prices = [get_real_time_price(api_key, symbol) for symbol in sheet_names]\n",
        "\n",
        "# Replace None values with 0.3\n",
        "real_time_stock_prices = [price if price is not None else 0.3 for price in real_time_stock_prices]\n",
        "\n",
        "print(f\"The latest closing price of {sheet_names} is: {real_time_stock_prices}\")"
      ],
      "metadata": {
        "id": "4WWm8uB715wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_quantities = [10, 15, 12, 20, 18, 5, 8, 25, 7, 9,\n",
        "                    11, 14, 13, 19, 17, 6, 10, 23, 8, 10]\n",
        "\n",
        "initial_amount = 10000  # Example amount\n",
        "\n",
        "weights = calculate_portfolio_weights(initial_amount, real_time_stock_prices, stock_quantities)\n",
        "print(f\"Weights of each stock in the portfolio: {weights}\")\n",
        "\n",
        "investment_per_stock = calculate_investment_by_weights(initial_amount, weights)\n",
        "print(f\"Investment in each stock: {investment_per_stock}\")"
      ],
      "metadata": {
        "id": "byARHfRvASP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Historical Method**"
      ],
      "metadata": {
        "id": "jPYWRPpg-Ng4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_plot_historical_VaR(dataframes, weights, confidence_level):\n",
        "    # Calculate daily returns for each stock\n",
        "    return_frames = [df['Close'].pct_change().dropna() for df in dataframes]\n",
        "\n",
        "    # Combine returns into a single DataFrame\n",
        "    combined_returns = pd.concat(return_frames, axis=1)\n",
        "\n",
        "    # Calculate portfolio's daily returns\n",
        "    portfolio_returns = combined_returns.dot(weights)\n",
        "\n",
        "    # Sort the portfolio returns\n",
        "    sorted_returns = portfolio_returns.sort_values()\n",
        "\n",
        "    # Calculate VaR\n",
        "    VaR_index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    VaR = sorted_returns.iloc[VaR_index]\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(portfolio_returns, bins=50, alpha=0.7, color='blue')\n",
        "    plt.axvline(x=VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title(\"Portfolio Returns Distribution with VaR\")\n",
        "    plt.xlabel(\"Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "HDeGXWeDCa95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_plot_VaR_CVaR(dataframes, weights, confidence_level, lambda_decay):\n",
        "    # Calculate daily returns for each stock with time decay applied\n",
        "    return_frames = []\n",
        "    for df in dataframes:\n",
        "        daily_returns = df['Close'].pct_change().dropna()\n",
        "        # Apply time decay (exponential weighting)\n",
        "        daily_returns_weighted = daily_returns.ewm(alpha=lambda_decay).mean()\n",
        "        return_frames.append(daily_returns_weighted)\n",
        "\n",
        "    # Combine returns data\n",
        "    combined_returns = pd.concat(return_frames, axis=1)\n",
        "    portfolio_returns = combined_returns.dot(weights)\n",
        "\n",
        "    # Calculate VaR\n",
        "    sorted_returns = portfolio_returns.sort_values()\n",
        "    VaR_index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    VaR = sorted_returns.iloc[VaR_index]\n",
        "\n",
        "    # Calculate CVaR\n",
        "    CVaR = sorted_returns[sorted_returns <= VaR].mean()\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(portfolio_returns, bins=50, alpha=0.7, color='blue')\n",
        "    plt.axvline(x=VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.axvline(x=CVaR, color='green', linestyle='dashed', linewidth=2, label=f'CVaR: {CVaR}')\n",
        "    plt.title(\"Portfolio Returns Distribution with VaR and CVaR\")\n",
        "    plt.xlabel(\"Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR, CVaR"
      ],
      "metadata": {
        "id": "DxdImmtBKNEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_level = 0.95\n",
        "lambda_decay = 0.94\n",
        "\n",
        "# Calculate and visualize VaR and CVaR\n",
        "VaR, CVaR = calculate_and_plot_VaR_CVaR(dataframes, weights, confidence_level, lambda_decay)\n",
        "print(f\"Calculated VaR: {VaR}, CVaR: {CVaR}\")"
      ],
      "metadata": {
        "id": "o9IooVq-KQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistic Method**"
      ],
      "metadata": {
        "id": "4RV1DDZp-NkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_plot_statistical_VaR(dataframes, weights, confidence_level, time_horizon=1):\n",
        "    # Calculate daily returns for each stock\n",
        "    return_frames = [df['Close'].pct_change().dropna() for df in dataframes]\n",
        "\n",
        "    # Combine returns into a single DataFrame\n",
        "    combined_returns = pd.concat(return_frames, axis=1)\n",
        "\n",
        "    # Calculate portfolio's daily returns\n",
        "    portfolio_returns = combined_returns.dot(weights)\n",
        "\n",
        "    # Compute portfolio standard deviation (volatility)\n",
        "    portfolio_std = np.std(portfolio_returns)\n",
        "\n",
        "    # Calculate VaR\n",
        "    Z_score = norm.ppf(confidence_level)\n",
        "    VaR = Z_score * portfolio_std * np.sqrt(time_horizon)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(portfolio_returns, bins=50, alpha=0.7, color='blue')\n",
        "    plt.axvline(x=-VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title(\"Portfolio Returns Distribution with VaR\")\n",
        "    plt.xlabel(\"Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "eUEMD07hLSJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_plot_t_VaR(dataframes, weights, confidence_level, time_horizon=1):\n",
        "\n",
        "    return_frames = [df['Close'].pct_change().dropna() for df in dataframes]\n",
        "\n",
        "    # Combine returns into a single DataFrame\n",
        "    combined_returns = pd.concat(return_frames, axis=1)\n",
        "\n",
        "    # Clean the data: Remove rows with non-finite values (NaN, inf, -inf)\n",
        "    combined_returns = combined_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    # Calculate portfolio's daily returns\n",
        "    portfolio_returns = combined_returns.dot(weights)\n",
        "\n",
        "    # Fit a t-distribution to the portfolio returns\n",
        "    df, loc, scale = t.fit(portfolio_returns)\n",
        "\n",
        "    # Calculate VaR\n",
        "    VaR = t.ppf(1 - confidence_level, df, loc=loc, scale=scale) * np.sqrt(time_horizon)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(portfolio_returns, bins=50, alpha=0.7, color='blue', density=True)\n",
        "    x = np.linspace(min(portfolio_returns), max(portfolio_returns), 100)\n",
        "    plt.plot(x, t.pdf(x, df, loc, scale), 'r-', lw=2, label='t-distribution')\n",
        "    plt.axvline(x=VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title(\"Portfolio Returns Distribution with VaR (t-Distribution)\")\n",
        "    plt.xlabel(\"Returns\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "ye4tRZDrMSbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onfidence_level = 0.95  # Confidence level\n",
        "# Calculate and visualize VaR\n",
        "VaR = calculate_and_plot_statistical_VaR(dataframes, weights, confidence_level)\n",
        "print(f\"Calculated VaR at {confidence_level*100}% confidence level: {VaR}\")"
      ],
      "metadata": {
        "id": "kGHiB1L3LSv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_level = 0.95  # Confidence level\n",
        "# Calculate and visualize VaR\n",
        "VaR = calculate_and_plot_t_VaR(dataframes, weights, confidence_level)\n",
        "print(f\"Calculated VaR using t-distribution at {confidence_level*100}% confidence level: {VaR}\")"
      ],
      "metadata": {
        "id": "W__maFnMMSr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monte-Carlo Method**"
      ],
      "metadata": {
        "id": "i6GoVfp--dh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Function\n"
      ],
      "metadata": {
        "id": "nhLsgkHBwJbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(final_returns, VaR, CVaR, confidence_level):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(final_returns, kde=True, color='blue', bins=50)\n",
        "    plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR:.2f}\")\n",
        "    plt.axvline(x=CVaR, color='green', linestyle='--', label=f\"CVaR: {CVaR:.2f}\")\n",
        "    plt.title(\"Portfolio Return Distribution with VaR and CVaR\")\n",
        "    plt.xlabel(\"Portfolio Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kjm6eBrndLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_simulations(simulated_prices, simulated_returns, VaR, CVaR, confidence_level):\n",
        "    # Plot the simulated prices\n",
        "    plt.figure(figsize=(21, 10))  # Adjusting figure size for prices plot\n",
        "    for i in range(simulated_prices.shape[0]):\n",
        "        plt.plot(simulated_prices[i], alpha=0.5)\n",
        "    plt.title(\"Simulated Portfolio Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Prices\")\n",
        "    plt.show()  # Display the prices plot\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    # Plot the simulated returns\n",
        "    plt.figure(figsize=(21, 10))  # Adjusting figure size for returns plot\n",
        "    for i in range(simulated_returns.shape[0]):\n",
        "        plt.plot(simulated_returns[i], alpha=0.5)\n",
        "    plt.title(\"Simulated Portfolio Returns\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Returns\")\n",
        "\n",
        "    # Check if VaR and CVaR are numbers and not NaN before plotting\n",
        "    if VaR and not np.isnan(VaR):\n",
        "        plt.axhline(y=-VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {-VaR}')\n",
        "    if CVaR and not np.isnan(CVaR):\n",
        "        plt.axhline(y=-CVaR, color='orange', linestyle='dashed', linewidth=2, label=f'CVaR: {-CVaR}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()  # Display the returns plot"
      ],
      "metadata": {
        "id": "D2rSUwC7RcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_monte_carlo_simulation(simulated_portfolio_returns, VaR, CVaR, confidence_level):\n",
        "    plt.figure(figsize=(21, 14))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    colors = sns.color_palette(\"hsv\", len(simulated_portfolio_returns))\n",
        "    for i in range(len(simulated_portfolio_returns)):\n",
        "        plt.plot(simulated_portfolio_returns[i, :], color=colors[i], alpha=0.5)\n",
        "    plt.axhline(y=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR:.2f}\")\n",
        "    plt.axhline(y=CVaR, color='green', linestyle='--', label=f\"CVaR: {CVaR:.2f}\")\n",
        "    plt.title(f\"Monte Carlo Simulation of Portfolio\")\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Simulated Portfolio Returns')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vgK-xJ5LdLIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation_heatmap(combined_returns):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(combined_returns.corr(), annot=True, cmap='viridis')\n",
        "    plt.title(\"Assets Correlation Heatmap\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "znyfW_OCdLK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_VaR_impacts(VaR_impact):\n",
        "    categories = list(VaR_impact.keys())\n",
        "    impacts = list(VaR_impact.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(categories, impacts, color='skyblue')\n",
        "    plt.xlabel('Analysis Type')\n",
        "    plt.ylabel('Adjusted VaR')\n",
        "    plt.title('Impact on VaR by Different Analyses')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MNaj3FKmihEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic VaR"
      ],
      "metadata": {
        "id": "SXGFN0ez1RCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_t_distribution(returns, confidence_level=0.99, iterations=10000):\n",
        "    # Fit t-distribution to the data\n",
        "    df, loc, scale = t.fit(returns)\n",
        "    # Simulate returns\n",
        "    simulated_returns = t.rvs(df, loc, scale, size=iterations)\n",
        "    # Calculate VaR\n",
        "    VaR = np.percentile(simulated_returns, (1 - confidence_level) * 100)\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "GVC-bOkWf_R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liquidity Risk"
      ],
      "metadata": {
        "id": "8mTTQi2Kgf9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_liquidity_risk(dataframes, liquidity_threshold):\n",
        "    liquidity_risk = {}\n",
        "    for df, name in zip(dataframes, sheet_names):\n",
        "        average_volume = df['Volume'].mean()\n",
        "        liquidity_risk[name] = 'High' if average_volume < liquidity_threshold else 'Low'\n",
        "    return liquidity_risk"
      ],
      "metadata": {
        "id": "qoHK-1vXghNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stress Testing"
      ],
      "metadata": {
        "id": "Af3v_uY1ggF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stress_test(dataframes, weights, stress_factors):\n",
        "    stress_results = {}\n",
        "    for factor, change in stress_factors.items():\n",
        "        stressed_returns = []\n",
        "        for df, weight in zip(dataframes, weights):\n",
        "            stressed_price = df['Close'] * (1 + change)\n",
        "            stressed_return = stressed_price.pct_change().dropna()\n",
        "            stressed_returns.append(stressed_return * weight)\n",
        "        portfolio_stressed_return = pd.concat(stressed_returns, axis=1).sum(axis=1)\n",
        "        stress_results[factor] = portfolio_stressed_return\n",
        "    return stress_results"
      ],
      "metadata": {
        "id": "eNRWtUXUghj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario Analysis"
      ],
      "metadata": {
        "id": "ElKyoDVGggM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_scenario_analysis(dataframes, weights, scenarios):\n",
        "    scenario_results = {}\n",
        "    for scenario_name, scenario_changes in scenarios.items():\n",
        "        scenario_portfolio_returns = pd.DataFrame()\n",
        "        for df, weight, name in zip(dataframes, weights, sheet_names):\n",
        "            scenario_df = df.copy()\n",
        "            for column, change in scenario_changes.items():\n",
        "                if column in scenario_df.columns:\n",
        "                    scenario_df[column] *= (1 + change)\n",
        "            scenario_portfolio_returns[name] = scenario_df[column].pct_change() * weight\n",
        "        total_scenario_return = scenario_portfolio_returns.sum(axis=1)\n",
        "        scenario_VaR = np.percentile(total_scenario_return.dropna(), 5)\n",
        "        scenario_results[scenario_name] = scenario_VaR\n",
        "    return scenario_results"
      ],
      "metadata": {
        "id": "_dNIC94Hgh5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VaR Impact"
      ],
      "metadata": {
        "id": "VztbPR_Hc0Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_VaR_impact(dataframes, weights, sheet_names, liquidity_threshold, stress_factors, scenarios, VaR, confidence_level=0.99):\n",
        "\n",
        "    liquidity_risks = calculate_liquidity_risk(dataframes, liquidity_threshold)\n",
        "    stress_test_results = stress_test(dataframes, weights, stress_factors)\n",
        "    scenario_analysis_results = conduct_scenario_analysis(dataframes, weights, scenarios)\n",
        "\n",
        "    adjusted_VaR = {\n",
        "        'Liquidity Risk': {},\n",
        "        'Stress Test': {},\n",
        "        'Scenario Analysis': {}\n",
        "    }\n",
        "\n",
        "    for name, risk in liquidity_risks.items():\n",
        "        adjusted_VaR['Liquidity Risk'][name] = VaR * 1.1 if risk == 'High' else VaR\n",
        "\n",
        "    for condition, returns in stress_test_results.items():\n",
        "        adjusted_VaR['Stress Test'][condition] = calculate_VaR_t_distribution(returns, confidence_level)\n",
        "\n",
        "    for scenario, returns in scenario_analysis_results.items():\n",
        "        adjusted_VaR['Scenario Analysis'][scenario] = calculate_VaR_t_distribution(returns, confidence_level)\n",
        "\n",
        "    flat_adjusted_VaR = {}\n",
        "\n",
        "    for analysis_type, results in adjusted_VaR.items():\n",
        "        for name, value in results.items():\n",
        "            flat_adjusted_VaR[f\"{analysis_type} - {name}\"] = value\n",
        "\n",
        "    return flat_adjusted_VaR"
      ],
      "metadata": {
        "id": "IXGnV-L2hcf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters such as alpha need to be coordinated with the parameters of basic VaR"
      ],
      "metadata": {
        "id": "EnH2wQ6CVjm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_portfolio_analysis(dataframes, weights, sheet_names, days=252, iterations=100, confidence_level=0.99, increased_volatility_factor=1.5):\n",
        "    # Convert days and iterations to integers\n",
        "    days = int(days)\n",
        "    iterations = int(iterations)\n",
        "\n",
        "    # Combine the returns of the different dataframes\n",
        "    combined_returns = pd.DataFrame()\n",
        "    combined_prices = pd.DataFrame()\n",
        "    for df, weight, name in zip(dataframes, weights, sheet_names):\n",
        "        daily_returns = df['Close'].pct_change().dropna()\n",
        "        combined_returns[name] = daily_returns * weight\n",
        "        combined_prices[name] = df['Close'] * weight\n",
        "\n",
        "    portfolio_returns = combined_returns.sum(axis=1)  # Portfolio returns\n",
        "    portfolio_prices = combined_prices.sum(axis=1)  # Portfolio prices\n",
        "\n",
        "    # Initialize arrays for simulated portfolio prices and returns\n",
        "    simulated_portfolio_prices = np.zeros((iterations, days))\n",
        "    simulated_portfolio_returns = np.zeros((iterations, days))\n",
        "    initial_price = portfolio_prices.iloc[0]  # Initial portfolio price\n",
        "\n",
        "    # Run Monte Carlo simulation for prices and returns\n",
        "    for i in range(iterations):\n",
        "        prices = [initial_price]\n",
        "        for d in range(1, days):\n",
        "            simulated_return = np.random.normal(portfolio_returns.mean(), portfolio_returns.std() * increased_volatility_factor)\n",
        "            price = prices[d-1] * (1 + simulated_return)\n",
        "            prices.append(price)\n",
        "        simulated_portfolio_prices[i, :] = prices\n",
        "        simulated_portfolio_returns[i, :] = np.array(prices) / initial_price - 1\n",
        "\n",
        "    # Calculate final returns and VaR\n",
        "    final_returns = simulated_portfolio_returns[:, -1]\n",
        "    VaR = calculate_VaR_t_distribution(final_returns, confidence_level, iterations)\n",
        "\n",
        "    # Calculate CVaR if there are returns below VaR\n",
        "    returns_below_VaR = final_returns[final_returns <= VaR]\n",
        "    CVaR = np.nan  # Initialize CVaR as NaN\n",
        "    if len(returns_below_VaR) > 0:\n",
        "        CVaR = returns_below_VaR.mean()  # Calculate CVaR\n",
        "\n",
        "    # Call plotting functions\n",
        "    # Plotting the simulations with increased size\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plot_simulations(simulated_portfolio_prices, simulated_portfolio_returns, VaR, CVaR, confidence_level)\n",
        "    print(\"\\nGraph Description: Simulated Portfolio Price Trajectories\\n\")  # Description for the first plot\n",
        "\n",
        "    # Plotting the distribution of final returns\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plot_distribution(final_returns, VaR, CVaR, confidence_level)\n",
        "    print(\"\\nGraph Description: Distribution of Final Returns\\n\")  # Description for the second plot\n",
        "\n",
        "    # Plotting the Monte Carlo simulation\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plot_monte_carlo_simulation(simulated_portfolio_returns, VaR, CVaR, confidence_level)\n",
        "    print(\"\\nGraph Description: Monte Carlo Simulation Results\\n\")  # Description for the third plot\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plot_correlation_heatmap(combined_returns)\n",
        "    print(\"\\nGraph Description: Correlation Heatmap of Portfolio Returns\\n\")  # Description for the fourth plot\n",
        "\n",
        "    VaR_impact = analyze_VaR_impact(dataframes, weights, sheet_names, liquidity_threshold, stress_factors, scenarios, VaR, confidence_level)\n",
        "\n",
        "    plot_VaR_impacts(VaR_impact)\n",
        "\n",
        "    return VaR, CVaR, simulated_portfolio_prices, simulated_portfolio_returns"
      ],
      "metadata": {
        "id": "9gOpkUY6dLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liquidity_threshold = 100\n",
        "\n",
        "stress_factors = {\n",
        "    'Market Drop': -0.10,\n",
        "    'Interest Rate Rise': 0.05\n",
        "}\n",
        "\n",
        "scenarios = {\n",
        "    'Economic Boom': {'Close': 0.1},\n",
        "    'Market Crash': {'Close': -0.2}\n",
        "}"
      ],
      "metadata": {
        "id": "uQ-xmfKxrhGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing *(NLP)*"
      ],
      "metadata": {
        "id": "BZ45TRzjFD8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prestored Function"
      ],
      "metadata": {
        "id": "-nFPUmhr9p0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_gdp_growth(text):\n",
        "    pattern = r\"GDP growth rate is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "HI-HMdCvnPBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unemployment_rate(text):\n",
        "    pattern = r\"unemployment rate is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "Gwf7jOS6agVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_inflation_rate(text):\n",
        "    pattern = r\"inflation rate is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "2EfXcMYfagXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_interest_rate(text):\n",
        "    pattern = r\"interest rate is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "oxdxFvua9pVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_budget_deficit(text):\n",
        "    pattern = r\"budget deficit is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "hkZzQztH9uqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_trade_balance(text):\n",
        "    pattern = r\"trade balance is (\\d+\\.?\\d*)%\"\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) if match else None\n",
        "\n",
        "# Add more..."
      ],
      "metadata": {
        "id": "wkZMemnP9uwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_economic_data(text):\n",
        "    data = {\n",
        "        \"GDP Growth Rate\": extract_gdp_growth(text),\n",
        "        \"Unemployment Rate\": extract_unemployment_rate(text),\n",
        "        \"Inflation Rate\": extract_inflation_rate(text),\n",
        "        \"Interest Rate\": extract_interest_rate(text),\n",
        "        \"Budget Deficit\": extract_budget_deficit(text),\n",
        "        \"Trade Balance\": extract_trade_balance(text),\n",
        "        # Add more indicators...\n",
        "    }\n",
        "    return {k: v for k, v in data.items() if v is not None}"
      ],
      "metadata": {
        "id": "IadqWeYU9x3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "Qml4ygSAatuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = \"This year's GDP growth rate is 3.2%, the unemployment rate has dropped to 4.5%, the interest rate remains at 2%, and the trade balance is 1.5%.\"\n",
        "\n",
        "# Extract economic data\n",
        "economic_data = extract_all_economic_data(text_test)\n",
        "\n",
        "# Print results\n",
        "for key, value in economic_data.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "pPOQ8OFd94Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Model Construction"
      ],
      "metadata": {
        "id": "yATNuUoZnxx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the tokenizer and model with the 'bert-base-uncased' pre-trained model.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_Bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Assuming final_df_nlp is your DataFrame containing 'text' and 'label'\n",
        "# Convert labels to numerical values\n",
        "label_map = {'Bullish': 0, 'Bearish': 1, 'Neutral': 2}\n",
        "final_df_nlp['label'] = final_df_nlp['label'].map(label_map)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_df, test_df = train_test_split(final_df_nlp, test_size=0.2)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "wOHyK3Maaw9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for tokenization. It processes the text data for BERT.\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    result[\"labels\"] = list(map(int, examples[\"label\"]))\n",
        "    return result"
      ],
      "metadata": {
        "id": "mwe2S6FBaxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the tokenization function to the dataset in batches.\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Setting up training arguments. Specifies how the model should be trained.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory where the training results will be saved.\n",
        "    num_train_epochs=50,  # Number of epochs to train for.\n",
        "    per_device_train_batch_size=16,  # Batch size per device during training.\n",
        "    warmup_steps=500,  # Number of steps for the warmup phase.\n",
        "    weight_decay=0.01,  # Weight decay for regularization.\n",
        "    logging_dir='./logs',  # Directory for storing logs.\n",
        "    logging_steps=10,  # Log metrics every 10 steps.\n",
        ")"
      ],
      "metadata": {
        "id": "MJqQ4G3Ha2wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Trainer with the model, training arguments, and dataset.\n",
        "trainer = Trainer(\n",
        "    model=model_Bert,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset\n",
        ")\n",
        "\n",
        "# Start training the model.\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the test dataset.\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "I_d1yRLJa2y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to classify a single piece of text using the trained model.\n",
        "def classify_text_Bert(text, model_Bert, tokenizer):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_Bert.to(device)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model_Bert(**inputs)\n",
        "    predictions = outputs.logits.argmax(-1)\n",
        "    return predictions.item()"
      ],
      "metadata": {
        "id": "qQr1AiJKa21Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text for classification.\n",
        "text = \"The market is expected to grow significantly in the next quarter.\"\n",
        "\n",
        "# Classify the example text.\n",
        "prediction = classify_text_Bert(text, model_Bert, tokenizer)\n",
        "print(\"Classification result:\", prediction)"
      ],
      "metadata": {
        "id": "PYuO-3S7a23s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Search"
      ],
      "metadata": {
        "id": "0F8XuDR4vOhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameter ranges\n",
        "param_distributions = {\n",
        "    \"num_train_epochs\": [2, 3, 4],  # Number of training epochs\n",
        "    \"per_device_train_batch_size\": [8, 16, 32],  # Batch size per device\n",
        "    \"learning_rate\": uniform(1e-5, 5e-5),  # Learning rate\n",
        "    \"warmup_steps\": [0, 500, 1000],  # Number of warmup steps\n",
        "    \"weight_decay\": uniform(0.0, 0.1)  # Weight decay\n",
        "}"
      ],
      "metadata": {
        "id": "sGwaq_2svJH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_params(param_dist, num_samples):\n",
        "    sampled_params = []\n",
        "    for _ in range(num_samples):\n",
        "        params = {k: random.choice(v) if isinstance(v, list) else v.rvs() for k, v in param_dist.items()}\n",
        "        sampled_params.append(params)\n",
        "    return sampled_params"
      ],
      "metadata": {
        "id": "e0HY9Ic-vKZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample parameters\n",
        "sampled_params = sample_params(param_distributions, num_samples=10)  # Generate 10 random parameter sets"
      ],
      "metadata": {
        "id": "pCfxcMvnbcd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate for each parameter set\n",
        "for params in sampled_params:\n",
        "    # Update training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=params['num_train_epochs'],\n",
        "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        warmup_steps=params['warmup_steps'],\n",
        "        weight_decay=params['weight_decay'],\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model_Bert,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_test_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = trainer.evaluate()\n",
        "    print(f\"Params: {params}\")\n",
        "    print(f\"Results: {results}\\n\")"
      ],
      "metadata": {
        "id": "jhtHJ7c3bchF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = classify_text_Bert(text, model_Bert, tokenizer)\n",
        "print(\"Classification result:\", prediction)"
      ],
      "metadata": {
        "id": "5WiQEiwMZXuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network Model Construction *(CNN)*"
      ],
      "metadata": {
        "id": "y5ceIP8fbqeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, embedding_dim, max_length, filter_sizes, num_filters, num_classes):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input_layer)\n",
        "    pooled_outputs = []\n",
        "    for size in filter_sizes:\n",
        "        conv = Conv1D(filters=num_filters, kernel_size=size, activation='relu')(embedding)\n",
        "        pool = MaxPooling1D(pool_size=max_length - size + 1)(conv)\n",
        "        pooled_outputs.append(pool)\n",
        "    merged = Concatenate(axis=1)(pooled_outputs)\n",
        "    flatten = Flatten()(merged)\n",
        "    dense = Dense(10, activation='relu')(flatten)\n",
        "    output = Dense(num_classes, activation='softmax')(dense)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    optimizer = Adam(lr=1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qH-PCnD7l2Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_enhanced_cnn_model(vocab_size, embedding_dim, max_length, num_filters, filter_sizes, entity_embedding_dim, num_relations):\n",
        "    # Input and embedding layers\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "    # Convolution and pooling layers\n",
        "    conv_blocks = []\n",
        "    for size in filter_sizes:\n",
        "        conv = Conv1D(filters=num_filters, kernel_size=size, activation='relu')(text_embedding)\n",
        "        conv = MaxPooling1D(pool_size=2)(conv)\n",
        "        conv = Flatten()(conv)\n",
        "        conv_blocks.append(conv)\n",
        "\n",
        "    # Merge convolution layer outputs\n",
        "    merged = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "\n",
        "    # Dense, dropout, and batch normalization layers\n",
        "    merged = Dense(256, activation='relu')(merged)\n",
        "    merged = Dropout(0.3)(merged)\n",
        "    merged = BatchNormalization()(merged)\n",
        "\n",
        "    # Bidirectional LSTM layer\n",
        "    merged = Reshape((1, -1))(merged)\n",
        "    lstm = Bidirectional(LSTM(128, return_sequences=False))(merged)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(num_relations, activation='softmax')(lstm)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=text_input, outputs=output)\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "M0kJpmqimJ_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Compile the given model with Adam optimizer and categorical crossentropy loss.\n",
        "\n",
        "    :param model: The Keras model to compile.\n",
        "    :return: Compiled model.\n",
        "    \"\"\"\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "GnpzdRJbmPBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your hyperparameters and create the model\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "max_length = 200\n",
        "num_filters = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "entity_embedding_dim = 50\n",
        "num_relations = 10\n",
        "\n",
        "final_df_nlp['text'] = final_df_nlp['text'].apply(lambda x: x.lower())\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(final_df_nlp['text'])\n",
        "sequences = tokenizer.texts_to_sequences(final_df_nlp['text'])\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(final_df_nlp['label'])\n",
        "categorical_labels = to_categorical(encoded_labels, num_classes=3)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded, categorical_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model = build_enhanced_cnn_model(vocab_size, embedding_dim, max_length, num_filters, filter_sizes, entity_embedding_dim, len(label_encoder.classes_))\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "naJpyQkYmQeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Parameters / Random rersearch"
      ],
      "metadata": {
        "id": "2XoxSqK9K5zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    max_length = 200  # or another value of your choice\n",
        "    num_relations = 3  # number of classes, adjust according to your task\n",
        "\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    embedding_dim = hp.Choice('embedding_dim', values=[50, 100, 150])\n",
        "    text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "    conv_blocks = []\n",
        "    for size in [3, 4, 5]:\n",
        "        num_filters = hp.Int(f'num_filters_{size}', min_value=32, max_value=128, step=32)\n",
        "        conv = Conv1D(filters=num_filters, kernel_size=size, activation='relu')(text_embedding)\n",
        "        conv = MaxPooling1D(pool_size=2)(conv)\n",
        "        conv = Flatten()(conv)\n",
        "        conv_blocks.append(conv)\n",
        "\n",
        "    merged = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "    dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=64)\n",
        "    merged = Dense(dense_units, activation='relu')(merged)\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    merged = BatchNormalization()(merged)\n",
        "\n",
        "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=64)\n",
        "    merged = Reshape((1, -1))(merged)\n",
        "    lstm = Bidirectional(LSTM(lstm_units, return_sequences=False))(merged)\n",
        "\n",
        "    output = Dense(num_relations, activation='softmax')(lstm)\n",
        "    model = Model(inputs=text_input, outputs=output)\n",
        "\n",
        "    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a5Vm_6TwK5EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=1,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='nlp_tuning'\n",
        ")"
      ],
      "metadata": {
        "id": "yi9n9bKHcKfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model_CNN = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "print(\"Best hyperparameters:\", best_hps.values)\n",
        "model_CNN.summary()"
      ],
      "metadata": {
        "id": "VUiE8VCAcKh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify Text CNN Function Achieve"
      ],
      "metadata": {
        "id": "eSe5sCu_z7ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "wAj9bR4DcG3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text_CNN(text, model_CNN, bert_tokenizer, max_length):\n",
        "    inputs = bert_tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    predictions = model_CNN.predict([inputs['input_ids']])\n",
        "    prediction_class = np.argmax(predictions, axis=-1)\n",
        "    confidence = np.max(predictions, axis=-1)\n",
        "    return prediction_class[0], confidence[0]"
      ],
      "metadata": {
        "id": "MfSMIqM_adEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The market is expected to grow significantly in the next quarter.\"\n",
        "prediction_class, confidence = classify_text_CNN(text, model_CNN, bert_tokenizer, max_length)\n",
        "print(\"Classification result:\", prediction_class, \"with confidence\", confidence)"
      ],
      "metadata": {
        "id": "nItoaahicHjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify Text Bert Function Achieve"
      ],
      "metadata": {
        "id": "AMQduW_tcfqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text_Bert(text, model_Bert, tokenizer):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_Bert.to(device)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model_Bert(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    confidence, predictions = torch.max(probabilities, dim=-1)\n",
        "    return predictions.item(), confidence.item()"
      ],
      "metadata": {
        "id": "Iee0B8aMbYjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "prediction_class, confidence = classify_text_Bert(text, model_Bert, tokenizer)\n",
        "print(\"Classification result:\", prediction_class, \"with confidence\", confidence)"
      ],
      "metadata": {
        "id": "XdXFadeucVr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_final_classification(result_bert, result_cnn):\n",
        "    classification_bert, confidence_bert = result_bert\n",
        "    classification_cnn, confidence_cnn = result_cnn\n",
        "\n",
        "    if classification_bert == classification_cnn:\n",
        "        return classification_bert\n",
        "\n",
        "    return classification_bert if confidence_bert > confidence_cnn else classification_cnn"
      ],
      "metadata": {
        "id": "I7tls7gNYkro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_scenarios_and_factors(classification, economic_data):\n",
        "\n",
        "    adjustment_rules = {\n",
        "        'Bullish': {'GDP Growth Rate': (0.02, 'positive'), 'Unemployment Rate': (-0.01, 'negative')},\n",
        "        'Bearish': {'GDP Growth Rate': (-0.02, 'negative'), 'Unemployment Rate': (0.01, 'positive')},\n",
        "        'Neutral': {}\n",
        "    }\n",
        "\n",
        "    adjustments = adjustment_rules.get(classification, {})\n",
        "\n",
        "    for key, (adjustment, impact) in adjustments.items():\n",
        "        if key in economic_data:\n",
        "            value = economic_data[key]\n",
        "\n",
        "            if key == 'GDP Growth Rate':\n",
        "                if impact == 'positive':\n",
        "                    scenarios['Economic Boom']['Close'] += adjustment * value\n",
        "                    scenarios['Market Crash']['Close'] -= adjustment * value\n",
        "                elif impact == 'negative':\n",
        "                    scenarios['Economic Boom']['Close'] -= adjustment * value\n",
        "                    scenarios['Market Crash']['Close'] += adjustment * value\n",
        "\n",
        "    return scenarios, stress_factors"
      ],
      "metadata": {
        "id": "51JtzaXpXWLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "for report in df_reports['report']:\n",
        "    classification_bert, confidence_bert = classify_text_Bert(report, model_Bert, tokenizer)\n",
        "    classification_cnn, confidence_cnn = classify_text_CNN(report, model_CNN, tokenizer, max_length)\n",
        "    final_classification = determine_final_classification((classification_bert, confidence_bert), (classification_cnn, confidence_cnn))\n",
        "    economic_data = extract_all_economic_data(report)\n",
        "    adjusted_scenarios, adjusted_factors = adjust_scenarios_and_factors(final_classification, economic_data)\n",
        "\n",
        "print(adjusted_scenarios)\n",
        "print(adjusted_factors)"
      ],
      "metadata": {
        "id": "iqYO5q5vXjlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liquidity_threshold = 100\n",
        "scenario_results = conduct_scenario_analysis(dataframes, weights, scenarios)\n",
        "print(weights)\n",
        "print(scenario_results)\n",
        "# Call the function to plot the analysis\n",
        "monte_carlo_portfolio_analysis(dataframes, weights, sheet_names)"
      ],
      "metadata": {
        "id": "UTB6ZQicr0hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_portfolio_VaR(weights, dataframes, confidence_level=0.99, scale_factor=10):\n",
        "    portfolio_returns = pd.DataFrame()\n",
        "\n",
        "    for weight, df in zip(weights, dataframes):\n",
        "        returns = df['Close'].pct_change().dropna() * scale_factor\n",
        "        portfolio_returns = pd.concat([portfolio_returns, returns * weight], axis=1)\n",
        "\n",
        "    total_returns = portfolio_returns.sum(axis=1)\n",
        "    df, loc, scale = t.fit(total_returns)\n",
        "    simulated_returns = t.rvs(df, loc, scale, size=10000)\n",
        "\n",
        "    simulated_price_changes = np.exp(simulated_returns) - 1\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "P9e0OOqzDFG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPtimizing"
      ],
      "metadata": {
        "id": "984czvi5YjeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Minimize"
      ],
      "metadata": {
        "id": "c5b7EaGyl8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimize_VaR(dataframes, sheet_names, confidence_level=0.99, scale_factor=10):\n",
        "\n",
        "    num_assets = len(dataframes)\n",
        "    initial_weights = np.array([1.0 / num_assets] * num_assets)\n",
        "\n",
        "    # Constraints: Weights sum to 1, and each weight is between 0 and 1\n",
        "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "    bounds = tuple((0, 1) for asset in range(num_assets))\n",
        "\n",
        "    # Objective function: Minimize VaR\n",
        "    def objective(weights):\n",
        "        return calculate_portfolio_VaR(weights, dataframes, confidence_level, scale_factor)\n",
        "\n",
        "    # Optimization\n",
        "    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "    if result.success:\n",
        "        optimized_weights = result.x\n",
        "        optimized_VaR = calculate_portfolio_VaR(optimized_weights, dataframes, confidence_level, scale_factor)\n",
        "        print(f\"Optimized Weights: {optimized_weights}\")\n",
        "        print(f\"Optimized Portfolio VaR: {optimized_VaR}\")\n",
        "        return optimized_weights, optimized_VaR\n",
        "    else:\n",
        "        raise ValueError('Optimization failed')"
      ],
      "metadata": {
        "id": "-uxEXNt1AtwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_weights, optimized_VaR = minimize_VaR(dataframes, sheet_names)"
      ],
      "metadata": {
        "id": "MCBJTq4eAt0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genetic Algorithm"
      ],
      "metadata": {
        "id": "i211Bo_EquC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PARAMS = 6"
      ],
      "metadata": {
        "id": "HVBNw1KjR0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_individual():\n",
        "    number_of_assets = len(dataframes)\n",
        "    weights = [random.uniform(0, 1) for _ in range(number_of_assets)]\n",
        "    weights /= np.sum(weights)  # Normalize the weights to sum to 1\n",
        "\n",
        "    days = random.randint(200, 300)  # Example values\n",
        "    iterations = random.randint(800, 1200)\n",
        "    confidence_level = random.uniform(0.95, 0.99)\n",
        "\n",
        "    return [weights, days, iterations, confidence_level]"
      ],
      "metadata": {
        "id": "XO68yYUBUu-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness_function(individual):\n",
        "    weights, days, iterations, confidence_level = individual\n",
        "    VaR, CVaR, VaR_impact, additional_value = monte_carlo_portfolio_analysis(dataframes, weights, sheet_names, days, iterations, confidence_level)\n",
        "    return (-VaR,)\n",
        "\n",
        "# Set up GA parameters\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", fitness_function)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
      ],
      "metadata": {
        "id": "7o0Py5K5quLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_and_track():\n",
        "    population = toolbox.population(n=1)\n",
        "    ngen = 2\n",
        "    best_scores = []\n",
        "    best_individuals = []\n",
        "\n",
        "    for gen in range(ngen):\n",
        "        offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.2)\n",
        "        fits = toolbox.map(toolbox.evaluate, offspring)\n",
        "\n",
        "        for fit, ind in zip(fits, offspring):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        population = toolbox.select(offspring, k=len(population))\n",
        "        best_ind = tools.selBest(population, k=1)[0]\n",
        "        best_scores.append(best_ind.fitness.values[0])\n",
        "        best_individuals.append(best_ind)\n",
        "\n",
        "\n",
        "        print(f\"Generation {gen}: Best individual is {best_ind}, Best VaR is {-best_ind.fitness.values[0]}\")\n",
        "\n",
        "    overall_best_ind = tools.selBest(population, k=1)[0]\n",
        "    print(f\"Overall best individual is {overall_best_ind}, with VaR: {-overall_best_ind.fitness.values[0]}\")\n",
        "\n",
        "    return best_scores, best_individuals"
      ],
      "metadata": {
        "id": "W_6tiniRquOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_scores, best_individuals = optimize_and_track()"
      ],
      "metadata": {
        "id": "nNUd_ks_rX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitness scores are defined with negative VaR values, so a larger score on the graph (closer to zero or a positive number) actually means better fitness because it represents lower risk."
      ],
      "metadata": {
        "id": "eBHbR3ynOdoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(best_scores)\n",
        "plt.title(\"Optimization Progress\")\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Best Fitness Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0QLF8vAarjuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo ML"
      ],
      "metadata": {
        "id": "37WpY1LNw4ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "egO_1T1bxDZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "7K2jIJBpmCO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = combined_df.dropna()\n",
        "\n",
        "combined_df['Return'] = combined_df['Close'].pct_change()\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "combined_df['DayOfWeek'] = combined_df['Date'].dt.dayofweek\n",
        "combined_df['Month'] = combined_df['Date'].dt.month\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "combined_df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(combined_df[['Open', 'High', 'Low', 'Close', 'Volume']])"
      ],
      "metadata": {
        "id": "FwQA9jy8mCR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = combined_df[['Open', 'High', 'Low', 'Volume']]\n",
        "y = combined_df['Close']"
      ],
      "metadata": {
        "id": "VTdy7nZkmCbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5JtLxgKsoG6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=1000, max_features='sqrt', min_samples_leaf=4, random_state=42)"
      ],
      "metadata": {
        "id": "P0oNVCRcoKD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_simulations = 10000\n",
        "simulated_price_paths = np.zeros((num_simulations, len(X_test)))"
      ],
      "metadata": {
        "id": "LzofEu-LoMt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"Average Cross-Validation Score:\", np.mean(cross_val_scores))"
      ],
      "metadata": {
        "id": "dvgkEAXVjcir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-ZauvltmjiAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poisson Distribution"
      ],
      "metadata": {
        "id": "0OJmTjpkrOwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_simulations = 5000\n",
        "lambda_jump = 0.05\n",
        "jump_sd = 0.4"
      ],
      "metadata": {
        "id": "Fjs6uIsUjmZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_stock_prices(predictions, X_test, num_simulations, lambda_jump, jump_sd):\n",
        "    # Simulate volatility - GARCH\n",
        "    garch = arch_model(predictions, vol='Garch', p=1, q=1)\n",
        "    garch_fitted = garch.fit()\n",
        "\n",
        "    simulation_length = len(X_test)\n",
        "    simulated_price_paths = np.zeros((num_simulations, simulation_length))\n",
        "\n",
        "    # Initial stock prices\n",
        "    initial_prices = X_test['Open'].values\n",
        "\n",
        "    # Monte Carlo simulation\n",
        "    for i in range(num_simulations):\n",
        "        simulated_prices = [initial_prices[0]]\n",
        "        for t in range(1, simulation_length):\n",
        "            # Fetch volatility\n",
        "            vol = garch_fitted.conditional_volatility[t]\n",
        "            # Random shock\n",
        "            shock = np.random.normal(0, vol)\n",
        "            # Jump part\n",
        "            jump = np.random.normal(0, jump_sd) if np.random.random() < lambda_jump else 0\n",
        "            # Simulate price\n",
        "            simulated_price = simulated_prices[t-1] * (1 + predictions[t] + shock + jump)\n",
        "            simulated_prices.append(simulated_price)\n",
        "        simulated_price_paths[i, :] = simulated_prices\n",
        "\n",
        "    return simulated_price_paths"
      ],
      "metadata": {
        "id": "9hErmDT6ruxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_simulation_results(simulated_price_paths, VaR):\n",
        "    print(VaR)\n",
        "    # Visualize simulated stock price paths\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(len(simulated_price_paths)):  # Plot only 100 paths for clarity\n",
        "        plt.plot(simulated_price_paths[i, :], alpha=0.2)\n",
        "    plt.title(\"Simulated Stock Price Paths\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize VaR distribution\n",
        "    initial_prices = simulated_price_paths[:, 0]\n",
        "    end_returns = (simulated_price_paths[:, -1] - initial_prices) / initial_prices\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(end_returns, bins=25, alpha=0.7, color='blue')\n",
        "    plt.axvline(-VaR, color='red', linestyle='dashed', linewidth=2)\n",
        "    plt.title(\"Distribution of Returns and VaR\")\n",
        "    plt.xlabel(\"Return\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kRgzqg_vru8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_randomForest(simulated_price_paths, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk (VaR) from simulated stock price paths.\n",
        "    \"\"\"\n",
        "    initial_prices = simulated_price_paths[:, 0]\n",
        "    end_prices = simulated_price_paths[:, -1]\n",
        "    returns = (end_prices - initial_prices) / initial_prices\n",
        "    sorted_returns = np.sort(returns)\n",
        "    var_index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    VaR = -sorted_returns[var_index]\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "-LZ9Tv7vwS1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulated_price_paths = simulate_stock_prices(predictions, X_test, num_simulations, lambda_jump, jump_sd)\n",
        "VaR = calculate_VaR_randomForest(simulated_price_paths)\n",
        "visualize_simulation_results(simulated_price_paths, VaR)"
      ],
      "metadata": {
        "id": "6CacBsXAo4Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Parameters"
      ],
      "metadata": {
        "id": "Ytz34HSGQhEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes Research / Random Forest"
      ],
      "metadata": {
        "id": "LC5rSwFFko6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "6YticTOekJBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_space = {\n",
        "    'n_estimators': (100, 1000),\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}"
      ],
      "metadata": {
        "id": "i2zqCTwG7Yf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_search = BayesSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=32,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3-SBDlKrkLGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DXKTyDs_kPLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters found: \", bayes_search.best_params_)"
      ],
      "metadata": {
        "id": "p8BGT-fgkRmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = bayes_search.best_estimator_\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "print(\"Test set score of best model: \", test_score)"
      ],
      "metadata": {
        "id": "UCWinGxbkUiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Composite Conmtruction"
      ],
      "metadata": {
        "id": "ThMpeUzCww1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copula"
      ],
      "metadata": {
        "id": "LdKoZ9VxcsEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genenerate diifferent Asset class data\n"
      ],
      "metadata": {
        "id": "yThTLKubkiHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_fund_data(days, mean_return=0, std_dev=0.01):\n",
        "\n",
        "    dates = pd.date_range(start=\"2022-01-01\", periods=days)\n",
        "    returns = np.random.normal(mean_return, std_dev, days)\n",
        "    return pd.DataFrame({'Date': dates, 'FUND_Close': returns.cumsum()})"
      ],
      "metadata": {
        "id": "GoFQnf6zkhFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_Copula_and_Fund(dataframes, fund_data, weights, days=252, iterations=10000, confidence_level=0.99):\n",
        "\n",
        "    returns = pd.DataFrame()\n",
        "    for i, df in enumerate(dataframes):\n",
        "        stock_name = sheet_names[i]\n",
        "        returns[stock_name] = df['Close'].pct_change().dropna()\n",
        "    returns['FUND'] = fund_data['FUND_Close'].pct_change().dropna()\n",
        "\n",
        "\n",
        "    copula = GaussianMultivariate()\n",
        "    copula.fit(returns)\n",
        "\n",
        "\n",
        "    simulated_returns = copula.sample(iterations)\n",
        "\n",
        "\n",
        "\n",
        "    if simulated_returns.shape[1] != len(weights):\n",
        "        raise ValueError(\"Mismatch in number of assets and weights.\")\n",
        "\n",
        "    portfolio_returns = simulated_returns.dot(weights)\n",
        "\n",
        "\n",
        "    VaR = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(portfolio_returns, bins=50, kde=True)\n",
        "    plt.axvline(x=VaR, color='r', linestyle='--', label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title('Simulated Portfolio Returns Distribution with VaR')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "oUH2B_s7cr8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fund_data = simulate_fund_data(252)\n",
        "\n",
        "sheet_names = [\"META\", \"MSFT\", \"NFLX\"]\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "\n",
        "weights = np.array([0.2, 0.2, 0.2, 0.4])\n",
        "\n",
        "VaR = calculate_VaR_with_Copula_and_Fund(dataframes, fund_data, weights)"
      ],
      "metadata": {
        "id": "v45gYwdVktTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Theory"
      ],
      "metadata": {
        "id": "qyFnyYt4MVTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_asset_price(S0, days, mu, sigma):\n",
        "    dt = 1 / 252  # One trading day\n",
        "    random_shocks = np.random.normal(0, 1, days)\n",
        "    price = S0 * np.cumprod(np.exp((mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * random_shocks))\n",
        "    return price"
      ],
      "metadata": {
        "id": "08OhGX1CMVcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_option_prices(S0, K, T, r, sigma, num_days, num_simulations):\n",
        "    \"\"\"\n",
        "    Simulate option prices using GBM for the underlying asset and the Black-Scholes model.\n",
        "    S0: Initial stock price\n",
        "    K: Strike price\n",
        "    T: Time to maturity\n",
        "    r: Risk-free interest rate\n",
        "    sigma: Volatility of the underlying asset\n",
        "    num_days: Number of days to simulate\n",
        "    num_simulations: Number of simulated paths\n",
        "    \"\"\"\n",
        "    dt = T / num_days\n",
        "    option_price_paths = []\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        # Simulate price path for the underlying asset\n",
        "        price_path = simulate_gbm(S0, r, sigma, T, num_days)\n",
        "\n",
        "        # Calculate option price for each day\n",
        "        option_prices = [black_scholes(S, K, T - i*dt, r, sigma) for i, S in enumerate(price_path)]\n",
        "        option_price_paths.append(option_prices)\n",
        "\n",
        "    return np.array(option_price_paths)"
      ],
      "metadata": {
        "id": "pg1tlpF1dGJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def black_scholes(S, K, T, r, sigma, option_type=\"call\"):\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    if option_type == \"call\":\n",
        "        option_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "    else:\n",
        "        option_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
        "    return option_price"
      ],
      "metadata": {
        "id": "WVOwaDNuMVkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Asset Price Simulation (e.g., Geometric Brownian Motion)"
      ],
      "metadata": {
        "id": "Rr8rpSk4MiGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_gbm(S0, mu, sigma, T, steps):\n",
        "    \"\"\"\n",
        "    Simulate asset prices using Geometric Brownian Motion.\n",
        "    S0: Initial asset price\n",
        "    mu: Drift coefficient\n",
        "    sigma: Volatility coefficient\n",
        "    T: Time horizon\n",
        "    steps: Number of time steps\n",
        "    \"\"\"\n",
        "    dt = T / steps\n",
        "    random_component = np.random.normal(0, np.sqrt(dt), steps)\n",
        "    price_path = np.exp((mu - 0.5 * sigma**2) * dt + sigma * random_component)\n",
        "    return S0 * np.cumprod(price_path)"
      ],
      "metadata": {
        "id": "HcBPheehMVmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_gbm_european_call(S0, K, T, r, sigma, num_simulations, steps):\n",
        "    \"\"\"\n",
        "    Monte Carlo simulation for European call option using GBM.\n",
        "    S0: Initial stock price\n",
        "    K: Strike price\n",
        "    T: Time to maturity\n",
        "    r: Risk-free interest rate\n",
        "    sigma: Volatility\n",
        "    num_simulations: Number of simulated paths\n",
        "    steps: Number of time steps\n",
        "    \"\"\"\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "    option_price_paths = []\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        asset_prices = simulate_gbm(S0, r, sigma, T, steps)\n",
        "        option_prices = np.maximum(asset_prices - K, 0)  # Calculate option prices at each step\n",
        "        discounted_option_prices = np.exp(-r * np.arange(1, steps + 1) / 252) * option_prices\n",
        "        option_price_paths.append(discounted_option_prices)\n",
        "\n",
        "    # Convert to numpy array for easier manipulation\n",
        "    option_price_paths = np.array(option_price_paths)\n",
        "\n",
        "    # Calculate the average option price for each day across all simulations\n",
        "    mean_option_prices = np.mean(option_price_paths, axis=0)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Plot simulated option price paths\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for option_path in option_price_paths:\n",
        "        plt.plot(option_path, color='blue', alpha=0.1)\n",
        "    plt.title('Simulated Option Price Paths')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Option Price')\n",
        "\n",
        "    # Plot average option price\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(mean_option_prices, color='green')\n",
        "    plt.title('Average Option Price Over Time')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Average Option Price')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mean_option_prices"
      ],
      "metadata": {
        "id": "5oVCpMCUMqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Cost consideration"
      ],
      "metadata": {
        "id": "1Rm8BMysOBEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the simulation\n",
        "S0 = 100\n",
        "K = 110\n",
        "T = 1\n",
        "r = 0.05\n",
        "sigma = 0.2\n",
        "num_simulations = 10000\n",
        "steps = 252\n",
        "\n",
        "# Run simulation and plot\n",
        "option_price_series = monte_carlo_gbm_european_call(S0, K, T, r, sigma, num_simulations, steps)\n",
        "\n",
        "# Print the entire series of average option prices\n",
        "print(\"Simulated European Call Option Price Series:\")\n",
        "print(option_price_series)\n",
        "\n",
        "# Print the final average option price\n",
        "final_option_price = option_price_series[-1]\n",
        "print(f\"Final Simulated European Call Option Price: {final_option_price:.2f}\")"
      ],
      "metadata": {
        "id": "pIUcqLfpMr5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copula"
      ],
      "metadata": {
        "id": "cuwlSYqhOFMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_option_returns(days, S0, K, T, r, sigma, option_type=\"call\"):\n",
        "    # Simulate underlying asset prices\n",
        "    asset_prices = simulate_asset_price(S0, days, r, sigma)\n",
        "\n",
        "    # Calculate option prices for each day\n",
        "    option_prices = [black_scholes(S, K, T - i/252, r, sigma, option_type) for i, S in enumerate(asset_prices)]\n",
        "\n",
        "    # Convert to DataFrame and calculate returns\n",
        "    option_prices_df = pd.DataFrame({'OPTION_Close': option_prices})\n",
        "    option_returns = option_prices_df['OPTION_Close'].pct_change().dropna()\n",
        "    return option_returns"
      ],
      "metadata": {
        "id": "NGEYbmvNQBlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, weights, days=252, iterations=10000, confidence_level=0.99, include_fund=True, include_option=True):\n",
        "    returns = pd.DataFrame()\n",
        "    for i, df in enumerate(dataframes):\n",
        "        stock_name = sheet_names[i]\n",
        "        returns[stock_name] = df['Close'].pct_change().dropna()\n",
        "\n",
        "    if include_fund:\n",
        "        returns['FUND'] = fund_data['FUND_Close'].pct_change().dropna()\n",
        "\n",
        "    if include_option:\n",
        "        # Assuming option_data is already a series of returns\n",
        "        returns['OPTION'] = option_data\n",
        "\n",
        "    # Fit the Copula model\n",
        "    copula = GaussianMultivariate()\n",
        "    copula.fit(returns)\n",
        "\n",
        "    simulated_returns = copula.sample(iterations)\n",
        "\n",
        "    if simulated_returns.shape[1] != len(weights):\n",
        "        raise ValueError(\"Mismatch in number of assets and weights.\")\n",
        "\n",
        "    portfolio_returns = simulated_returns.dot(weights)\n",
        "\n",
        "    VaR = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(portfolio_returns, bins=50, kde=True)\n",
        "    plt.axvline(x=VaR, color='r', linestyle='--', label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title('Simulated Portfolio Returns Distribution with VaR')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "oAxEjBorPFJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_stocks = len(sheet_names)\n",
        "include_fund = True\n",
        "include_option = True\n",
        "\n",
        "num_assets = num_stocks + (1 if include_fund else 0) + (1 if include_option else 0)\n",
        "\n",
        "weights = [1/num_assets for _ in range(num_assets)]\n",
        "\n",
        "option_data = simulate_option_returns(days=252, S0=100, K=110, T=1, r=0.05, sigma=0.2, option_type=\"call\")\n",
        "VaR = calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, weights, include_fund=include_fund, include_option=include_option)"
      ],
      "metadata": {
        "id": "_jzRvehYwQeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annealing"
      ],
      "metadata": {
        "id": "yQHo0vLzEMgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def portfolio_variance(weights, cov_matrix):\n",
        "\n",
        "    return np.dot(weights.T, np.dot(cov_matrix, weights))"
      ],
      "metadata": {
        "id": "gTuzgYxnj5ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulated_annealing_monte_carlo(cov_matrix, initial_weights, steps=1000, temp=1.0, cooling_rate=0.95):\n",
        "\n",
        "    current_solution = np.array(initial_weights)\n",
        "    best_solution = np.array(initial_weights)\n",
        "    best_variance = portfolio_variance(current_solution, cov_matrix)\n",
        "\n",
        "    variances = [best_variance]  # Store variances for visualization\n",
        "\n",
        "    for step in range(steps):\n",
        "        new_solution = np.random.normal(current_solution, 0.1)\n",
        "        new_solution = new_solution / np.sum(new_solution)\n",
        "\n",
        "        current_variance = portfolio_variance(current_solution, cov_matrix)\n",
        "        new_variance = portfolio_variance(new_solution, cov_matrix)\n",
        "\n",
        "        if new_variance < best_variance:\n",
        "            best_solution, best_variance = new_solution, new_variance\n",
        "        elif np.random.random() < np.exp(-(new_variance - current_variance) / temp):\n",
        "            current_solution = new_solution\n",
        "\n",
        "        temp *= cooling_rate\n",
        "        variances.append(best_variance)\n",
        "\n",
        "    # Visualization\n",
        "    plt.plot(variances)\n",
        "    plt.title('Portfolio Variance over Simulated Annealing Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.show()\n",
        "\n",
        "    return best_solution"
      ],
      "metadata": {
        "id": "c8cc49ASEMn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example covariance matrix (5 assets)\n",
        "cov_matrix = np.array([\n",
        "    [0.005, -0.002, 0.004, 0.001, 0.002],\n",
        "    [-0.002, 0.004, -0.001, 0.002, 0.003],\n",
        "    [0.004, -0.001, 0.01, 0.003, 0.002],\n",
        "    [0.001, 0.002, 0.003, 0.006, 0.001],\n",
        "    [0.002, 0.003, 0.002, 0.001, 0.007]\n",
        "])\n",
        "\n",
        "num_assets = len(cov_matrix)\n",
        "\n",
        "# Initial weights (equal distribution for 5 assets)\n",
        "initial_weights = [1/num_assets] * num_assets\n",
        "\n",
        "# Run simulated annealing to optimize weights\n",
        "optimized_weights = simulated_annealing_monte_carlo(cov_matrix, initial_weights)\n",
        "\n",
        "# Assuming you have the following data ready\n",
        "fund_data = simulate_fund_data(252)  # 1 year of trading days\n",
        "option_data = simulate_option_returns(252, S0, K, T, r, sigma)\n",
        "\n",
        "# Now calculate VaR with optimized weights\n",
        "VaR = calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, optimized_weights, include_fund=True, include_option=True)\n",
        "print(\"Calculated VaR:\", VaR)"
      ],
      "metadata": {
        "id": "6QeE0olcjMvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}