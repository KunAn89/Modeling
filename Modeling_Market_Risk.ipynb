{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "mount_file_id": "12qNFYhKl5MuoDqGu9iyzRDUTbv5rqAIY",
      "authorship_tag": "ABX9TyMJ/1JRmnEeWK33PTufJQmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunAn89/Modeling_Risk/blob/main/Modeling_Market_Risk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VaR\n",
        "Value at Risk"
      ],
      "metadata": {
        "id": "Eo-bQwKPBsaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/12qNFYhKl5MuoDqGu9iyzRDUTbv5rqAIY?usp=sharing"
      ],
      "metadata": {
        "id": "Dt_cC5t7YDSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Running Information"
      ],
      "metadata": {
        "id": "UfbdIPYv7wSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package"
      ],
      "metadata": {
        "id": "ozKwot_r73co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch\n",
        "!pip install deap\n",
        "!pip install scikit-optimize\n",
        "!pip install copulas"
      ],
      "metadata": {
        "id": "g4dsrUkDEvkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDPU6dXtBa_-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gspread\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import t\n",
        "from scipy.stats import norm\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "from copulas.multivariate import GaussianMultivariate\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "from arch import arch_model\n",
        "\n",
        "from google.colab import auth\n",
        "from google.auth import default"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Reading"
      ],
      "metadata": {
        "id": "KeUX1C1mi5VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup for Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "pdlr0eZVvfRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sheet_data(spreadsheet_name, worksheet_index=0):\n",
        "    spreadsheet = gc.open(spreadsheet_name)\n",
        "    worksheet = spreadsheet.get_worksheet(worksheet_index)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df.drop(0)\n",
        "    df['Date'] = df['Date'].str.split().str[0]\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y').dt.date\n",
        "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "    return df"
      ],
      "metadata": {
        "id": "S4wvZGopigMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names = [\"META\", \"MSFT\", \"NFLX\"]\n",
        "\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "for df in dataframes:\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "k4nrTiRNvYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Data"
      ],
      "metadata": {
        "id": "SZvZo9w6zM9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names_test = [\"Test Data\", \"Test Data1\", \"Test Data2\"]\n",
        "\n",
        "dataframes_test = [get_sheet_data(name) for name in sheet_names_test]\n",
        "\n",
        "combined_df = pd.concat(dataframes_test, ignore_index=True)\n",
        "\n",
        "for df in dataframes_test:\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "rA54nqtMzKJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visulization"
      ],
      "metadata": {
        "id": "nEFMbIP-zSsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes_forSeen = {name: get_sheet_data(name) for name in sheet_names}\n",
        "plt.figure(figsize=(12, 6))\n",
        "for name, df in dataframes_forSeen.items():\n",
        "    plt.plot(df['Date'], df['Close'], label=name)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Stock Closing Prices')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jNc2A449zki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Reinforcement"
      ],
      "metadata": {
        "id": "CX2TyWWsOVHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstraping"
      ],
      "metadata": {
        "id": "DcpUmZoHvX4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_data(df, n_bootstraps=1):\n",
        "    bootstrap_samples = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        sample = df.sample(n=len(df), replace=True)  # Allow Repeat\n",
        "        bootstrap_samples.append(sample)\n",
        "    return bootstrap_samples"
      ],
      "metadata": {
        "id": "zjLbKfk5OVAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'dataframes' is your list of original dataframes\n",
        "bootstraped_dataframes = [bootstrap_data(df) for df in dataframes]\n",
        "\n",
        "# Iterating through each bootstrap sample for each original dataframe\n",
        "for i, bootstrapped_dfs in enumerate(bootstraped_dataframes):\n",
        "    print(f\"Original DataFrame {i}:\")\n",
        "    for j, df_sample in enumerate(bootstrapped_dfs):\n",
        "        print(f\"Bootstrap Sample {j + 1} Head:\")\n",
        "        print(df_sample.head())\n",
        "        print()"
      ],
      "metadata": {
        "id": "gFvs0h1Ouh-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "3WBntsZpvZCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_lstm(df, feature_columns, target_column, n_steps):\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    # Create sequences\n",
        "    for i in range(n_steps, len(df)):\n",
        "        X.append(scaled_data[i-n_steps:i, :])\n",
        "        y.append(scaled_data[i, df.columns.get_loc(target_column)])\n",
        "\n",
        "    # Convert to arrays and reshape for LSTM\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # LSTM expects input shape of [samples, time steps, features]\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], len(feature_columns)))\n",
        "\n",
        "    return X, y, scaler"
      ],
      "metadata": {
        "id": "I5r8B8ISxamu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 5  # Use data from the past 5 time points to predict the next time point"
      ],
      "metadata": {
        "id": "afvX_tVRxfG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
        "        LSTM(50),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "3sV6QrIhu-_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_models = []\n",
        "predictions = []\n",
        "\n",
        "for df in dataframes:\n",
        "    X, y, scaler = prepare_data_for_lstm(df=dataframes[0], feature_columns=['Open', 'High', 'Low', 'Close', 'Volume'], target_column='Close', n_steps=n_steps)\n",
        "    model = build_lstm_model(X.shape[1:])\n",
        "    model.fit(X, y, epochs=10, batch_size=32)\n",
        "    lstm_models.append(model)\n",
        "    predictions.append(model.predict(X))"
      ],
      "metadata": {
        "id": "TNBQe-sWu_CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GANS"
      ],
      "metadata": {
        "id": "WWw9PAiNvZ55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(seq_length, latent_dim, n_features=5):\n",
        "    input_noise = Input(shape=(latent_dim,))\n",
        "    x = Dense(128)(input_noise)\n",
        "    x = LeakyReLU(alpha=0.01)(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = Dense(seq_length * n_features)(x)  # Adjust the number of units\n",
        "    x = Reshape((seq_length, n_features))(x)  # Reshape to match the discriminator's input\n",
        "    return Model(input_noise, x)"
      ],
      "metadata": {
        "id": "R7KRmtEivgFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    z = Input(shape=(latent_dim,))\n",
        "    fake_seq = generator(z)\n",
        "    discriminator.trainable = False\n",
        "    validity = discriminator(fake_seq)\n",
        "    return Model(z, validity)"
      ],
      "metadata": {
        "id": "pzLx-C5ivoYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(seq_length, n_features=5):\n",
        "    seq = Input(shape=(seq_length, n_features))\n",
        "    x = LSTM(64, return_sequences=True)(seq)\n",
        "    x = LSTM(64)(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(seq, x)"
      ],
      "metadata": {
        "id": "8PpmFd1pe45j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and create sequences\n",
        "def preprocess_and_create_sequences(df, selected_columns, seq_length):\n",
        "    data = df[selected_columns]\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Create sequences\n",
        "    def create_sequences(data, seq_length):\n",
        "        xs, ys = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            x = data[i:(i + seq_length)]\n",
        "            y = data[i + seq_length]\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)\n",
        "\n",
        "    X, y = create_sequences(scaled_data, seq_length)\n",
        "    return X, y, scaler\n",
        "\n",
        "# Preprocess each DataFrame and create sequences\n",
        "latent_dim = 32\n",
        "seq_length = 60\n",
        "processed_data = [preprocess_and_create_sequences(df, ['Open', 'High', 'Low', 'Close', 'Volume'], seq_length) for df in dataframes]"
      ],
      "metadata": {
        "id": "8LP91xbceZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator(seq_length, latent_dim, n_features=5)  # Make sure to match n_features\n",
        "discriminator = build_discriminator(seq_length, n_features=5)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
      ],
      "metadata": {
        "id": "OwMgqzXRvt3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, processed_data, epochs, batch_size, latent_dim):\n",
        "    for data_tuple in processed_data:\n",
        "        X, y, _ = data_tuple  # Assuming each tuple is (X, y, scaler)\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Randomly select real sequences\n",
        "            idx = np.random.randint(0, X.shape[0], batch_size)\n",
        "            real_seqs = X[idx]\n",
        "\n",
        "            # Generate fake sequences\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            fake_seqs = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator\n",
        "            d_loss_real = discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")"
      ],
      "metadata": {
        "id": "sN-o478pwqsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gan(generator, discriminator, gan, processed_data, epochs=3, batch_size=32, latent_dim=latent_dim)"
      ],
      "metadata": {
        "id": "hkwGO7WIwu0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stat Method"
      ],
      "metadata": {
        "id": "lilVXlmWwuL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_var(df, confidence_level=95):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Calculate VaR at the specified confidence level\n",
        "    var = np.percentile(df['Return'].dropna(), 100 - confidence_level)\n",
        "    return var"
      ],
      "metadata": {
        "id": "k6MuqYg-uy23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_var(df)\n",
        "    print(f\"VaR at 95% confidence level for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "xh-BqNGnwoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historical Historical Maximum Loss"
      ],
      "metadata": {
        "id": "ssOPLGq6xyzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate VaR by finding the greatest loss\n",
        "def calculate_VaR_simple_loss(df):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Find the greatest loss (minimum return)\n",
        "    greatest_loss = df['Return'].min()\n",
        "    return greatest_loss"
      ],
      "metadata": {
        "id": "XyTAytO9x7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_simple_loss(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "b6PsufsrylFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Decay Factor EWMA"
      ],
      "metadata": {
        "id": "qymWmLg4fBgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_time_decay(df, decay_factor=1):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Apply exponential weighting\n",
        "    weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "    weighted_returns = df['Return'] * weights\n",
        "    weighted_returns /= weights.sum()\n",
        "\n",
        "    # Find the greatest loss (minimum return) in weighted returns\n",
        "    greatest_loss = weighted_returns.min()\n",
        "    return greatest_loss"
      ],
      "metadata": {
        "id": "bxnQ9H2ldUal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_with_time_decay(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "58DkR0-qdU8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_time_decay_amend(df, decay_factor=1):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Handle NaN values in returns\n",
        "    df.dropna(subset=['Return'], inplace=True)\n",
        "\n",
        "    # When decay_factor is 1, use unweighted returns directly\n",
        "    if decay_factor == 1:\n",
        "        greatest_loss = df['Return'].min()\n",
        "    else:\n",
        "        # Apply exponential weighting\n",
        "        weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "        weighted_returns = df['Return'] * weights\n",
        "        weighted_returns /= weights.sum()\n",
        "\n",
        "        # Find the greatest loss in weighted returns\n",
        "        greatest_loss = weighted_returns.min()\n",
        "\n",
        "    return greatest_loss\n"
      ],
      "metadata": {
        "id": "YsviZ6Nh9N1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VaR for each stock by identifying the greatest loss\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    var = calculate_VaR_with_time_decay_amend(df)\n",
        "    print(f\"VaR (greatest loss) for {name}: {var*100:.2f}%\")"
      ],
      "metadata": {
        "id": "yIHgYXaH9OD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CVaR  Expected Shortfall (ES) Tail VaR"
      ],
      "metadata": {
        "id": "3RbWiNqfaO_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_CVaR_with_time_decay(df, decay_factor=1, confidence_level=0.95):\n",
        "    # Convert columns to numeric (excluding Date)\n",
        "    for col in df.columns:\n",
        "        if col != 'Date':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "\n",
        "    # Apply exponential weighting\n",
        "    weights = np.array([decay_factor**i for i in range(len(df))])[::-1]\n",
        "    weighted_returns = df['Return'] * weights\n",
        "    weighted_returns /= weights.sum()\n",
        "\n",
        "    # Find the VaR (Value at Risk)\n",
        "    VaR_threshold = np.percentile(weighted_returns.dropna(), (1 - confidence_level) * 100)\n",
        "\n",
        "    # Calculate CVaR (Conditional Value at Risk)\n",
        "    # Only consider the returns that are less than the VaR threshold\n",
        "    tail_losses = weighted_returns[weighted_returns < VaR_threshold]\n",
        "    CVaR = tail_losses.mean()  # Conditional VaR is the mean of the losses beyond the VaR threshold\n",
        "\n",
        "    return CVaR"
      ],
      "metadata": {
        "id": "JVPeji_daPX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating CVaR for each stock by identifying the conditional mean loss beyond the VaR threshold\n",
        "for name, df in zip(sheet_names, dataframes):\n",
        "    cvar = calculate_CVaR_with_time_decay(df)\n",
        "    print(f\"CVaR (conditional mean loss) for {name}: {cvar*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Wi81N0NjCtLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo"
      ],
      "metadata": {
        "id": "055X-n-oxwjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual Stock"
      ],
      "metadata": {
        "id": "Tbjx_CqjikZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_stock_returns(initial_value, final_value, days=365):\n",
        "    \"\"\"\n",
        "    Calculate the annual and daily returns of a stock.\n",
        "\n",
        "    :param initial_value: The initial value of the stock.\n",
        "    :param final_value: The final value of the stock after a period.\n",
        "    :param days: The number of days over which the final value is measured. Default is 365 for one year.\n",
        "    :return: A tuple containing the annual return and daily return as percentages.\n",
        "    \"\"\"\n",
        "    # Calculate annual return\n",
        "    annual_return = ((final_value - initial_value) / initial_value) * 100\n",
        "\n",
        "    # Calculate daily return based on the number of days\n",
        "    daily_return = ((final_value / initial_value) ** (1/days) - 1) * 100\n",
        "\n",
        "    return annual_return, daily_return"
      ],
      "metadata": {
        "id": "3VftVtbotgXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate return"
      ],
      "metadata": {
        "id": "wx0ar6U4izqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "\n",
        "        initial_value = df['Close'].iloc[0]\n",
        "        final_value = df['Close'].iloc[-1]\n",
        "\n",
        "        days = (df['Date'].iloc[-1] - df['Date'].iloc[0]).days\n",
        "\n",
        "        annual_return, daily_return = calculate_stock_returns(initial_value, final_value, days)\n",
        "\n",
        "        print(name, f\"Annual Return: {annual_return:.2f}%, Daily Return: {daily_return:.4f}%\")"
      ],
      "metadata": {
        "id": "X_jpAbRm3PMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal Distribution"
      ],
      "metadata": {
        "id": "7HGP_OtqIIe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_MonteCarlo_Normal(df, days=252, iterations=10000, confidence_level=0.95, plot=False, plot_paths=False):\n",
        "    # Calculate daily returns\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    # Fit a GARCH model to estimate volatility\n",
        "    garch = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    model = garch.fit(disp='off')\n",
        "    forecast = model.forecast(horizon=days)\n",
        "    vol = np.sqrt(forecast.variance.iloc[-1])\n",
        "\n",
        "    # Calculate mean return\n",
        "    mean_return = returns.mean()\n",
        "    # Simulate returns using the normal distribution\n",
        "    simulated_returns = np.random.normal(mean_return, vol, (iterations, days))\n",
        "    # Calculate simulated price changes\n",
        "    simulated_price_changes = np.exp(simulated_returns) - 1\n",
        "\n",
        "    # Calculate VaR\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    if plot:\n",
        "      # Flatten the array to make it one-dimensional\n",
        "      flattened_simulated_changes = simulated_price_changes.flatten()\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.hist(flattened_simulated_changes, bins=50, alpha=0.7, color='red')\n",
        "      plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR*100:.2f}%\")\n",
        "      plt.title(f\"Simulated Price Changes Distribution\\nVaR (at {confidence_level*100}%): {VaR*100:.2f}%\")\n",
        "      plt.xlabel('Simulated Price Changes')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "    if plot_paths:\n",
        "        vol = np.sqrt(forecast.variance.dropna().mean(axis=0))\n",
        "        simulated_paths = np.zeros((iterations, days))\n",
        "\n",
        "        for i in range(iterations):\n",
        "            daily_returns = np.random.normal(mean_return, vol, days)\n",
        "            simulated_paths[i, :] = np.cumprod(1 + daily_returns) * df['Close'].iloc[-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i in range(iterations):\n",
        "            plt.plot(simulated_paths[i], alpha=0.2)\n",
        "\n",
        "        plt.title(f\"Monte Carlo Simulation Paths ({iterations} iterations)\")\n",
        "        plt.xlabel('Days')\n",
        "        plt.ylabel('Simulated Price')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "T8YdcDivGw-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T Distrubtion"
      ],
      "metadata": {
        "id": "L_CtSlKkIH5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_MonteCarlo_Advanced(df, days=252, iterations=10000, confidence_level=0.99, scale_factor=10, plot=False, plot_paths=False):\n",
        "    # Rescaling returns\n",
        "    returns = df['Close'].pct_change().dropna() * scale_factor\n",
        "\n",
        "    # GARCH model\n",
        "    garch = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    model = garch.fit(update_freq=10, disp='off')\n",
        "    forecast = model.forecast(horizon=days)\n",
        "    vol = np.sqrt(forecast.variance.iloc[-1].iloc[-1]) / scale_factor\n",
        "\n",
        "    # Fitting t-distribution\n",
        "    deg_freedom, loc, scale = t.fit(returns)\n",
        "    simulated_returns = t.rvs(deg_freedom, loc, scale, size=(iterations, days))\n",
        "\n",
        "    # Simulated price changes\n",
        "    simulated_price_changes = np.exp(simulated_returns * vol) - 1\n",
        "\n",
        "    # VaR calculation\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    if plot:\n",
        "        flattened_simulated_changes = simulated_price_changes.flatten()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(flattened_simulated_changes, bins=50, alpha=0.7, color='blue')\n",
        "        plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR*100:.2f}%\")\n",
        "        plt.title(f\"Simulated Price Changes Distribution\\nVaR (at {confidence_level*100}%): {VaR*100:.2f}%\")\n",
        "        plt.xlabel('Simulated Price Changes')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if plot_paths:\n",
        "        vol = np.sqrt(forecast.variance.iloc[-1].iloc[-1]) / scale_factor\n",
        "        simulated_paths = np.zeros((iterations, days))\n",
        "\n",
        "        for i in range(iterations):\n",
        "            daily_returns = t.rvs(deg_freedom, loc, scale, size=days) * vol\n",
        "            simulated_paths[i, :] = np.cumprod(1 + daily_returns) * df['Close'].iloc[-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i in range(iterations):\n",
        "            plt.plot(simulated_paths[i], alpha=0.2)\n",
        "\n",
        "        plt.title(f\"Monte Carlo Simulation Paths ({iterations} iterations)\")\n",
        "        plt.xlabel('Days')\n",
        "        plt.ylabel('Simulated Price')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return VaR\n"
      ],
      "metadata": {
        "id": "yZ3MtxGQGw2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "        # Call the function for the normal distribution\n",
        "        var_normal = calculate_VaR_MonteCarlo_Normal(df, days=252, iterations=10, confidence_level=0.95, plot=True, plot_paths=True)\n",
        "        print(f\"Normal VaR (greatest loss) for {name}: {var_normal*100:.2f}%\")\n",
        "\n",
        "        # Call the function for the t-distribution\n",
        "        var_advanced = calculate_VaR_MonteCarlo_Advanced(df, days=252, iterations=10, confidence_level=0.99, scale_factor=10, plot=True, plot_paths=True)\n",
        "        print(f\"Advanced VaR (greatest loss) for {name}: {var_advanced*100:.2f}%\")"
      ],
      "metadata": {
        "id": "e3MZDySsfskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_monte_carlo_simulation(df, days, iterations, confidence_level, plot=True, plot_subset=100):\n",
        "    # Calculate daily returns\n",
        "    returns = df['Close'].pct_change().dropna()\n",
        "\n",
        "    # Estimate parameters\n",
        "    mean_return = returns.mean()\n",
        "    vol = returns.std()  # Fixed volatility estimate\n",
        "    df_param, loc, scale = t.fit(returns)  # Fit t-distribution\n",
        "\n",
        "    # Simulate price paths\n",
        "    normal_simulated_returns = norm.rvs(mean_return, vol, (iterations, days))\n",
        "    t_simulated_returns = t.rvs(df_param, loc, scale, size=(iterations, days))\n",
        "\n",
        "    initial_price = df['Close'].iloc[-1]\n",
        "    normal_price_paths = initial_price * np.cumprod(1 + normal_simulated_returns, axis=1)\n",
        "    t_price_paths = initial_price * np.cumprod(1 + t_simulated_returns, axis=1)\n",
        "\n",
        "    # Calculate VaR from the final prices\n",
        "    normal_final_prices = normal_price_paths[:, -1]\n",
        "    t_final_prices = t_price_paths[:, -1]\n",
        "    normal_price_VaR = np.percentile(normal_final_prices, (1 - confidence_level) * 100)\n",
        "    t_price_VaR = np.percentile(t_final_prices, (1 - confidence_level) * 100)\n",
        "\n",
        "    if plot:\n",
        "      # Plot the simulated price paths\n",
        "      plt.figure(figsize=(14, 7))\n",
        "      subset_indices = np.random.choice(range(iterations), size=plot_subset, replace=False)\n",
        "      plt.plot(normal_price_paths[subset_indices].T, alpha=0.1, color='blue')\n",
        "      plt.plot(t_price_paths[subset_indices].T, alpha=0.1, color='red')\n",
        "      plt.title(f\"Combined Monte Carlo Simulation ({iterations} simulations)\")\n",
        "      plt.xlabel('Days')\n",
        "      plt.ylabel('Simulated Price Paths')\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "      # Plot the histogram of the final simulated prices\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.hist(normal_final_prices, bins=50, alpha=0.7, color='blue')\n",
        "      plt.hist(t_final_prices, bins=50, alpha=0.7, color='red')\n",
        "      plt.axvline(x=normal_price_VaR, color='navy', linestyle='--', label=f\"Normal VaR: {normal_price_VaR:.2f}\")\n",
        "      plt.axvline(x=t_price_VaR, color='darkred', linestyle='--', label=f\"t-Distribution VaR: {t_price_VaR:.2f}\")\n",
        "      plt.title(f\"Distribution of Simulated Final Prices\\nNormal VaR: {normal_price_VaR:.2f}, t-Distribution VaR: {t_price_VaR:.2f}\")\n",
        "      plt.xlabel('Simulated Final Prices')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "      return normal_price_VaR, t_price_VaR"
      ],
      "metadata": {
        "id": "aMs7MFetfsb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in zip(sheet_names, dataframes):\n",
        "    if not df.empty:\n",
        "        var_normal, var_advanced = combined_monte_carlo_simulation(\n",
        "            df,\n",
        "            days=252,\n",
        "            iterations=10000,\n",
        "            confidence_level=0.95,\n",
        "            plot=True,\n",
        "            plot_subset=100\n",
        "        )\n",
        "        print(f\"Normal VaR (greatest loss) for {name}: {var_normal*100:.2f}%\")\n",
        "        print(f\"Advanced VaR (greatest loss) for {name}: {var_advanced*100:.2f}%\")"
      ],
      "metadata": {
        "id": "YupwbzDxgm4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Portfolio\n",
        "Asset Allocation"
      ],
      "metadata": {
        "id": "pEA9oN6Sy_vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "nhLsgkHBwJbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(final_returns, VaR, CVaR, confidence_level):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(final_returns, kde=True, color='blue', bins=50)\n",
        "    plt.axvline(x=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR:.2f}\")\n",
        "    plt.axvline(x=CVaR, color='green', linestyle='--', label=f\"CVaR: {CVaR:.2f}\")\n",
        "    plt.title(\"Portfolio Return Distribution with VaR and CVaR\")\n",
        "    plt.xlabel(\"Portfolio Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kjm6eBrndLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_simulations(simulated_prices, simulated_returns, VaR, CVaR, confidence_level):\n",
        "    # Plot the simulated prices and returns\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # Plot simulated prices\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i in range(simulated_prices.shape[0]):\n",
        "        plt.plot(simulated_prices[i], alpha=0.5)\n",
        "    plt.title(\"Simulated Portfolio Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Prices\")\n",
        "\n",
        "    # Plot simulated returns\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i in range(simulated_returns.shape[0]):\n",
        "        plt.plot(simulated_returns[i], alpha=0.5)\n",
        "    plt.title(\"Simulated Portfolio Returns\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Returns\")\n",
        "\n",
        "    # Check if VaR and CVaR are numbers and not NaN before plotting\n",
        "    if VaR and not np.isnan(VaR):\n",
        "        plt.axhline(y=-VaR, color='red', linestyle='dashed', linewidth=2, label=f'VaR at {confidence_level*100}%: {-VaR}')\n",
        "    if CVaR and not np.isnan(CVaR):\n",
        "        plt.axhline(y=-CVaR, color='orange', linestyle='dashed', linewidth=2, label=f'CVaR: {-CVaR}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "D2rSUwC7RcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_monte_carlo_simulation(simulated_portfolio_returns, VaR, CVaR, confidence_level):\n",
        "    plt.figure(figsize=(21, 14))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    colors = sns.color_palette(\"hsv\", len(simulated_portfolio_returns))\n",
        "    for i in range(len(simulated_portfolio_returns)):\n",
        "        plt.plot(simulated_portfolio_returns[i, :], color=colors[i], alpha=0.5)\n",
        "    plt.axhline(y=VaR, color='red', linestyle='--', label=f\"VaR at {confidence_level*100}%: {VaR:.2f}\")\n",
        "    plt.axhline(y=CVaR, color='green', linestyle='--', label=f\"CVaR: {CVaR:.2f}\")\n",
        "    plt.title(f\"Monte Carlo Simulation of Portfolio\")\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Simulated Portfolio Returns')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vgK-xJ5LdLIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation_heatmap(combined_returns):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(combined_returns.corr(), annot=True, cmap='viridis')\n",
        "    plt.title(\"Assets Correlation Heatmap\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "znyfW_OCdLK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_VaR_impacts(VaR_impact):\n",
        "    categories = list(VaR_impact.keys())\n",
        "    impacts = list(VaR_impact.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(categories, impacts, color='skyblue')\n",
        "    plt.xlabel('Analysis Type')\n",
        "    plt.ylabel('Adjusted VaR')\n",
        "    plt.title('Impact on VaR by Different Analyses')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MNaj3FKmihEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SXGFN0ez1RCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_t_distribution(returns, confidence_level=0.99, iterations=10000):\n",
        "    # Fit t-distribution to the data\n",
        "    df, loc, scale = t.fit(returns)\n",
        "    # Simulate returns\n",
        "    simulated_returns = t.rvs(df, loc, scale, size=iterations)\n",
        "    # Calculate VaR\n",
        "    VaR = np.percentile(simulated_returns, (1 - confidence_level) * 100)\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "GVC-bOkWf_R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liquidity"
      ],
      "metadata": {
        "id": "8mTTQi2Kgf9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_liquidity_risk(dataframes, liquidity_threshold):\n",
        "    liquidity_risk = {}\n",
        "    for df, name in zip(dataframes, sheet_names):\n",
        "        average_volume = df['Volume'].mean()\n",
        "        liquidity_risk[name] = 'High' if average_volume < liquidity_threshold else 'Low'\n",
        "    return liquidity_risk"
      ],
      "metadata": {
        "id": "qoHK-1vXghNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stress Testing"
      ],
      "metadata": {
        "id": "Af3v_uY1ggF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stress_test(dataframes, weights, stress_factors):\n",
        "    stress_results = {}\n",
        "    for factor, change in stress_factors.items():\n",
        "        stressed_returns = []\n",
        "        for df, weight in zip(dataframes, weights):\n",
        "            stressed_price = df['Close'] * (1 + change)\n",
        "            stressed_return = stressed_price.pct_change().dropna()\n",
        "            stressed_returns.append(stressed_return * weight)\n",
        "        portfolio_stressed_return = pd.concat(stressed_returns, axis=1).sum(axis=1)\n",
        "        stress_results[factor] = portfolio_stressed_return\n",
        "    return stress_results"
      ],
      "metadata": {
        "id": "eNRWtUXUghj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario Analysis"
      ],
      "metadata": {
        "id": "ElKyoDVGggM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_scenario_analysis(dataframes, weights, scenarios):\n",
        "    scenario_results = {}\n",
        "    for scenario_name, scenario_changes in scenarios.items():\n",
        "        scenario_portfolio_returns = pd.DataFrame()\n",
        "        for df, weight, name in zip(dataframes, weights, sheet_names):\n",
        "            scenario_df = df.copy()\n",
        "            for column, change in scenario_changes.items():\n",
        "                if column in scenario_df.columns:\n",
        "                    scenario_df[column] *= (1 + change)\n",
        "            scenario_portfolio_returns[name] = scenario_df[column].pct_change() * weight\n",
        "        total_scenario_return = scenario_portfolio_returns.sum(axis=1)\n",
        "        scenario_VaR = np.percentile(total_scenario_return.dropna(), 5)\n",
        "        scenario_results[scenario_name] = scenario_VaR\n",
        "    return scenario_results"
      ],
      "metadata": {
        "id": "_dNIC94Hgh5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_VaR_impact(dataframes, weights, sheet_names, liquidity_threshold, stress_factors, scenarios, VaR, confidence_level=0.99):\n",
        "\n",
        "    liquidity_risks = calculate_liquidity_risk(dataframes, liquidity_threshold)\n",
        "    stress_test_results = stress_test(dataframes, weights, stress_factors)\n",
        "    scenario_analysis_results = conduct_scenario_analysis(dataframes, weights, scenarios)\n",
        "\n",
        "    adjusted_VaR = {\n",
        "        'Liquidity Risk': {},\n",
        "        'Stress Test': {},\n",
        "        'Scenario Analysis': {}\n",
        "    }\n",
        "\n",
        "    for name, risk in liquidity_risks.items():\n",
        "        adjusted_VaR['Liquidity Risk'][name] = VaR * 1.1 if risk == 'High' else VaR\n",
        "\n",
        "    for condition, returns in stress_test_results.items():\n",
        "        adjusted_VaR['Stress Test'][condition] = calculate_VaR_t_distribution(returns, confidence_level)\n",
        "\n",
        "    for scenario, returns in scenario_analysis_results.items():\n",
        "        adjusted_VaR['Scenario Analysis'][scenario] = calculate_VaR_t_distribution(returns, confidence_level)\n",
        "\n",
        "    flat_adjusted_VaR = {}\n",
        "\n",
        "    for analysis_type, results in adjusted_VaR.items():\n",
        "        for name, value in results.items():\n",
        "            flat_adjusted_VaR[f\"{analysis_type} - {name}\"] = value\n",
        "\n",
        "    return flat_adjusted_VaR"
      ],
      "metadata": {
        "id": "IXGnV-L2hcf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "the alpha need cositnet with up calculate function"
      ],
      "metadata": {
        "id": "EnH2wQ6CVjm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_portfolio_analysis(dataframes, weights, sheet_names, days=252, iterations=100, confidence_level=0.99, increased_volatility_factor=1.5):\n",
        "    # Convert days and iterations to integers\n",
        "    days = int(days)\n",
        "    iterations = int(iterations)\n",
        "\n",
        "    # Combine the returns of the different dataframes\n",
        "    combined_returns = pd.DataFrame()\n",
        "    combined_prices = pd.DataFrame()\n",
        "    for df, weight, name in zip(dataframes, weights, sheet_names):\n",
        "        daily_returns = df['Close'].pct_change().dropna()\n",
        "        combined_returns[name] = daily_returns * weight\n",
        "        combined_prices[name] = df['Close'] * weight\n",
        "\n",
        "    portfolio_returns = combined_returns.sum(axis=1)  # Portfolio returns\n",
        "    portfolio_prices = combined_prices.sum(axis=1)  # Portfolio prices\n",
        "\n",
        "    # Initialize arrays for simulated portfolio prices and returns\n",
        "    simulated_portfolio_prices = np.zeros((iterations, days))\n",
        "    simulated_portfolio_returns = np.zeros((iterations, days))\n",
        "    initial_price = portfolio_prices.iloc[0]  # Initial portfolio price\n",
        "\n",
        "    # Run Monte Carlo simulation for prices and returns\n",
        "    for i in range(iterations):\n",
        "        prices = [initial_price]\n",
        "        for d in range(1, days):\n",
        "            simulated_return = np.random.normal(portfolio_returns.mean(), portfolio_returns.std() * increased_volatility_factor)\n",
        "            price = prices[d-1] * (1 + simulated_return)\n",
        "            prices.append(price)\n",
        "        simulated_portfolio_prices[i, :] = prices\n",
        "        simulated_portfolio_returns[i, :] = np.array(prices) / initial_price - 1\n",
        "\n",
        "    # Calculate final returns and VaR\n",
        "    final_returns = simulated_portfolio_returns[:, -1]\n",
        "    VaR = calculate_VaR_t_distribution(final_returns, confidence_level, iterations)\n",
        "\n",
        "    # Calculate CVaR if there are returns below VaR\n",
        "    returns_below_VaR = final_returns[final_returns <= VaR]\n",
        "    CVaR = np.nan  # Initialize CVaR as NaN\n",
        "    if len(returns_below_VaR) > 0:\n",
        "        CVaR = returns_below_VaR.mean()  # Calculate CVaR\n",
        "\n",
        "    # Call plotting functions\n",
        "    plot_simulations(simulated_portfolio_prices, simulated_portfolio_returns, VaR, CVaR, confidence_level)\n",
        "\n",
        "    plot_distribution(final_returns, VaR, CVaR, confidence_level)\n",
        "    plot_monte_carlo_simulation(simulated_portfolio_returns, VaR, CVaR, confidence_level)\n",
        "    plot_correlation_heatmap(combined_returns)\n",
        "\n",
        "    VaR_impact = analyze_VaR_impact(dataframes, weights, sheet_names, liquidity_threshold, stress_factors, scenarios, VaR, confidence_level)\n",
        "\n",
        "    plot_VaR_impacts(VaR_impact)\n",
        "\n",
        "    return VaR, CVaR, simulated_portfolio_prices, simulated_portfolio_returns"
      ],
      "metadata": {
        "id": "9gOpkUY6dLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liquidity_threshold = 100\n",
        "\n",
        "stress_factors = {\n",
        "    'Market Drop': -0.10,\n",
        "    'Interest Rate Rise': 0.05\n",
        "}\n",
        "\n",
        "scenarios = {\n",
        "    'Economic Boom': {'Close': 0.1},\n",
        "    'Market Crash': {'Close': -0.2}\n",
        "}"
      ],
      "metadata": {
        "id": "uQ-xmfKxrhGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MC Price to get return"
      ],
      "metadata": {
        "id": "RWOd6ytprIpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [0.4,0.2,0.4]\n",
        "liquidity_threshold = 100\n",
        "scenario_results = conduct_scenario_analysis(dataframes, weights, scenarios)\n",
        "\n",
        "print(scenario_results)\n",
        "# Call the function to plot the analysis\n",
        "monte_carlo_portfolio_analysis(dataframes, weights, sheet_names)"
      ],
      "metadata": {
        "id": "UTB6ZQicr0hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_portfolio_VaR(weights, dataframes, confidence_level=0.99, scale_factor=10):\n",
        "    portfolio_returns = pd.DataFrame()\n",
        "\n",
        "    for weight, df in zip(weights, dataframes):\n",
        "        returns = df['Close'].pct_change().dropna() * scale_factor\n",
        "        portfolio_returns = pd.concat([portfolio_returns, returns * weight], axis=1)\n",
        "\n",
        "    total_returns = portfolio_returns.sum(axis=1)\n",
        "    df, loc, scale = t.fit(total_returns)\n",
        "    simulated_returns = t.rvs(df, loc, scale, size=10000)\n",
        "\n",
        "    simulated_price_changes = np.exp(simulated_returns) - 1\n",
        "    VaR = np.percentile(simulated_price_changes, (1 - confidence_level) * 100)\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "P9e0OOqzDFG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Minimize"
      ],
      "metadata": {
        "id": "c5b7EaGyl8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimize_VaR(dataframes, sheet_names, confidence_level=0.99, scale_factor=10):\n",
        "\n",
        "    num_assets = len(dataframes)\n",
        "    initial_weights = np.array([1.0 / num_assets] * num_assets)\n",
        "\n",
        "    # Constraints: Weights sum to 1, and each weight is between 0 and 1\n",
        "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "    bounds = tuple((0, 1) for asset in range(num_assets))\n",
        "\n",
        "    # Objective function: Minimize VaR\n",
        "    def objective(weights):\n",
        "        return calculate_portfolio_VaR(weights, dataframes, confidence_level, scale_factor)\n",
        "\n",
        "    # Optimization\n",
        "    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "    if result.success:\n",
        "        optimized_weights = result.x\n",
        "        optimized_VaR = calculate_portfolio_VaR(optimized_weights, dataframes, confidence_level, scale_factor)\n",
        "        print(f\"Optimized Weights: {optimized_weights}\")\n",
        "        print(f\"Optimized Portfolio VaR: {optimized_VaR}\")\n",
        "        return optimized_weights, optimized_VaR\n",
        "    else:\n",
        "        raise ValueError('Optimization failed')"
      ],
      "metadata": {
        "id": "-uxEXNt1AtwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_weights, optimized_VaR = minimize_VaR(dataframes, sheet_names)"
      ],
      "metadata": {
        "id": "MCBJTq4eAt0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPtimize"
      ],
      "metadata": {
        "id": "984czvi5YjeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genetic Algo"
      ],
      "metadata": {
        "id": "i211Bo_EquC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PARAMS = 6"
      ],
      "metadata": {
        "id": "HVBNw1KjR0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_individual():\n",
        "    number_of_assets = len(dataframes)\n",
        "    weights = [random.uniform(0, 1) for _ in range(number_of_assets)]\n",
        "    weights /= np.sum(weights)  # Normalize the weights to sum to 1\n",
        "\n",
        "    days = random.randint(200, 300)  # Example values\n",
        "    iterations = random.randint(800, 1200)\n",
        "    confidence_level = random.uniform(0.95, 0.99)\n",
        "\n",
        "    return [weights, days, iterations, confidence_level]"
      ],
      "metadata": {
        "id": "XO68yYUBUu-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness_function(individual):\n",
        "    weights, days, iterations, confidence_level = individual\n",
        "    VaR, CVaR, VaR_impact, additional_value = monte_carlo_portfolio_analysis(dataframes, weights, sheet_names, days, iterations, confidence_level)\n",
        "    return (-VaR,)\n",
        "\n",
        "# Set up GA parameters\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", fitness_function)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
      ],
      "metadata": {
        "id": "7o0Py5K5quLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_and_track():\n",
        "    population = toolbox.population(n=1)\n",
        "    ngen = 2\n",
        "    best_scores = []\n",
        "    best_individuals = []\n",
        "\n",
        "    for gen in range(ngen):\n",
        "        offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.2)\n",
        "        fits = toolbox.map(toolbox.evaluate, offspring)\n",
        "\n",
        "        for fit, ind in zip(fits, offspring):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        population = toolbox.select(offspring, k=len(population))\n",
        "        best_ind = tools.selBest(population, k=1)[0]\n",
        "        best_scores.append(best_ind.fitness.values[0])\n",
        "        best_individuals.append(best_ind)\n",
        "\n",
        "\n",
        "        print(f\"Generation {gen}: Best individual is {best_ind}, Best VaR is {-best_ind.fitness.values[0]}\")\n",
        "\n",
        "    overall_best_ind = tools.selBest(population, k=1)[0]\n",
        "    print(f\"Overall best individual is {overall_best_ind}, with VaR: {-overall_best_ind.fitness.values[0]}\")\n",
        "\n",
        "    return best_scores, best_individuals"
      ],
      "metadata": {
        "id": "W_6tiniRquOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_scores, best_individuals = optimize_and_track()"
      ],
      "metadata": {
        "id": "nNUd_ks_rX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitness scores are defined with negative VaR values, so a larger score on the graph (closer to zero or a positive number) actually means better fitness because it represents lower risk."
      ],
      "metadata": {
        "id": "eBHbR3ynOdoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(best_scores)\n",
        "plt.title(\"Optimization Progress\")\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Best Fitness Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0QLF8vAarjuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo ML"
      ],
      "metadata": {
        "id": "37WpY1LNw4ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "egO_1T1bxDZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "7K2jIJBpmCO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = combined_df.dropna()\n",
        "\n",
        "combined_df['Return'] = combined_df['Close'].pct_change()\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "combined_df['DayOfWeek'] = combined_df['Date'].dt.dayofweek\n",
        "combined_df['Month'] = combined_df['Date'].dt.month\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "combined_df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(combined_df[['Open', 'High', 'Low', 'Close', 'Volume']])"
      ],
      "metadata": {
        "id": "FwQA9jy8mCR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = combined_df[['Open', 'High', 'Low', 'Volume']]\n",
        "y = combined_df['Close']"
      ],
      "metadata": {
        "id": "VTdy7nZkmCbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5JtLxgKsoG6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=1000, max_features='sqrt', min_samples_leaf=4, random_state=42)"
      ],
      "metadata": {
        "id": "P0oNVCRcoKD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_simulations = 10000\n",
        "simulated_price_paths = np.zeros((num_simulations, len(X_test)))"
      ],
      "metadata": {
        "id": "LzofEu-LoMt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"Average Cross-Validation Score:\", np.mean(cross_val_scores))"
      ],
      "metadata": {
        "id": "dvgkEAXVjcir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-ZauvltmjiAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poisson"
      ],
      "metadata": {
        "id": "0OJmTjpkrOwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_simulations = 5000\n",
        "lambda_jump = 0.05\n",
        "jump_sd = 0.4"
      ],
      "metadata": {
        "id": "Fjs6uIsUjmZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_stock_prices(predictions, X_test, num_simulations, lambda_jump, jump_sd):\n",
        "    # Simulate volatility - GARCH\n",
        "    garch = arch_model(predictions, vol='Garch', p=1, q=1)\n",
        "    garch_fitted = garch.fit()\n",
        "\n",
        "    simulation_length = len(X_test)\n",
        "    simulated_price_paths = np.zeros((num_simulations, simulation_length))\n",
        "\n",
        "    # Initial stock prices\n",
        "    initial_prices = X_test['Open'].values\n",
        "\n",
        "    # Monte Carlo simulation\n",
        "    for i in range(num_simulations):\n",
        "        simulated_prices = [initial_prices[0]]\n",
        "        for t in range(1, simulation_length):\n",
        "            # Fetch volatility\n",
        "            vol = garch_fitted.conditional_volatility[t]\n",
        "            # Random shock\n",
        "            shock = np.random.normal(0, vol)\n",
        "            # Jump part\n",
        "            jump = np.random.normal(0, jump_sd) if np.random.random() < lambda_jump else 0\n",
        "            # Simulate price\n",
        "            simulated_price = simulated_prices[t-1] * (1 + predictions[t] + shock + jump)\n",
        "            simulated_prices.append(simulated_price)\n",
        "        simulated_price_paths[i, :] = simulated_prices\n",
        "\n",
        "    return simulated_price_paths"
      ],
      "metadata": {
        "id": "9hErmDT6ruxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_simulation_results(simulated_price_paths, VaR):\n",
        "    print(VaR)\n",
        "    # Visualize simulated stock price paths\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(len(simulated_price_paths)):  # Plot only 100 paths for clarity\n",
        "        plt.plot(simulated_price_paths[i, :], alpha=0.2)\n",
        "    plt.title(\"Simulated Stock Price Paths\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize VaR distribution\n",
        "    initial_prices = simulated_price_paths[:, 0]\n",
        "    end_returns = (simulated_price_paths[:, -1] - initial_prices) / initial_prices\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(end_returns, bins=25, alpha=0.7, color='blue')\n",
        "    plt.axvline(-VaR, color='red', linestyle='dashed', linewidth=2)\n",
        "    plt.title(\"Distribution of Returns and VaR\")\n",
        "    plt.xlabel(\"Return\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kRgzqg_vru8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_randomForest(simulated_price_paths, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk (VaR) from simulated stock price paths.\n",
        "    \"\"\"\n",
        "    initial_prices = simulated_price_paths[:, 0]\n",
        "    end_prices = simulated_price_paths[:, -1]\n",
        "    returns = (end_prices - initial_prices) / initial_prices\n",
        "    sorted_returns = np.sort(returns)\n",
        "    var_index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    VaR = -sorted_returns[var_index]\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "-LZ9Tv7vwS1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulated_price_paths = simulate_stock_prices(predictions, X_test, num_simulations, lambda_jump, jump_sd)\n",
        "VaR = calculate_VaR_randomForest(simulated_price_paths)\n",
        "visualize_simulation_results(simulated_price_paths, VaR)"
      ],
      "metadata": {
        "id": "6CacBsXAo4Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Parameters"
      ],
      "metadata": {
        "id": "Ytz34HSGQhEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF"
      ],
      "metadata": {
        "id": "LC5rSwFFko6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "6YticTOekJBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_space = {\n",
        "    'n_estimators': (100, 1000),\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}"
      ],
      "metadata": {
        "id": "i2zqCTwG7Yf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_search = BayesSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=32,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3-SBDlKrkLGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DXKTyDs_kPLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters found: \", bayes_search.best_params_)"
      ],
      "metadata": {
        "id": "p8BGT-fgkRmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = bayes_search.best_estimator_\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "print(\"Test set score of best model: \", test_score)"
      ],
      "metadata": {
        "id": "UCWinGxbkUiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Composite"
      ],
      "metadata": {
        "id": "ThMpeUzCww1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copula"
      ],
      "metadata": {
        "id": "LdKoZ9VxcsEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genenerate diifferent class data"
      ],
      "metadata": {
        "id": "yThTLKubkiHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_fund_data(days, mean_return=0, std_dev=0.01):\n",
        "\n",
        "    dates = pd.date_range(start=\"2022-01-01\", periods=days)\n",
        "    returns = np.random.normal(mean_return, std_dev, days)\n",
        "    return pd.DataFrame({'Date': dates, 'FUND_Close': returns.cumsum()})"
      ],
      "metadata": {
        "id": "GoFQnf6zkhFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_Copula_and_Fund(dataframes, fund_data, weights, days=252, iterations=10000, confidence_level=0.99):\n",
        "\n",
        "    returns = pd.DataFrame()\n",
        "    for i, df in enumerate(dataframes):\n",
        "        stock_name = sheet_names[i]\n",
        "        returns[stock_name] = df['Close'].pct_change().dropna()\n",
        "    returns['FUND'] = fund_data['FUND_Close'].pct_change().dropna()\n",
        "\n",
        "\n",
        "    copula = GaussianMultivariate()\n",
        "    copula.fit(returns)\n",
        "\n",
        "\n",
        "    simulated_returns = copula.sample(iterations)\n",
        "\n",
        "\n",
        "\n",
        "    if simulated_returns.shape[1] != len(weights):\n",
        "        raise ValueError(\"Mismatch in number of assets and weights.\")\n",
        "\n",
        "    portfolio_returns = simulated_returns.dot(weights)\n",
        "\n",
        "\n",
        "    VaR = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(portfolio_returns, bins=50, kde=True)\n",
        "    plt.axvline(x=VaR, color='r', linestyle='--', label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title('Simulated Portfolio Returns Distribution with VaR')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "oUH2B_s7cr8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fund_data = simulate_fund_data(252)\n",
        "\n",
        "sheet_names = [\"META\", \"MSFT\", \"NFLX\"]\n",
        "dataframes = [get_sheet_data(name) for name in sheet_names]\n",
        "\n",
        "weights = np.array([0.2, 0.2, 0.2, 0.4])\n",
        "\n",
        "VaR = calculate_VaR_with_Copula_and_Fund(dataframes, fund_data, weights)"
      ],
      "metadata": {
        "id": "v45gYwdVktTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Theory"
      ],
      "metadata": {
        "id": "qyFnyYt4MVTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_asset_price(S0, days, mu, sigma):\n",
        "    dt = 1 / 252  # One trading day\n",
        "    random_shocks = np.random.normal(0, 1, days)\n",
        "    price = S0 * np.cumprod(np.exp((mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * random_shocks))\n",
        "    return price"
      ],
      "metadata": {
        "id": "08OhGX1CMVcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_option_prices(S0, K, T, r, sigma, num_days, num_simulations):\n",
        "    \"\"\"\n",
        "    Simulate option prices using GBM for the underlying asset and the Black-Scholes model.\n",
        "    S0: Initial stock price\n",
        "    K: Strike price\n",
        "    T: Time to maturity\n",
        "    r: Risk-free interest rate\n",
        "    sigma: Volatility of the underlying asset\n",
        "    num_days: Number of days to simulate\n",
        "    num_simulations: Number of simulated paths\n",
        "    \"\"\"\n",
        "    dt = T / num_days\n",
        "    option_price_paths = []\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        # Simulate price path for the underlying asset\n",
        "        price_path = simulate_gbm(S0, r, sigma, T, num_days)\n",
        "\n",
        "        # Calculate option price for each day\n",
        "        option_prices = [black_scholes(S, K, T - i*dt, r, sigma) for i, S in enumerate(price_path)]\n",
        "        option_price_paths.append(option_prices)\n",
        "\n",
        "    return np.array(option_price_paths)"
      ],
      "metadata": {
        "id": "pg1tlpF1dGJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def black_scholes(S, K, T, r, sigma, option_type=\"call\"):\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    if option_type == \"call\":\n",
        "        option_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "    else:\n",
        "        option_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
        "    return option_price"
      ],
      "metadata": {
        "id": "WVOwaDNuMVkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Asset Price Simulation (e.g., Geometric Brownian Motion)"
      ],
      "metadata": {
        "id": "Rr8rpSk4MiGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_gbm(S0, mu, sigma, T, steps):\n",
        "    \"\"\"\n",
        "    Simulate asset prices using Geometric Brownian Motion.\n",
        "    S0: Initial asset price\n",
        "    mu: Drift coefficient\n",
        "    sigma: Volatility coefficient\n",
        "    T: Time horizon\n",
        "    steps: Number of time steps\n",
        "    \"\"\"\n",
        "    dt = T / steps\n",
        "    random_component = np.random.normal(0, np.sqrt(dt), steps)\n",
        "    price_path = np.exp((mu - 0.5 * sigma**2) * dt + sigma * random_component)\n",
        "    return S0 * np.cumprod(price_path)"
      ],
      "metadata": {
        "id": "HcBPheehMVmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_gbm_european_call(S0, K, T, r, sigma, num_simulations, steps):\n",
        "    \"\"\"\n",
        "    Monte Carlo simulation for European call option using GBM.\n",
        "    S0: Initial stock price\n",
        "    K: Strike price\n",
        "    T: Time to maturity\n",
        "    r: Risk-free interest rate\n",
        "    sigma: Volatility\n",
        "    num_simulations: Number of simulated paths\n",
        "    steps: Number of time steps\n",
        "    \"\"\"\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "    option_price_paths = []\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        asset_prices = simulate_gbm(S0, r, sigma, T, steps)\n",
        "        option_prices = np.maximum(asset_prices - K, 0)  # Calculate option prices at each step\n",
        "        discounted_option_prices = np.exp(-r * np.arange(1, steps + 1) / 252) * option_prices\n",
        "        option_price_paths.append(discounted_option_prices)\n",
        "\n",
        "    # Convert to numpy array for easier manipulation\n",
        "    option_price_paths = np.array(option_price_paths)\n",
        "\n",
        "    # Calculate the average option price for each day across all simulations\n",
        "    mean_option_prices = np.mean(option_price_paths, axis=0)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Plot simulated option price paths\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for option_path in option_price_paths:\n",
        "        plt.plot(option_path, color='blue', alpha=0.1)\n",
        "    plt.title('Simulated Option Price Paths')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Option Price')\n",
        "\n",
        "    # Plot average option price\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(mean_option_prices, color='green')\n",
        "    plt.title('Average Option Price Over Time')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Average Option Price')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mean_option_prices"
      ],
      "metadata": {
        "id": "5oVCpMCUMqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Cost consideration"
      ],
      "metadata": {
        "id": "1Rm8BMysOBEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the simulation\n",
        "S0 = 100\n",
        "K = 110\n",
        "T = 1\n",
        "r = 0.05\n",
        "sigma = 0.2\n",
        "num_simulations = 10000\n",
        "steps = 252\n",
        "\n",
        "# Run simulation and plot\n",
        "option_price_series = monte_carlo_gbm_european_call(S0, K, T, r, sigma, num_simulations, steps)\n",
        "\n",
        "# Print the entire series of average option prices\n",
        "print(\"Simulated European Call Option Price Series:\")\n",
        "print(option_price_series)\n",
        "\n",
        "# Print the final average option price\n",
        "final_option_price = option_price_series[-1]\n",
        "print(f\"Final Simulated European Call Option Price: {final_option_price:.2f}\")"
      ],
      "metadata": {
        "id": "pIUcqLfpMr5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copula"
      ],
      "metadata": {
        "id": "cuwlSYqhOFMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_option_returns(days, S0, K, T, r, sigma, option_type=\"call\"):\n",
        "    # Simulate underlying asset prices\n",
        "    asset_prices = simulate_asset_price(S0, days, r, sigma)\n",
        "\n",
        "    # Calculate option prices for each day\n",
        "    option_prices = [black_scholes(S, K, T - i/252, r, sigma, option_type) for i, S in enumerate(asset_prices)]\n",
        "\n",
        "    # Convert to DataFrame and calculate returns\n",
        "    option_prices_df = pd.DataFrame({'OPTION_Close': option_prices})\n",
        "    option_returns = option_prices_df['OPTION_Close'].pct_change().dropna()\n",
        "    return option_returns"
      ],
      "metadata": {
        "id": "NGEYbmvNQBlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, weights, days=252, iterations=10000, confidence_level=0.99, include_fund=True, include_option=True):\n",
        "    returns = pd.DataFrame()\n",
        "    for i, df in enumerate(dataframes):\n",
        "        stock_name = sheet_names[i]\n",
        "        returns[stock_name] = df['Close'].pct_change().dropna()\n",
        "\n",
        "    if include_fund:\n",
        "        returns['FUND'] = fund_data['FUND_Close'].pct_change().dropna()\n",
        "\n",
        "    if include_option:\n",
        "        # Assuming option_data is already a series of returns\n",
        "        returns['OPTION'] = option_data\n",
        "\n",
        "    # Fit the Copula model\n",
        "    copula = GaussianMultivariate()\n",
        "    copula.fit(returns)\n",
        "\n",
        "    simulated_returns = copula.sample(iterations)\n",
        "\n",
        "    if simulated_returns.shape[1] != len(weights):\n",
        "        raise ValueError(\"Mismatch in number of assets and weights.\")\n",
        "\n",
        "    portfolio_returns = simulated_returns.dot(weights)\n",
        "\n",
        "    VaR = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(portfolio_returns, bins=50, kde=True)\n",
        "    plt.axvline(x=VaR, color='r', linestyle='--', label=f'VaR at {confidence_level*100}%: {VaR}')\n",
        "    plt.title('Simulated Portfolio Returns Distribution with VaR')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return VaR"
      ],
      "metadata": {
        "id": "oAxEjBorPFJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example portfolio: 2 stocks, 1 fund, 1 option\n",
        "num_stocks = len(dataframes)\n",
        "include_fund = True\n",
        "include_option = True\n",
        "\n",
        "# Adjust weights based on included assets\n",
        "if include_fund and include_option:\n",
        "    num_assets = num_stocks + 2  # stocks, 1 fund, 1 option\n",
        "elif include_fund:\n",
        "    num_assets = num_stocks + 1  # stocks, 1 fund\n",
        "elif include_option:\n",
        "    num_assets = num_stocks + 1  # stocks, 1 option\n",
        "else:\n",
        "    num_assets = num_stocks  # only stocks\n",
        "\n",
        "# Define weights (ensure they sum up to 1)\n",
        "weights = [1/num_assets] * num_assets  # Adjust this according to your specific portfolio\n",
        "\n",
        "# Now calculate VaR\n",
        "option_data = simulate_option_returns(days=252, S0=100, K=110, T=1, r=0.05, sigma=0.2, option_type=\"call\")\n",
        "VaR = calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, weights, include_fund=include_fund, include_option=include_option)"
      ],
      "metadata": {
        "id": "_jzRvehYwQeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annealing"
      ],
      "metadata": {
        "id": "yQHo0vLzEMgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def portfolio_variance(weights, cov_matrix):\n",
        "    \"\"\"\n",
        "    Calculate the variance of a portfolio.\n",
        "    weights: Portfolio weights (numpy array)\n",
        "    cov_matrix: Covariance matrix of asset returns (numpy array)\n",
        "    \"\"\"\n",
        "    return np.dot(weights.T, np.dot(cov_matrix, weights))"
      ],
      "metadata": {
        "id": "gTuzgYxnj5ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulated_annealing_monte_carlo(cov_matrix, initial_weights, steps=1000, temp=1.0, cooling_rate=0.95):\n",
        "    \"\"\"\n",
        "    Simulated Annealing combined with Monte Carlo Simulation for portfolio optimization.\n",
        "    cov_matrix: Covariance matrix of asset returns\n",
        "    initial_weights: Initial portfolio weights\n",
        "    steps: Number of iterations\n",
        "    temp: Initial temperature for annealing\n",
        "    cooling_rate: Rate at which temperature decreases\n",
        "    \"\"\"\n",
        "    current_solution = np.array(initial_weights)\n",
        "    best_solution = np.array(initial_weights)\n",
        "    best_variance = portfolio_variance(current_solution, cov_matrix)\n",
        "\n",
        "    variances = [best_variance]  # Store variances for visualization\n",
        "\n",
        "    for step in range(steps):\n",
        "        new_solution = np.random.normal(current_solution, 0.1)\n",
        "        new_solution = new_solution / np.sum(new_solution)\n",
        "\n",
        "        current_variance = portfolio_variance(current_solution, cov_matrix)\n",
        "        new_variance = portfolio_variance(new_solution, cov_matrix)\n",
        "\n",
        "        if new_variance < best_variance:\n",
        "            best_solution, best_variance = new_solution, new_variance\n",
        "        elif np.random.random() < np.exp(-(new_variance - current_variance) / temp):\n",
        "            current_solution = new_solution\n",
        "\n",
        "        temp *= cooling_rate\n",
        "        variances.append(best_variance)\n",
        "\n",
        "    # Visualization\n",
        "    plt.plot(variances)\n",
        "    plt.title('Portfolio Variance over Simulated Annealing Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.show()\n",
        "\n",
        "    return best_solution"
      ],
      "metadata": {
        "id": "c8cc49ASEMn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example covariance matrix (5 assets)\n",
        "cov_matrix = np.array([\n",
        "    [0.005, -0.002, 0.004, 0.001, 0.002],\n",
        "    [-0.002, 0.004, -0.001, 0.002, 0.003],\n",
        "    [0.004, -0.001, 0.01, 0.003, 0.002],\n",
        "    [0.001, 0.002, 0.003, 0.006, 0.001],\n",
        "    [0.002, 0.003, 0.002, 0.001, 0.007]\n",
        "])\n",
        "\n",
        "num_assets = len(cov_matrix)\n",
        "\n",
        "# Initial weights (equal distribution for 5 assets)\n",
        "initial_weights = [1/num_assets] * num_assets\n",
        "\n",
        "# Run simulated annealing to optimize weights\n",
        "optimized_weights = simulated_annealing_monte_carlo(cov_matrix, initial_weights)\n",
        "\n",
        "# Assuming you have the following data ready\n",
        "fund_data = simulate_fund_data(252)  # 1 year of trading days\n",
        "option_data = simulate_option_returns(252, S0, K, T, r, sigma)\n",
        "\n",
        "# Now calculate VaR with optimized weights\n",
        "VaR = calculate_VaR_with_Copula_and_FundorOption(dataframes, fund_data, option_data, optimized_weights, include_fund=True, include_option=True)\n",
        "print(\"Calculated VaR:\", VaR)"
      ],
      "metadata": {
        "id": "6QeE0olcjMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "FdSKEh2wPmFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Do：\n",
        "\n",
        "Update Test Data such as 2023 -> 2024\n",
        "\n",
        "Need use different Machine Learning / Deep Learning /Reniforcement Learning\n",
        "\n",
        "HMM\n",
        "\n",
        "Annealing\n",
        "\n",
        "Comment and Text compelte\n",
        "\n",
        "Scenario quantify\n",
        "\n",
        "More Data\n",
        "\n",
        "EVT"
      ],
      "metadata": {
        "id": "vN3C3KHRiofN"
      }
    }
  ]
}